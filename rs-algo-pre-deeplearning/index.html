<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>推荐系统前深度学习时代：从协同过滤到 GBDT+LR - 二手知识</title>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3CREWXCLR7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-3CREWXCLR7")</script><meta name=Description content><meta property="og:title" content="推荐系统前深度学习时代：从协同过滤到 GBDT+LR"><meta property="og:description" content="[toc] 本人身处广告营销领域，广告系统本质是个推荐系统，本文旨在梳理相关推荐算法。《深度学习推荐系统》是一本很不错的入门书籍，很系统，不过很多地方"><meta property="og:type" content="article"><meta property="og:url" content="https://chienlungcheung.github.io/rs-algo-pre-deeplearning/"><meta property="og:image" content="https://chienlungcheung.github.io/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-09-12T00:43:30+08:00"><meta property="article:modified_time" content="2023-09-12T00:43:30+08:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://chienlungcheung.github.io/logo.png"><meta name=twitter:title content="推荐系统前深度学习时代：从协同过滤到 GBDT+LR"><meta name=twitter:description content="[toc] 本人身处广告营销领域，广告系统本质是个推荐系统，本文旨在梳理相关推荐算法。《深度学习推荐系统》是一本很不错的入门书籍，很系统，不过很多地方"><meta name=twitter:site content="@haricheung"><meta name=application-name content="DoIt"><meta name=apple-mobile-web-app-title content="DoIt"><meta name=theme-color content="#f8f8f8"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=canonical href=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/><link rel=prev href=https://chienlungcheung.github.io/socialization/><link rel=next href=https://chienlungcheung.github.io/complexity-and-how-to-deal-with-it/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/color.css><link rel=stylesheet href=/css/style.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/fontawesome-free/all.min.css><noscript><link rel=stylesheet href=/lib/fontawesome-free/all.min.css></noscript><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/animate/animate.min.css><noscript><link rel=stylesheet href=/lib/animate/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/chienlungcheung.github.io\/rs-algo-pre-deeplearning\/"},"genre":"posts","keywords":"推荐系统, 深度学习, 机器学习","wordcount":4993,"url":"https:\/\/chienlungcheung.github.io\/rs-algo-pre-deeplearning\/","datePublished":"2023-09-12T00:43:30+08:00","dateModified":"2023-09-12T00:43:30+08:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Hari"},"description":""}</script><script src=//instant.page/5.1.1 defer type=module integrity=sha384-MWfCL6g1OTGsbSwfuMHc8+8J2u71/LA8dzlIN3ycajckxuZZmF+DNjdm7O6H3PSq></script></head><body header-desktop=fixed header-mobile=auto><script type=text/javascript>function setTheme(e){document.body.setAttribute("theme",e),document.documentElement.style.setProperty("color-scheme",e==="light"?"light":"dark"),window.theme=e}function saveTheme(e){window.localStorage&&localStorage.setItem("theme",e)}function getMeta(e){const t=document.getElementsByTagName("meta");for(let n=0;n<t.length;n++)if(t[n].getAttribute("name")===e)return t[n];return""}if(window.localStorage&&localStorage.getItem("theme")){let e=localStorage.getItem("theme");e==="light"||e==="dark"||e==="black"?setTheme(e):setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light")}else"auto"==="light"||"auto"==="dark"||"auto"==="black"?(setTheme("auto"),saveTheme("auto")):(saveTheme("auto"),setTheme(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?"dark":"light"));let metaColors={light:"#f8f8f8",dark:"#252627",black:"#000000"};getMeta("theme-color").content=metaColors[document.body.getAttribute("theme")]</script><div id=back-to-top></div><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=二手知识>二手知识</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=# class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i>
</a><a href=# class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i>
</span></span><a href=# class="menu-item theme-select" title=切换主题><i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-desktop title=切换主题><option value=light>浅色</option><option value=dark>深色</option><option value=black>黑色</option><option value=auto>跟随系统</option></select></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=二手知识>二手知识</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=# class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i>
</a><a href=# class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=# class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a href=# class="menu-item theme-select" title=切换主题>
<i class="fas fa-adjust fa-fw"></i>
<select class=color-theme-select id=theme-select-mobile title=切换主题><option value=light>浅色</option><option value=dark>深色</option><option value=black>黑色</option><option value=auto>跟随系统</option></select></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class=toc-content id=toc-content-auto><nav id=TableOfContents><ul><li><a href=#0-推荐算法四要素>0 推荐算法四要素</a></li><li><a href=#1-协同过滤collaborative-filtering-cf>1 协同过滤（Collaborative Filtering, CF）</a><ul><li><a href=#11-user-based-cf>1.1 User-based CF</a></li><li><a href=#12-item-based-cf>1.2 Item-based CF</a></li><li><a href=#13-user-based-cf-和-item-based-cf-比较>1.3 User-based CF 和 Item-based CF 比较</a></li><li><a href=#14-优势>1.4 优势</a></li><li><a href=#15-劣势>1.5 劣势</a></li></ul></li><li><a href=#2-矩阵分解>2 矩阵分解</a><ul><li><a href=#21-背景>2.1 背景</a></li><li><a href=#22-算法原理>2.2 算法原理</a></li><li><a href=#23-优势>2.3 优势</a></li><li><a href=#24-劣势>2.4 劣势</a></li></ul></li><li><a href=#3-lrlogistic-regression>3 LR（logistic regression）</a><ul><li><a href=#31-背景>3.1 背景</a></li><li><a href=#32-算法原理>3.2 算法原理</a></li><li><a href=#33-优势>3.3 优势</a></li><li><a href=#34-劣势>3.4 劣势</a></li></ul></li><li><a href=#4-poly2polynomial-regression-of-degree-2>4 Poly2（Polynomial Regression of degree 2）</a><ul><li><a href=#41-背景>4.1 背景</a></li><li><a href=#42-算法原理>4.2 算法原理</a></li><li><a href=#43-优势>4.3 优势</a></li><li><a href=#44-劣势>4.4 劣势</a></li></ul></li><li><a href=#5-fmfactorization-machines>5 FM（factorization machines）</a><ul><li><a href=#51-背景>5.1 背景</a></li><li><a href=#52-优势>5.2 优势</a></li><li><a href=#53-劣势>5.3 劣势</a></li></ul></li><li><a href=#6-ffmfiled-awareness-factorization-machines>6 FFM（filed-awareness factorization machines）</a><ul><li><a href=#61-背景>6.1 背景</a></li><li><a href=#62-算法原理>6.2 算法原理</a></li><li><a href=#63-优势>6.3 优势</a></li><li><a href=#64-劣势>6.4 劣势</a></li></ul></li><li><a href=#7-ls-plmlarge-scale-piece-wise-linear-model>7 LS-PLM（Large Scale Piece-wise Linear Model）</a><ul><li><a href=#71-背景>7.1 背景</a></li><li><a href=#72-算法原理>7.2 算法原理</a></li><li><a href=#73-优势>7.3 优势</a></li><li><a href=#74-劣势>7.4 劣势</a></li></ul></li><li><a href=#8gbdtlr>8.GBDT+LR</a><ul><li><a href=#81-背景>8.1 背景</a></li><li><a href=#82-算法原理>8.2 算法原理</a></li><li><a href=#83-优势>8.3 优势</a></li><li><a href=#84-劣势>8.4 劣势</a></li></ul></li></ul></nav></div></div><script>document.getElementsByTagName("main")[0].setAttribute("autoTOC","true")</script><article class="page single"><h1 class="single-title animate__animated animate__flipInX">推荐系统前深度学习时代：从协同过滤到 GBDT+LR</h1><div class=post-meta><div class=post-meta-line><span class=post-author><i class="author fas fa-user-circle fa-fw"></i><a href=/ title=Author rel=author class=author>Hari</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2023-09-12>2023-09-12</time>&nbsp;<i class="far fa-edit fa-fw"></i>&nbsp;<time datetime=2023-09-12>2023-09-12</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 4993 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 10 分钟&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#0-推荐算法四要素>0 推荐算法四要素</a></li><li><a href=#1-协同过滤collaborative-filtering-cf>1 协同过滤（Collaborative Filtering, CF）</a><ul><li><a href=#11-user-based-cf>1.1 User-based CF</a></li><li><a href=#12-item-based-cf>1.2 Item-based CF</a></li><li><a href=#13-user-based-cf-和-item-based-cf-比较>1.3 User-based CF 和 Item-based CF 比较</a></li><li><a href=#14-优势>1.4 优势</a></li><li><a href=#15-劣势>1.5 劣势</a></li></ul></li><li><a href=#2-矩阵分解>2 矩阵分解</a><ul><li><a href=#21-背景>2.1 背景</a></li><li><a href=#22-算法原理>2.2 算法原理</a></li><li><a href=#23-优势>2.3 优势</a></li><li><a href=#24-劣势>2.4 劣势</a></li></ul></li><li><a href=#3-lrlogistic-regression>3 LR（logistic regression）</a><ul><li><a href=#31-背景>3.1 背景</a></li><li><a href=#32-算法原理>3.2 算法原理</a></li><li><a href=#33-优势>3.3 优势</a></li><li><a href=#34-劣势>3.4 劣势</a></li></ul></li><li><a href=#4-poly2polynomial-regression-of-degree-2>4 Poly2（Polynomial Regression of degree 2）</a><ul><li><a href=#41-背景>4.1 背景</a></li><li><a href=#42-算法原理>4.2 算法原理</a></li><li><a href=#43-优势>4.3 优势</a></li><li><a href=#44-劣势>4.4 劣势</a></li></ul></li><li><a href=#5-fmfactorization-machines>5 FM（factorization machines）</a><ul><li><a href=#51-背景>5.1 背景</a></li><li><a href=#52-优势>5.2 优势</a></li><li><a href=#53-劣势>5.3 劣势</a></li></ul></li><li><a href=#6-ffmfiled-awareness-factorization-machines>6 FFM（filed-awareness factorization machines）</a><ul><li><a href=#61-背景>6.1 背景</a></li><li><a href=#62-算法原理>6.2 算法原理</a></li><li><a href=#63-优势>6.3 优势</a></li><li><a href=#64-劣势>6.4 劣势</a></li></ul></li><li><a href=#7-ls-plmlarge-scale-piece-wise-linear-model>7 LS-PLM（Large Scale Piece-wise Linear Model）</a><ul><li><a href=#71-背景>7.1 背景</a></li><li><a href=#72-算法原理>7.2 算法原理</a></li><li><a href=#73-优势>7.3 优势</a></li><li><a href=#74-劣势>7.4 劣势</a></li></ul></li><li><a href=#8gbdtlr>8.GBDT+LR</a><ul><li><a href=#81-背景>8.1 背景</a></li><li><a href=#82-算法原理>8.2 算法原理</a></li><li><a href=#83-优势>8.3 优势</a></li><li><a href=#84-劣势>8.4 劣势</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>[toc]</p><p>本人身处广告营销领域，广告系统本质是个推荐系统，本文旨在梳理相关推荐算法。《深度学习推荐系统》是一本很不错的入门书籍，很系统，不过很多地方写得不太清楚，阅读过程查阅了大量资料。这篇文章作为开篇，会写一个系列，目的是将推荐领域相关核心概念和算法全覆盖。</p><h2 id=0-推荐算法四要素 class=headerLink><a href=#0-%e6%8e%a8%e8%8d%90%e7%ae%97%e6%b3%95%e5%9b%9b%e8%a6%81%e7%b4%a0 class=header-mark></a>0 推荐算法四要素</h2><p>如果把推荐系统看作一个函数，可以表达为 $F(i,c,u)$，其中 $i$ 为 item 即待推荐物品，$c$ 为 context 即当前上下文（时间、地理等），$u$ 为 user 即用户，$F$ 记为推荐算法。</p><p>整个算法演进就在围绕这四个要素做文章。</p><h2 id=1-协同过滤collaborative-filtering-cf class=headerLink><a href=#1-%e5%8d%8f%e5%90%8c%e8%bf%87%e6%bb%a4collaborative-filtering-cf class=header-mark></a>1 协同过滤（Collaborative Filtering, CF）</h2><p>推荐领域最经典的算法之一是协同过滤，该算法是 Xerox 发明，后在 2003 年被 Amazon 发扬光大。该算法将用户和物品构成一个大矩阵，每行是一个用户对每个物品的态度（喜欢 or 不喜欢），这个矩阵就能刻画每个用户对物品的喜好程度，该矩阵成为共现矩阵。</p><p>该算法分为 User-Based CF 和 Item-Based CF。</p><h3 id=11-user-based-cf class=headerLink><a href=#11-user-based-cf class=header-mark></a>1.1 User-based CF</h3><p>站在用户角度，针对用户 A，计算与其喜好相似的 topN 用户；然后遍历每个物品，让这些与自己相似的用户针对这个物品投票，计算出每个物品得分；最后按照每个物品得分排序，排名最高的几个物品就可以推荐给用户了。这里有两个关键点：</p><ol><li>用户相似度计算，可以取出每个用户在共现矩阵对应的行向量，与目标用户向量计算余弦相似度，找出 topN。</li><li>物品得分计算 $Ru,i=\frac{\sum(w_{u,s}R_{s,i})}{\sum{w_{u,s}}}$，其中 $w_{u,s}$ 是用户 u 和 s 的相似度，$R_{s,i}$ 是用户 s 对物品 i 的喜好程度作为投票的权重，$Ru,i$ 即为用户 u 对物品 i 的喜好程度。</li></ol><p>获得 u 对全部物品喜好后，排序即可得推荐列表。</p><h3 id=12-item-based-cf class=headerLink><a href=#12-item-based-cf class=header-mark></a>1.2 Item-based CF</h3><p>站在物品角度，沿用前面的共现矩阵，这次从列角度思考，计算两两列向量的相似度，得到一个对称方阵（只看上三角或下三角即可）。</p><p>当一个用户进站访问时，在共现矩阵里查看其对应的行向量，确定其之前表达过喜好的物品，然后针对每个喜欢过的物品到上面物品相似度矩阵查找最相似的 topK 物品，然后对这 K 个物品计算喜好得分，计算公式为 $R_{u,i}=\sum{w_{j,i}R_{u,j}}$，其中 i 是要确定用户喜好程度的物品， j 是用户 u 表达喜欢过的物品，$w_{j,i}$ 是物品 j 和 i 相似度，$R_{u,j}$ 是用户 u 对 j 的喜好程度。</p><h3 id=13-user-based-cf-和-item-based-cf-比较 class=headerLink><a href=#13-user-based-cf-%e5%92%8c-item-based-cf-%e6%af%94%e8%be%83 class=header-mark></a>1.3 User-based CF 和 Item-based CF 比较</h3><ol><li>User-based CF 适合社交类场景，朋友喜欢的自己大概率也喜欢，很符合直觉。但是一般这类场景用户多余物品而且持续增长的话，由于每次都要计算相似用户所以共现矩阵存储和维护是个问题；另外就是针对获取难的物品，比如价格较贵的酒店、奢侈品等，会导致用户对应的向量很稀疏。</li><li>Item-based CF 适合兴趣较为稳定的场景，比如电商和电影，用户一段时间内倾向寻找某几类爱好的物品。业界 item-based CF 用得较多。</li></ol><h3 id=14-优势 class=headerLink><a href=#14-%e4%bc%98%e5%8a%bf class=header-mark></a>1.4 优势</h3><p>CF 非常直观，可解释强。</p><h3 id=15-劣势 class=headerLink><a href=#15-%e5%8a%a3%e5%8a%bf class=header-mark></a>1.5 劣势</h3><ol><li>CF 泛化能力差，只用了用户和物品信息，上下文信息未用起来。另外因为它无法将两个物品相似这一信息推广到其它物品相似度计算上，两两相似性无法传递。</li><li>热门物品（大家都喜欢的物品）具有很强的头部效应，尾部物品由于向量稀疏，即使相似度很高但是无法识别出来，最后算出来的最相近物品都是热门物品。</li></ol><h2 id=2-矩阵分解 class=headerLink><a href=#2-%e7%9f%a9%e9%98%b5%e5%88%86%e8%a7%a3 class=header-mark></a>2 矩阵分解</h2><h3 id=21-背景 class=headerLink><a href=#21-%e8%83%8c%e6%99%af class=header-mark></a>2.1 背景</h3><p>为了解决 CF 泛化能力差、无法推广物品相似度的问题，矩阵分解技术被提出，2006 年在 netflix 举办的算法竞赛中大放异彩。</p><h3 id=22-算法原理 class=headerLink><a href=#22-%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86 class=header-mark></a>2.2 算法原理</h3><p>针对共现矩阵 $A_{mxn}$，通过矩阵分解技术（奇异值分解 svd、梯度下降等）得到两个矩阵之积：
$$A_{mxn}=U_{m,k}*V_{k,n}$$
其中 U 为用户矩阵，每一行代表一个用户隐向量； V 为物品矩阵，每一列代表一个物品隐向量；k 是一个超参数，越小隐向量表达能力越弱但则泛化能力越强。分解后得到的两个矩阵基于整个共现矩阵分解得到，不再稀疏。</p><p>由于奇异值分解适用于稠密矩阵，但是推荐场景下一般矩阵都非常稀疏所以更常用的是梯度下降，梯度是怎么引入的呢？</p><p>我们知道共现矩阵用户 u 针对物品 i 有个喜好值记为 $R_{u,i}$，矩阵分解后（假设已经分解完成了）的用户矩阵对应用户 u 的隐向量记为 $X_u$，物品矩阵对应物品 i 的隐向量记为 $Y_i$，则我们期望
$$X_uY_{i}=R_{u,i}$$
这个只是期望，左边能尽可能逼近右边我们在工程上就满足了。针对全部用户 u, i 我们可以得到针对全局的最优期望公式即均方误差为：
$$min\sum{(r_{u,i}-X_uY_i)^2}$$
基于上述公式针对 X 和 Y 求导，然后沿着梯度下降方向迭代 X 和 Y 即可。工程实现上为了避免过拟合，会在上述均方误差公式后面增加一个正则化项，关于正则化是个大话题先不展开，记得它存在目的是为了减少过拟合即可。</p><p>得到用户矩阵和物品矩阵后，针对每个用户，计算它和每个用户向量的内积即可得到喜好程度，排序即可地推荐列表。</p><h3 id=23-优势 class=headerLink><a href=#23-%e4%bc%98%e5%8a%bf class=header-mark></a>2.3 优势</h3><ol><li>相比 CF 泛化能力更强，一定程度解决了数据稀疏问题。</li><li>分解得到的用户隐向量和物品隐向量其实是一种变相的 embedding，这个结果便于和其它特征进行组合可以与深度学习无缝拼接，后面介绍深度学习类推荐算法时候会重提这一点。</li></ol><h3 id=24-劣势 class=headerLink><a href=#24-%e5%8a%a3%e5%8a%bf class=header-mark></a>2.4 劣势</h3><p>同 CF 一样，只用到了用户和物品两项信息，上下文没有利用起来。从下个算法开始将会重点解决特征利用不充分问题。</p><blockquote><p>过渡说明</p><blockquote><p>接下来描述的每个算法的输入都是一个样本矩阵，其中每一行为一个样本，每一列为一个特征维度；输出是一个概率值，作为点击率。</p></blockquote></blockquote><h2 id=3-lrlogistic-regression class=headerLink><a href=#3-lrlogistic-regression class=header-mark></a>3 LR（logistic regression）</h2><h3 id=31-背景 class=headerLink><a href=#31-%e8%83%8c%e6%99%af class=header-mark></a>3.1 背景</h3><p>Logistic 回归最早是在 20 世纪 50 年代为了解决生物统计问题而被提出的。</p><h3 id=32-算法原理 class=headerLink><a href=#32-%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86 class=header-mark></a>3.2 算法原理</h3><p>公式如下：
$$\widehat{y}=sigmoid(\sum{w_ix_i})=\frac{1}{1+e^{-\sum{w_ix_i}}}$$
公式说明：</p><p>$\widehat{y}$ 为针对某个样本的预测输出，$x_i$ 为前述样本的第 $i$ 特征取值，$w_i$ 为该特征对应的权重。需要说明的有两点：</p><ol><li>$x_i$ 为标量</li><li>$w_i$ 也为标量，它的值为学习对象</li></ol><h3 id=33-优势 class=headerLink><a href=#33-%e4%bc%98%e5%8a%bf class=header-mark></a>3.3 优势</h3><ol><li>输出就是一个 $(0, 1)$之间的数，符合直觉，可解释性强。</li><li>工程化简单，尤其针对海量数据。</li></ol><h3 id=34-劣势 class=headerLink><a href=#34-%e5%8a%a3%e5%8a%bf class=header-mark></a>3.4 劣势</h3><ol><li>仅能捕获特征之间的线性关系，不能很好地表示特征之间的交互。</li><li>为了改善预测效果，需要做大量的手工特征工程。</li></ol><h2 id=4-poly2polynomial-regression-of-degree-2 class=headerLink><a href=#4-poly2polynomial-regression-of-degree-2 class=header-mark></a>4 Poly2（Polynomial Regression of degree 2）</h2><h3 id=41-背景 class=headerLink><a href=#41-%e8%83%8c%e6%99%af class=header-mark></a>4.1 背景</h3><p>为了捕获特征之间的交互关系，业界引入了 Poly2 算法。</p><p>多项式核函数（包括二次多项式核）在支持向量机（SVM）的发展过程中变得非常重要。SVM 最初由 Vladimir Vapnik 和 Alexey Chervonenkis 在1960年代末到1970年代初提出。多项式核函数并不仅限于二次形式，也可以有更高阶的形式，但二次多项式核（Poly2）在许多应用中是很实用的。</p><h3 id=42-算法原理 class=headerLink><a href=#42-%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86 class=header-mark></a>4.2 算法原理</h3><p>公式如下：
$$\widehat{y_i}=sigmoid(w_0+\sum{w_ix_i}+\sum\sum{w_{ij}x_ix_j})$$
公式说明：</p><p>相比 LR 增加了一个二阶项，这个部分负责进行特征交叉。其中：</p><ol><li>$w_i$、$x_i$ 同 LR 算法说明。</li><li>$w_{ij}$ 是一个新增加的待学习标量参数，它负责学习 $x_i$ 和 $x_j$ 之间的交互关系。</li><li>$x_ix_j$ 可以看作两个特征之间的哈达玛乘积。</li></ol><h3 id=43-优势 class=headerLink><a href=#43-%e4%bc%98%e5%8a%bf class=header-mark></a>4.3 优势</h3><ol><li>能够捕获特征之间的二阶交互。</li><li>通过特征交互，可以建模非线性关系。</li></ol><h3 id=44-劣势 class=headerLink><a href=#44-%e5%8a%a3%e5%8a%bf class=header-mark></a>4.4 劣势</h3><ol><li>参数多计算复杂度高 $O(d^2)$，其中 $d$ 是特征个数。</li><li>由于参数太多，在数据稀疏场景下容易过拟合。</li><li>只能捕获二阶交互，更高阶的无法捕获。</li></ol><h2 id=5-fmfactorization-machines class=headerLink><a href=#5-fmfactorization-machines class=header-mark></a>5 FM（factorization machines）</h2><h3 id=51-背景 class=headerLink><a href=#51-%e8%83%8c%e6%99%af class=header-mark></a>5.1 背景</h3><p>为了解决 poly2 在稀疏数据集容易过拟合以及计算量过大的问题而提出。</p><p>这个算法是由 Steffen Rendle 在2010年左右提出的。它融合了矩阵分解（例如SVD）和线性回归模型的特性，以有效地处理高维稀疏数据和捕捉特征之间的交互。
公式如下</p><p>$$\widehat{y_i}=sigmoid(w_0+\sum{w_ix_i}+\sum\sum{&lt;v_i,v_j>x_ix_j})$$
公式说明</p><p>形式同 poly2，差别在二阶项。其中：</p><ol><li>$w_i$、$x_i$ 同 LR 算法说明。</li><li>$v_i$ 和 $v_j$ 是一个新增加的待学习向量参数，它们叫做隐向量，长度为 $k$，$k$ 也是一个超参数，大小远远小于特征个数 $d$。</li><li>$x_ix_j$ 可以看作两个特征之间的哈达玛乘积。</li></ol><p>poly2 是学习任何两个特征值之间的交互关系，而 fm 在全样本中学习每个特征对应的隐向量，这使得 fm 有更好的泛化性，而且参数量大幅减小（从 $d^2$ 降为 $kd$）。</p><h3 id=52-优势 class=headerLink><a href=#52-%e4%bc%98%e5%8a%bf class=header-mark></a>5.2 优势</h3><ol><li>可以捕获任意阶数的特征交互，尽管在实践中通常只计算到二阶。</li><li>通过参数共享，避免了 poly2 参数过多的问题，同时让模型泛化性更好。</li><li>相比 poly2 在稀疏数据集上表现更好，具体来说就是针对某个特征，其它特征都为它的隐向量产生贡献了力量，而且这种力量在其它特征与自己交叉时共享。</li></ol><h3 id=53-劣势 class=headerLink><a href=#53-%e5%8a%a3%e5%8a%bf class=header-mark></a>5.3 劣势</h3><ol><li>尽管计算复杂度相比 poly2 降低，但仍然较大，特别随着隐向量维度 $k$ 变大的时候。</li><li>多了一个超参数也就是 $k$ 的调整需求。</li></ol><h2 id=6-ffmfiled-awareness-factorization-machines class=headerLink><a href=#6-ffmfiled-awareness-factorization-machines class=header-mark></a>6 FFM（filed-awareness factorization machines）</h2><h3 id=61-背景 class=headerLink><a href=#61-%e8%83%8c%e6%99%af class=header-mark></a>6.1 背景</h3><p>FFM 算法是对 FM 算法的扩展，从名字就能看出来。广告点击率预测是一个利益驱动的事情，在大体量下（比如谷歌一年收入万亿）点击率预估稍微提升几个点带来的收益巨大。</p><p>FFM 在 FM 基础上引入了域（filed）的概念，隐向量不在共建共用，而是针对每个交互特征独立构建了一个隐向量，这类似 poly2 不过这里用的向量而非标量权重。</p><h3 id=62-算法原理 class=headerLink><a href=#62-%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86 class=header-mark></a>6.2 算法原理</h3><p>公式如下</p><p>$$\widehat{y_i}=sigmoid(w_0+\sum{w_ix_i}+\sum\sum{&lt;v_{i,\text{filed}(j)},v_{j,\text{filed}(i)}>x_ix_j})$$</p><p>公式说明</p><p>形式同 poly2 和 fm，差别在二阶项，ffm 的二阶项相当于在 fm 基础上融合了 poly2。下面详细解释：</p><ol><li>$w_i$、$x_i$ 同 LR 算法说明。</li><li>$v_{i,\text{filed}(j)}$ 和 $v_{j,\text{filed}(i)}$ 是一个新增加的待学习向量参数，它们类似 fm 中引入的隐向量，不过粒度更细化了。fm 中引入的隐向量，针对每个特征只有一个；而 ffm 相当于为每个特征学习了一组隐向量，而不是一个。这里相当于 poly2 为两两特征学习了独立的交互关系，不过不同于 poly2 只用一个标量权重，ffm 用的是向量。每个隐向量长度为 $k$，$k$ 也是一个超参数，大小远远小于特征个数 $d$。</li><li>$x_ix_j$ 可以看作两个特征之间的哈达玛乘积。</li><li>ffm 参数量为 $kd^2$，参数量和计算量都非常大。</li></ol><h3 id=63-优势 class=headerLink><a href=#63-%e4%bc%98%e5%8a%bf class=header-mark></a>6.3 优势</h3><p>相比 fm 更加注重两两特征之间的独立关系</p><h3 id=64-劣势 class=headerLink><a href=#64-%e5%8a%a3%e5%8a%bf class=header-mark></a>6.4 劣势</h3><ol><li>参数量和计算量以及存储开销相比 fm 增加非常多。</li><li>稀疏数据集容易过拟合。</li></ol><h2 id=7-ls-plmlarge-scale-piece-wise-linear-model class=headerLink><a href=#7-ls-plmlarge-scale-piece-wise-linear-model class=header-mark></a>7 LS-PLM（Large Scale Piece-wise Linear Model）</h2><h3 id=71-背景 class=headerLink><a href=#71-%e8%83%8c%e6%99%af class=header-mark></a>7.1 背景</h3><p>这个算法单纯是对 LR 算法的改进，不再像原始 LR 一样无差别应用，而是考虑到业务场景的差异化做得优化，这个优化思想在东方国家比较罕见。该算法在 2012 年便成为阿里巴巴内部最核心的 CTR 预估算法了。</p><h3 id=72-算法原理 class=headerLink><a href=#72-%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86 class=header-mark></a>7.2 算法原理</h3><p>公式如下</p><p>$$p(y=1|x)=\sum_{i=0}^{m}softmax(u_i x)LR(w_i x)=\sum_{i=0}^{m}\frac{e^{u_i x}}{\sum_{j=0}^{m}{e^{u_i x}}}\frac{1}{1+e^{-w_i x}}$$</p><p>该算法简单说就是主观上认为样本应该可以分为若干类，但不是把每个样本只划分为到某一类中，而是认为每个样本属于哪个分类都是有一定概率的（这个在结构上有点 attention 的意思），针对样本在每个分类上都用该分类所属的参数算一下 LR，然后将各个分类的 LR 结果做加权平均（这个相当于每个类别投票，不过投票权利不同）。这个算法也被叫做 MLR（Mixed-LR），不过我觉得这个叫法不足以反映投票加权的本质。</p><p>公式中 $u_i$ 是分类 $i$ 的权重，$w_i$ 是分类 $i$ 的 LR 参数，$x$ 是样本特征向量。输出即为 $y=1$ 的概率。</p><h3 id=73-优势 class=headerLink><a href=#73-%e4%bc%98%e5%8a%bf class=header-mark></a>7.3 优势</h3><ol><li>端到端非线性学习。通过分类投票，可以捕获特征之间的非线性交互，不用人工设计特征交叉。</li><li>可以分布式并行。</li><li>稀疏性好，也就是非零参数少，这对在线推理系统很重要因为计算量小。稀疏性好的原因是在目标函数（本文未展示）上用了 $L_{2,1}$ 正则化。</li></ol><h3 id=74-劣势 class=headerLink><a href=#74-%e5%8a%a3%e5%8a%bf class=header-mark></a>7.4 劣势</h3><p>相比后来的深度学习，该算法在更深层次的特征交叉以及序列特征使用上是不足的。</p><h2 id=8gbdtlr class=headerLink><a href=#8gbdtlr class=header-mark></a>8.GBDT+LR</h2><h3 id=81-背景 class=headerLink><a href=#81-%e8%83%8c%e6%99%af class=header-mark></a>8.1 背景</h3><p>为了提升模型效果，要么像前面的 lr、poly2、fm、ffm 一样进行手工或者半自动地进行特征组合和筛选，要么优化目标函数引入特征交叉项提升特征组合能力。</p><p>我们看到从 LR 到 FFM，这一路进化，核心目标函数并没有变化，进化只是表现在如何更好地进行特征交叉。</p><p>2014 年 Facebook 回到原点，提出了 GBDT+LR 组合算法模型。</p><p>GBDT（Gradient-Bootsted Discision Tree） 不会在本篇文章深入介绍，这个算法本质是用一堆决策树去逼近目标，但是要介绍下名字。我感觉能跟这个名字一拼的是 LSTM（Long short-term memory），后者好歹还有个短杠区分下 long short 分别修饰谁（short 修饰 term，long 修饰 short-term 即更长的短期），GBDT 前面的 GB 梯度提升比较误导人，算法本质是基于梯度下降寻找下个决策树（下个决策树拟合前序决策树们遗留的残差），这里的梯度下降用 G 表示，多个决策树一起协同用 B 表示。GBDT 是 Boosting 算法的一种，还有另一个流派 Bagging， 后续文章介绍。</p><h3 id=82-算法原理 class=headerLink><a href=#82-%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86 class=header-mark></a>8.2 算法原理</h3><p>这个算法分为两步：</p><ol><li>第一步先将原始样本输入 GBDT，这一步虽然也在做目标拟合，但新增了一个项目，就是将全部树的输出进行向量化然后串联起来作为新的特征（有点 embedding 的意思了）。</li><li>前一步每一颗树的叶子将被量化为 one-hot 向量（只有命中的叶子才是 1，其它叶子都是 0），然后依次将各个树对应的 one-hot 向量串联起来构成输入样本对应的 embedding 向量，该向量作为 LR 输入，后续同 LR 算法处理。</li></ol><h3 id=83-优势 class=headerLink><a href=#83-%e4%bc%98%e5%8a%bf class=header-mark></a>8.3 优势</h3><ol><li>LR 的优势，符合直觉，计算简单。</li><li>GBDT 可以捕捉特征之间的交互关系，而且是自动找到这些交互，并且实现了非线性映射，这使得模型泛化能力更强。</li></ol><h3 id=84-劣势 class=headerLink><a href=#84-%e5%8a%a3%e5%8a%bf class=header-mark></a>8.4 劣势</h3><ol><li>训练复杂性增加，先训练 gbdt 后训练 LR，这也增加了模型部署和维护复杂性。</li><li>推理速度相比单个模型可能较慢。</li><li>gbdt 训练复杂导致模型可能过拟合。</li></ol><p>&ndash;end&ndash;</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2023-09-12</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md><span><a class=link-to-mardown href=/rs-algo-pre-deeplearning/index.md target=_blank rel="noopener noreferrer">阅读原始文档</a></span></div><div class=post-info-share><span><a href=# title="分享到 Twitter" data-sharer=twitter data-url=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/ data-title="推荐系统前深度学习时代：从协同过滤到 GBDT+LR" data-via=haricheung data-hashtags=推荐系统,深度学习,机器学习><i class="fab fa-twitter fa-fw"></i></a><a href=# title="分享到 Facebook" data-sharer=facebook data-url=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/ data-hashtag=推荐系统><i class="fab fa-facebook-square fa-fw"></i></a><a href=# title="分享到 WhatsApp" data-sharer=whatsapp data-url=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/ data-title="推荐系统前深度学习时代：从协同过滤到 GBDT+LR" data-web><i class="fab fa-whatsapp fa-fw"></i></a><a href=# title="分享到 Line" data-sharer=line data-url=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/ data-title="推荐系统前深度学习时代：从协同过滤到 GBDT+LR"><i data-svg-src=/lib/simple-icons/icons/line.min.svg></i></a><a href=# title="分享到 微博" data-sharer=weibo data-url=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/ data-title="推荐系统前深度学习时代：从协同过滤到 GBDT+LR"><i class="fab fa-weibo fa-fw"></i></a><a href=# title="分享到 Myspace" data-sharer=myspace data-url=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/ data-title="推荐系统前深度学习时代：从协同过滤到 GBDT+LR" data-description><i data-svg-src=/lib/simple-icons/icons/myspace.min.svg></i></a><a href=# title="分享到 Blogger" data-sharer=blogger data-url=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/ data-title="推荐系统前深度学习时代：从协同过滤到 GBDT+LR" data-description><i class="fab fa-blogger fa-fw"></i></a><a href=# title="分享到 Evernote" data-sharer=evernote data-url=https://chienlungcheung.github.io/rs-algo-pre-deeplearning/ data-title="推荐系统前深度学习时代：从协同过滤到 GBDT+LR"><i class="fab fa-evernote fa-fw"></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/>推荐系统</a>,&nbsp;<a href=/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/>深度学习</a>,&nbsp;<a href=/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/>机器学习</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/socialization/ class=prev rel=prev title=社会化：当我们在讨论原生家庭时，我们在讨论什么><i class="fas fa-angle-left fa-fw"></i>社会化：当我们在讨论原生家庭时，我们在讨论什么</a>
<a href=/complexity-and-how-to-deal-with-it/ class=next rel=next title=复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么>复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么<i class="fas fa-angle-right fa-fw"></i></a></div></div><div id=comments><div id=disqus_thread class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://disqus.com/?ref_noscript>Disqus</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreferrer" title="Hugo 0.120.1">Hugo</a> 强力驱动&nbsp;|&nbsp;主题 - <a href=https://github.com/HEIGE-PCloud/DoIt target=_blank rel="noopener noreferrer" title="DoIt 0.3.0"><i class="far fa-edit fa-fw"></i> DoIt</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2019 - 2023</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank rel="noopener noreferrer">Hari</a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div><div class=footer-line></div><div class=footer-line></div></div></footer></div><div id=fixed-buttons><a href=#back-to-top id=back-to-top-button class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><div class=assets><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=preload as=style onload='this.onload=null,this.rel="stylesheet"' href=/lib/katex/copy-tex.min.css><noscript><link rel=stylesheet href=/lib/katex/copy-tex.min.css></noscript><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:10},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{distance:100,findAllMatches:!1,fuseIndexURL:"/index.json",highlightTag:"em",ignoreFieldNorm:!1,ignoreLocation:!1,isCaseSensitive:!1,location:0,maxResultLength:10,minMatchCharLength:2,noResultsFound:"没有找到结果",snippetLength:50,threshold:.3,type:"fuse",useExtendedSearch:!1},sharerjs:!0,table:{sort:!0}}</script><script type=text/javascript src=https://Chienlung.disqus.com/embed.js defer></script><script type=text/javascript src=/lib/tablesort/tablesort.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js defer></script><script type=text/javascript src=/lib/katex/auto-render.min.js defer></script><script type=text/javascript src=/lib/katex/copy-tex.min.js defer></script><script type=text/javascript src=/lib/katex/mhchem.min.js defer></script><script type=text/javascript src=/js/katex.min.js defer></script><script type=text/javascript src=/js/theme.min.js defer></script></div></body></html>