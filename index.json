[{"categories":null,"content":"上一篇讲了 leveldb 中 Table 的设计和实现, 它是磁盘 sstable 文件的内存形式, 但是 Table 在实际中不会被用户直接用到, 而是借助 TableCache. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#"},{"categories":null,"content":"1 TableCache: leveldb 的磁盘文件缓存结构(代码位于 db/table_cache.h) 磁盘上每个 sstable 文件都对应一个 Table 实例, TableCache 是一个用于缓存这些 Table 实例的缓存. 或者这么说, sstable 文件被加载到内存后, 被缓存到 TableCache 中. 每次用户进行查询操作的时候(即调用DBImpl::Get())可能需要去查询磁盘上的文件(即未在 memtable 中查到), 这就要求有个缓存功能来加速. TableCache 会缓存 sstable 文件对应的 Table 实例, 用于加速用户的查询, 否则每次读文件解析就很慢了. 目前在用的缓存策略是 LRU 以防内存占用过大. 每个 db 实例都会持有一个 TableCache 实例, 对该缓存的的填充是通过副作用实现的, 即当外部调用 DBImpl::Get()-\u003eVersion::Get()-\u003eVersionSet::table_cache_::Get() 进行查询的时候, 如果发现 sstable 对应 Table 实例不在缓存就会将其填充进来. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#1-tablecache-leveldb-的磁盘文件缓存结构"},{"categories":null,"content":"1.1 概览下面是 TableCache 类构成: class TableCache { public: // 为 file_number 标识的 sstable 文件构造对应的迭代器. Iterator* NewIterator(const ReadOptions\u0026 options, uint64_t file_number, uint64_t file_size, Table** tableptr = nullptr); // 从缓存中查找 internal_key 为 k 的数据项. Status Get(const ReadOptions\u0026 options, uint64_t file_number, uint64_t file_size, const Slice\u0026 k, void* arg, void (*handle_result)(void*, const Slice\u0026, const Slice\u0026)); // 驱逐 file_number 对应的 table 对象 void Evict(uint64_t file_number); private: // 一个基于特定淘汰算法(如 LRU)的 Cache Cache* cache_; }; ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#11-概览"},{"categories":null,"content":"1.2 数据成员TableCache 的核心数据成员就是 Cache(实际使用的是 ShardedLRUCache, 这里不展开, 后文详述). ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#12-数据成员"},{"categories":null,"content":"1.3 方法成员TableCache 的核心方法有三个. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#13-方法成员"},{"categories":null,"content":"1.3.1 查询Get() 方法负责实现从缓存中查找指定 key 的数据项. 若 key 对应的 sstable 文件不在缓存则会根据 file_number 读取文件生成 Table 实例放到缓存中然后再查询(注意这是个副作用), 查到后调用 handle_result 进行处理. 针对该方法的调用链为: DBImpl::Get()-\u003eVersion::Get()-\u003eVersionSet::table_cache_::Get(). ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#131-查询"},{"categories":null,"content":"1.3.2 遍历遍历就要有迭代器, NewIterator() 方法负责生成指定 sstable 文件内容对应的迭代器. 该方法主要用于在 Version::AddIterators() 遍历 level 架构中每一个 sstable 文件时构造对应的迭代器, 这些迭代器加上 memtable 的迭代器, 就能遍历整个数据库的内容了. Version::AddIterators() 会从其 table_cache_ 成员根据 file_number 查找其对应的 Table 对象, 若查到则返回其对应迭代器; 否则加载文件(这是一个副作用)并生成对应的 Table 对象放到 table_cache_ 然后返回新构造的 Table 的 iterator. 注意, 如果 NewIterator() 方法的 tableptr 参数非空, 则设置 *tableptr 指向返回的 iterator 底下的 Table 对象. 返回的 *tableptr 对象由 TableCache 所拥有, 所以用户不要删除它; 只要 iterator 还活着, 该对象就有效. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#132-遍历"},{"categories":null,"content":"1.3.3 删除Evict() 方法负责驱逐某个 sstable 文件对应的 Table 缓存, 它会在 DBImpl::DeleteObsoleteFiles() 删除过期文件时候执行. 这三个方法的具体实现比较简单, 不再详列. 下面重点说一下其最重要的数据成员 – Cache. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#133-删除"},{"categories":null,"content":"1.3.4 修改嘿, 没有这个接口. 由于 leveldb 是一个 append 类型数据库, 它不会做 inplace 修改, 这同时也避免了解决复杂的数据一致性问题. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#134-修改"},{"categories":null,"content":"2 Cache 接口(代码位于 include/leveldb/cache.h) 作为一个缓存定义, 最起码要提供的功能就是增删查, 注意没有改, 缓存只是一个视图, 如果要支持修改可能要引入一系列一致性问题. 该接口的设计还有很重要的一点, 也是 leveldb 设计中很重要的一点, 就是引用计数. Cache 中保存的数据项类型为 Cache::Handle, 具体实现中存在一个数据成员表示计数, 这样可以根据计数进行内存复用或回收. 这一点贯穿了各个重要方法的实现. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#2-cache-接口"},{"categories":null,"content":"2.1 增 // 插入一对 \u003ckey, value\u003e 到 cache 中. virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) = 0; 这个方法的参数列表看着有点吓人. 但最重要的就是前两个参数, 后面的参数简单介绍下: charge 主要用来计算缓存的成本, 也就是内存占用的, 每次插入时候调用方可以将插入的数据字节作为 charge 传进来. deleter 是一个用户针对自己插入的数据定制的清理器, 举个简单的例子(具体代码位于 table_cache.cc): // 一个 deleter, 用于从 Cache 中删除数据项时使用 static void DeleteEntry(const Slice\u0026 key, void* value) { TableAndFile* tf = reinterpret_cast\u003cTableAndFile*\u003e(value); delete tf-\u003etable; delete tf-\u003efile; delete tf; } 比较简单轻量, 就是做内存释放. 那 deleter 啥时候调用呢? Cache 析构的时候. 注意, Insert 的时候, 会给新的数据项设置引用计数. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#21-增"},{"categories":null,"content":"2.2 删 // 如果 cache 包含了 key 对应的映射, 删除之. virtual void Erase(const Slice\u0026 key) = 0; 注意, 因为引用计数的存在, Erase 可能不会发生物理删除, 除非数据项对应引用计数变为 0. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#22-删"},{"categories":null,"content":"2.3 查 // 如果 cache 中没有针对 key 的映射, 返回 nullptr. // 其它情况返回对应该映射的 handle. virtual Handle* Lookup(const Slice\u0026 key) = 0; 查询本身没有什么可讲的, 要重点说的还是引用计数相关, 针对查询到的数据项, 要递增其引用计数防止被其它客户端删除. 而这也引申出另一个需求, 就是当调用 Lookup 调用方不再使用数据项的时候, 需要主动调用 Release(handle) 来将引用计数减一. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#23-查"},{"categories":null,"content":"2.4 其它除了上面三个核心方法. Cache 还涉及下面几点: Cache 对象不支持任何拷贝(用技术语言讲就是它的拷贝构造和赋值构造都被禁掉了), 一个实例只能有一个副本存在, 这样做就省掉了维护各个 Cache 副本一致性的问题. 除了显式地删除, Cache 还有一个 Prune() 方法, 用于移除缓存中全部不再活跃的数据项, 具体取决于 Cache 具体实现, 以 LRUCache 为例就是将不在 in_use_ 链表中的数据项删除. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#24-其它"},{"categories":null,"content":"3 ShardedLRUCache 实现(代码位于 util/cache.cc) 前文提到的 Cache 是个抽象类, leveldb 通过 ShardedLRUCache 继承并实现了相关功能. 该类位于一个匿名 namespace, 通过下面的方法暴露相关功能: Cache* NewLRUCache(size_t capacity) { return new ShardedLRUCache(capacity); } (熟悉 golang 的同学可能看了会会心一笑, 真说不准后来的 golang 相关写法沿用了 Google 在 C++ 代码里的习惯.) 要理解这个类, 先从名字开始, 从右到左关键词分别是 LRU, Sharded. 这两个关键词说清楚了实现的关键点: 缓存基于 LRU 算法做淘汰 缓存支持 sharding 即分片 下面从顶向下来描述相关设计与实现. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#3-shardedlrucache-实现"},{"categories":null,"content":"3.1 ShardingSharding 算法一般有两种, 基于 hash 的或者基于 range 的, 这里是基于 hash 的. 不知道大家可发现这里有个问题. 就是 ShardedLRUCache 为何要进行 sharding, 明明这个类的一个实例只存在于一个节点的内存里(这么说有点绕但为了尽可能严谨先这么表达了. 换个不严格的问法就是 leveld 非分布式, ShardedLRUCache 实例也只在一个机器上, 为啥还要搞成分片的?)? 文档和代码里没有说明, 但我觉得这个问题值得思考一下. Sharding 最明显的目的就是分摊压力到各个 shard, 要么是分摊存储压力(多节点, 每个节点不承担一部分存储), 要么是分摊计算压力(多节点, 每个节点承担一部分计算), 总之这个在分布式环境下比较容易理解. 这里如此设计目的我认为是后者, 即计算压力, 更明确地讲是避免针对 ShardedLRUCache 锁粒度过大导致访问变为串行. 每个 shard 持有各自的锁, 这样可以尽可能地实现并行处理. 这里的 sharding 实质上是实现了 Java 里的 Lock Striping. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#31-sharding"},{"categories":null,"content":"3.1.1 sharding 存储ShardedLRUCache 有一个成员, 它包含一个 shard 数组, 每个 shard 就是一个 LRUCache: LRUCache shard_[kNumShards]; 每个 cache 默认 16 个 shards. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#311-sharding-存储"},{"categories":null,"content":"3.1.2 增插入数据的时候, 先计算 hash, 然后基于 hash 寻找对应 shard(即 LRUCache) 做真正插入操作: virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) { // 计算 hash const uint32_t hash = HashSlice(key); // 基于 hash 做 sharding return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#312-增"},{"categories":null,"content":"3.1.3 删和插入数据类似, 先计算 hash, 定位到所在 shard, 然后再做具体删除操作: virtual void Erase(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#313-删"},{"categories":null,"content":"3.1.4 查与插入数据类似, 先计算 hash, 定位到可能所在的 shard, 然后再做具体查询操作: virtual Handle* Lookup(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#314-查"},{"categories":null,"content":"3.2 LRUCache前面介绍 sharding 的时候提到了 LRUCache, 它是每个 shard 的实际形态. 虽然它叫 XXCache, 但该类并不是 Cache 接口的实现(虽然干的活其实差不多)但也无所谓. 由于它是 ShardedLRUCache 的实际存储, 所以相关增删查方法都有, 这些根据字面就基本知道干啥了, 不再细说, 我们要特别说说跟 LRU 相关的部分. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#32-lrucache"},{"categories":null,"content":"3.2.1 存储成员该类包括两个循环链表, 一个是 in_use_ 链表, 一个是 lru_ 链表, 同时还有一个用于快速查询数据项是否存在的哈希表 table_. 其中: in_use_ 存的是在用的数据项 lru_ 存的是可以淘汰(以节省空间, 调用 prune() 方法会将该链表清空)也可以重新提升到 in_use_ 中的数据项 table_ 是 leveldb 自己实现的一个哈希表(当时测试随机读比 g++ 4.4.3 版本的内置 hashtable 快了大约 5%), 存储了出现在 in_use_ 和 lru_ 中的全部数据项(用于快速判断某个数据项是否在 LRUCache 中). 注意它保存了前面提到的两个链表的数据, 如果响应用户查询时发现数据项在 lru_ 中则会自动将其提升到 in_use_ 链表中. 不管是链表还是哈希表, 存储的数据项都是 LRUHandle, 它是个变长数据结构, 有必要展示下其核心部分: struct LRUHandle { void* value; void (*deleter)(const Slice\u0026, void* value); // 下面这个成员专用于在哈希表中指向与自己同一个桶中的后续元素 LRUHandle* next_hash; // 下面两个成员用于 in_use_ 或 lru_ 链表 LRUHandle* next; LRUHandle* prev; // TODO(可选): 当前 charge 大小只允许 uint32_t size_t charge; size_t key_length; // 指示该数据项是否还在 cache 中. bool in_cache; // 引用计数, 包含客户端引用数以及 cache 对该数据项引用数(1). uint32_t refs; // 基于 key 的 hash, 用于 sharding 和比较. uint32_t hash; // key 的起始字符, 注意这个地方有个 trick, // 因为 key 本来是变长的, 所以这里需要 // 将 key_data 作为本数据结构最后一个元素, // 方便构造时根据 key 实际大小延伸. char key_data[1]; Slice key() const { // 仅当当前 LRUHandle 作为一个空列表的 dummy head 时, 下面的 // 断言才不成立. dummy head 不保存任何数据. assert(next != this); return Slice(key_data, key_length); } }; 几个关键点: 为啥有 next_hash? 由于 LRUCache 若仍在缓存则必定被哈希表和其中一个链表(in_use_ 或 lru_)引用, 且哈希表是基于桶的, 所以一个 next 成员就不够用了, 所以专门定义了一个 next_hash 用于哈希表桶里面的链接. 为啥单独保存 key? 变长部分就是保存 key 的 key_data. 这里有个疑问, 为啥 value 用的指针, 而这里要搞个变长结构单独保存一遍 key? 通过查看 Cache::Lookup(key) 的调用可以发现, 外部传入的 key 都是栈上维护的临时变量, 插入 cache 的时候就需要保存, 于是就有了这里的节省内存的变长结构. 引用计数 refs, 它的重要意义前面讲过了, 引用计数设计贯穿了整个 leveldb 的实现, 不再说了. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#321-存储成员"},{"categories":null,"content":"3.2.2 LRU 算法如何生效的LRUCache 的 prune() 方法会直接清除 lru_ 链表内容, 无差别地清除, 所以这里未体现 LRU 思想, 那在哪儿体现的呢? 在 insert() 方法里. 在 insert() 方法最后有这么一段代码(我是真不喜欢这类有副作用的设计): ... // 如果本 shard 的内存使用量大于容量并且 lru_ 链表不为空, // 则从 lru_ 链表里面淘汰数据项(lru_ 链表数据当前肯定未被使用), // 直至使用量小于容量或者 lru_ 清空. while (usage_ \u003e capacity_ \u0026\u0026 lru_.next != \u0026lru_) { // 这很重要, lru_.next 是 least recently used 的元素 LRUHandle* old = lru_.next; // lru 链表里面的数据项除了被该 shard 引用不会被任何客户端引用 assert(old-\u003erefs == 1); // 从 shard 将 old 彻底删除 bool erased = FinishErase(table_.Remove(old-\u003ekey(), old-\u003ehash)); if (!erased) { // to avoid unused variable when compiled NDEBUG assert(erased); } } ... 这个循环最最重要的是这一行: LRUHandle* old = lru_.next; 因为新加入的元素都被插在 lru_.pre 位置, 所以从 lru_.next 开始遍历就是从最老那个元素遍历. 就是这么简单. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#322-lru-算法如何生效的"},{"categories":null,"content":"4 总结本文介绍了 leveldb 的缓存相关设计和实现, 缓存用于保存 sstable 文件的内存形式, 可加速用户查询过程. Leveldb 缓存基于 hash 做 sharding 同时支持 LRU 淘汰机制, 前者减小了锁粒度提升了并发性, 后者在内存占用和缓存命中之间实现了折中. –End— ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#4-总结"},{"categories":null,"content":"leveldb, leveldb, 每个 level 保存的内容就是一组 sorted string table (简称 sstable) 文件. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#"},{"categories":null,"content":"1 sstable 文件布局SSTable 即 sorted string table, 是一个有序文件格式. 该文件主要包含五个部分: 一系列 data blocks, 这里保存的我们需要的数据. 一系列 meta blocks, 这里目前保存的只有布隆过滤器. 通过它, 在不解析 data blocks 的前提下就能知道某个 key 是否存在, 如果可能存在也能快速缩小到可能在哪个 data block. 一个 metaindex block, 包含指向 meta blocks 的索引. 一个 index block, 包含指向 data blocks 的索引. 一个 footer, sstable 文件入口, 保存着指向 metaindex block 和 index block 的索引, 相当于一个二级指针. 不像 kafka 文件存储结构的数据文件和索引文件是各自独立的(在查询时根据具体 key 先在索引文件确定是哪个数据文件), sstable 把索引和数据保存到了同一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 目标 block 起始位置在文件中的偏移量 offset: varint64 // 目标 block 的大小 size: varint64 形象地说, sstable 文件具体布局如下: \u003cbeginning_of_file\u003e [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) \u003cend_of_file\u003e 下面具体讲一下每个段的具体布局. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#1-sstable-文件布局"},{"categories":null,"content":"1.1 data block 布局每个 block 包含的数据笼统地讲, 包含 \u003c一系列数据项 + restart array + restart number\u003e “. 为了节省存储空间, block 中的数据项的 key 使用了前缀压缩. 具体来说, 存储某个 key 的时候先计算它和前一个数据项 key 的公共前缀长度, 公共前缀不再重复存储而是仅记录一个长度(shared), 由于 block 保存的数据是按 key 有序的, 排在一起的前缀都是比较相近的, 而且相似前缀可能还比较长所以该策略可以大幅节省存储空间. block 中有一个至关重要的概念, 叫 restart point. 这个概念和前面提到的前缀压缩密切相关, 每个 block 的前缀压缩不是从第一个数据项开始就一直下去, 而是每隔一段(间隔可配置)设置一个新的前缀压缩起点(作为新起点的数据项的 key 保存原值而非做前缀压缩), restart point 指的就是新起点, 从这个地方开始继续做前缀压缩. block 中每个数据项的格式如下: shared_bytes: varint32(与前一个 key 公共前缀的长度). 注意, 如果该数据项位于 restart 处, 则 shared_bytes 等于 0. unshared_bytes: varint32(当前 key 除去公共前缀后的长度) value_length: varint32(当前 key 对应的 value 的长度) key_delta: char[unshared_bytes](当前 key 除去共享前缀后的字节内容) value: char[value_length](当前 key 对应的 value 的数据内容) block 结尾处有个 trailer, 格式如下: restarts: uint32[num_restarts](保存 restart points 在 block 内偏移量的数组) num_restarts: uint32(restart points 偏移量数组大小) restarts[i] 保存的是第 i 个 restart point 在 block 内的偏移量. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#11-data-block-布局"},{"categories":null,"content":"1.2 meta block 布局它由 \u003c一系列 filters + filter-offset 数组 + filters 部分的结束偏移量(4 字节) + base log 值(1 字节)\u003e 构成. 注意该 block 最后 5 字节内容是固定的, 这也是该部分的解析入口. 该部分在写入文件时不进行压缩. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#12-meta-block-布局"},{"categories":null,"content":"1.3 meta-index block 布局只有一个数据项, key 为 \"filter.\"+过滤器名, value 为 meta block 的 handle. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#13-meta-index-block-布局"},{"categories":null,"content":"1.4 index block 布局同 data block, 每个数据项的 key 是某个 data block 的最后一个 key, 每个数据项的 value 是这个 data block 的 handle. 注意, 由于该 block 数据项数和 data blocks 个数一样, 相对来说非常少, 所以就没做前缀压缩(具体实现就是将 restart point interval 设置为 1). ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#14-index-block-布局"},{"categories":null,"content":"1.5 footer 布局Footer 虽然位于 sstable 文件尾部, 但它是名副其实的文件入口, 它的长度固定, 很容易从文件尾定位到, 它包含: 一个指向 metaindex block 的 BlockHandle 一个指向 index block 的 BlockHandle 一个 magic number. Footer 具体格式如下: // 指向 metaindex block 的 BlockHandle metaindex_handle: char[p]; // 指向 index block 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. 关于 sstable 的其它细节请见 Leveldb 源码详解系列之一: 接口与文件. 了解了布局, 下面让我们来看看针对 sstable 的读写实现. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:5:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#15-footer-布局"},{"categories":null,"content":"2 sstable 文件的序列化与反序列化sstable 文件集合保存着 leveldb 实例的数据, 定义在 db/version_set.h 中的 class leveldb::Version 跟踪每个 level 及其文件, 可以将这个类看做是对 leveldb 全部层级文件架构的抽象. 下面说明一下针对 sstable 文件的序列化和反序列化. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#2-sstable-文件的序列化与反序列化"},{"categories":null,"content":"2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#21-sstable-文件序列化"},{"categories":null,"content":"2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 \u003ckey,value\u003e 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 \u003ckey,value\u003e 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 data block 中 r-\u003edata_block.Add(key, value); const size_t estimated_block_size = r-\u003edata_block.CurrentSizeEstimate(); // 如果当前 data block 大小的估计值大于设定的阈值, // 则将该 data block 写入文件 if (estimated_block_size \u003e= r-\u003eoptions.block_size) { Flush(); } } Add() 在检测到 data block 大小达到阈值时会调用 Flush() 将数据刷入文件. 刷入完成, 会调用 FilterBlockBuilder 为其生成 filter. 具体如下: // Add() 依赖 Flush() 将大小满足要求的 block 写入文件中. void TableBuilder::Flush() { Rep* r = rep_; assert(!r-\u003eclosed); if (!ok()) return; if (r-\u003edata_b","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#211-总干事-tablebuilder"},{"categories":null,"content":"2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-status, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-closed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-num_entries 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-options.comparator-Compare(key, Slice(r-last_key)) 0); } // 需要构造一个新的 data block if (r-pending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-data_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 = 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-options.comparator-FindShortestSeparator(\u0026r-last_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-pending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-index_block.Add(r-last_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-pending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-filter_block != nullptr) { r-filter_block-AddKey(key); } // 用新 key 更新 last_key r-last_key.assign(key.data(), key.size()); r-num_entries++; // data block 相关: // 将 key,value 添加到 data block 中 r-data_block.Add(key, value); const size_t estimated_block_size = r-data_block.CurrentSizeEstimate(); // 如果当前 data block 大小的估计值大于设定的阈值, // 则将该 data block 写入文件 if (estimated_block_size = r-options.block_size) { Flush(); } } Add() 在检测到 data block 大小达到阈值时会调用 Flush() 将数据刷入文件. 刷入完成, 会调用 FilterBlockBuilder 为其生成 filter. 具体如下: // Add() 依赖 Flush() 将大小满足要求的 block 写入文件中. void TableBuilder::Flush() { Rep* r = rep_; assert(!r-closed); if (!ok()) return; if (r-data_b","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#2111-tablebuilder-的存储小助手-rep"},{"categories":null,"content":"2.1.2 写 data blocksBlockBuilder 被 TableBuilder 使用来构造 sstable 文件里的 block, 注意, 该类用于构造 block 的序列化形式, 也就是构造 sstable 时候使用; 反序列化用的是 Block 类. 要理解这里必须要清楚 block 布局, 这个在文章开头有详细描述. 建议回过头看一眼, 尤其 restart point 的设计. 同 TableBuilder 类似, BlockBuilder 最重要的两个方法也是 Add() 和 Finish(). TableBuilder 会重复使用 BlockBuilder 实例, 每写入一个 block 就会调用其 Reset() 将其恢复原状. BlockBuilder 核心成员如下: class BlockBuilder { public: explicit BlockBuilder(const Options* options); // 重置 BlockBuilder 对象状态信息, 就像该对象刚刚被创建时一样. void Reset(); // 与 Finish() 分工, 负责追加一个数据项到 buffer. // 前提: // 1. 自从上次调用 Reset() 还未调用过 Finish(); // 2. 参数 key 要大于任何之前已经添加过的数据项的 key, // 因为这是一个 append 类型操作. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法负责 block 构建的收尾工作, 具体是将 restart points // 数组及其长度追加到 block 的数据内容之后完成构建工作, 最后返回 // 一个指向 block 全部内容的 slice. // 返回的 slice 生命期同当前 builder, 若 builder 调用了 // Reset() 方法则返回的 slice 失效. Slice Finish(); private: // 存储目标 block 内容的缓冲区 std::string buffer_; // 存储目标 block 的全部 restart points // (即每个 restart point 在 block 中的偏移量, // 第一个 restart point 偏移量为 0) std::vector\u003cuint32_t\u003e restarts_; // 该 BlockBuilder 上次调用 Add 时追加的那个 key std::string last_key_; }; 最重要的 Add() 和 Finish() 实现如下: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { // 上次调用 Add 追加的 key, 用于计算公共前缀 Slice last_key_piece(last_key_); // 不能往一个完成构建的 block 里追加数据. assert(!finished_); // 自上个 restart 之后追加的 key 的个数没有超过要求的两个 // restart points 之间 keys 的个数. assert(counter_ \u003c= options_-\u003eblock_restart_interval); // 当前要追加的 key 要大于任何之前追加到 buffer 中的 key assert(buffer_.empty() // No values yet? || options_-\u003ecomparator-\u003eCompare(key, last_key_piece) \u003e 0); size_t shared = 0; // 如果自上个 restart 之后追加的 key 的个数小于所配置的 // 两个相邻 restart 之间 keys 的个数. if (counter_ \u003c options_-\u003eblock_restart_interval) { const size_t min_length = std::min(last_key_piece.size(), key.size()); // 计算当前要追加的 key 与上次追加的 key 的公共前缀长度. while ((shared \u003c min_length) \u0026\u0026 (last_key_piece[shared] == key[shared])) { shared++; } } else { // 否则, 新增一个 restart point, 而且作为 restart 的 key 不进行压缩. // - restart 就是一个 offset, 具体值为当前 buffer 所占空间大小. // - restart 后第一个数据项的 key 不进行压缩, 即不计算与前一个 key // 的公共前缀了, 而是把这个 key 整个保存起来, 但是本 \"restart\" // 段, 从这个 key 开始后面的 keys 都要进行压缩. restarts_.push_back(buffer_.size()); counter_ = 0; } const size_t non_shared = key.size() - shared; // Add \"\u003cshared\u003e\u003cnon_shared\u003e\u003cvalue_size\u003e\" to buffer_ // // buffer 里面的每个记录的格式为: // \u003cvarint32 类型的当前 key 与上个 key 公共前缀长度\u003e // \u003cvarint32 类型的当前 key 长度减去公共前缀后的长度\u003e // \u003cvarint32 类型的当前 value 的长度\u003e // \u003c与前一个 key 公共前缀之后的部分\u003e // \u003cvalue\u003e PutVarint32(\u0026buffer_, shared); PutVarint32(\u0026buffer_, non_shared); PutVarint32(\u0026buffer_, value.size()); // 将 key 非公共部分和 value 追加到 buffer_ buffer_.append(key.data() + shared, non_shared); buffer_.append(value.data(), value.size()); // 更新状态 last_key_.resize(shared); // 将 last_key 更新为当前 key last_key_.append(key.data() + shared, non_shared); assert(Slice(last_key_) == key); // 将自上个 restart 之后的记录数加一 counter_++; } Slice BlockBuilder::Finish() { /** * 先将 restarts 数组编码后追加到 buffer, * 然后将 restarts 数组长度编码后追加到 buffer 并将 finished 置位, * 最后根据 buffer 构造一个新的 slice 返回(注意该 slice 引用的内存是 * buffer, 所以生命期同 builder, 除非 builder 调用了 Reset) */ // Append restart array for (size_t i = 0; i \u003c restarts_.size(); i++) { PutFixed32(\u0026buffer_, restarts_[i]); } PutFixed32(\u0026buffer_, restarts_.size()); finished_ = true; return Slice(buffer_); } ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#212-写-data-blocks"},{"categories":null,"content":"2.1.3 写 meta(filter) block不同于 data block 和 data index block, filter block 有专用的 builder, 叫做 FilterBlockBuilder. 它的核心方法是 AddKey() 和 Finish(). // 该类在其它地方定义. 用于定义过滤器逻辑. class FilterPolicy; // FilterBlockBuilder 用于构造 table 的全部 filters. // 最后生成一个字符串保存在 Table 的一个 meta block 中. // // 该类的方法调用序列必须满足下面的正则表达式: // (StartBlock AddKey*)* Finish // 最少调用一次 Finish, 而且 AddKey 和 Finish 之间不能插入 StartBlock 调用. class FilterBlockBuilder { public: explicit FilterBlockBuilder(const FilterPolicy*); void StartBlock(uint64_t block_offset); void AddKey(const Slice\u0026 key); Slice Finish(); private: void GenerateFilter(); const FilterPolicy* policy_; // 调用 AddKey() 时每个 key 都会被 // 追加到这个字符串中(用于后续构造 filter 使用) std::string keys_; // 与 keys_ 配套, 每个被 AddKey() 方法追加的 key 在 // keys_ 中的起始索引. std::vector\u003csize_t\u003e start_; // 每个新计算出来的 filter 都是一个字符串, // 都会被追加到 result_ 中. // filter block 保存的内容就是 result_. std::string result_; // 是 keys_ 的列表形式, 临时变量, 每个成员是 Slice 类型, // 用于 policy_-\u003eCreateFilter() 生成构造器. std::vector\u003cSlice\u003e tmp_keys_; // 与 result_ 配套, 保存每个 filter 在 result_ // 中的起始偏移量. std::vector\u003cuint32_t\u003e filter_offsets_; // No copying allowed FilterBlockBuilder(const FilterBlockBuilder\u0026); void operator=(const FilterBlockBuilder\u0026); }; TableBuilder::Add() 会在追加 k,v 的时候调用 FilterBlockBuilder 的 AddKey() 将 k 追加到 FilterBlockBuilder 中, 其实现逻辑比较简单: // 向 keys_ 中增加一个 key, 同时将 key 在 keys_ 中 // 起始偏移量保存到 start_ 向量中. void FilterBlockBuilder::AddKey(const Slice\u0026 key) { Slice k = key; start_.push_back(keys_.size()); keys_.append(k.data(), k.size()); } 当 TableBuilder flush 一个 data block 到文件后, 就要为其生成 filter, 该过程通过调用 FilterBlockBuilder 的 StartBlock() 达成: // 为前一个已写入 table 文件的 data block 生成 // filter, 生成完毕后重置当前 FilterBlockBuilder 的状态为生成下一个 // filter 做准备. void FilterBlockBuilder::StartBlock(uint64_t block_offset) { // 计算以 block_offset 为起始地址的 block 对应的 filter 在 // filter-offset 数组中的索引. // 默认每两 KB 数据就要生成一个 filter, // 如果 block size 超过 2KB, 则会生成多个 filters. uint64_t filter_index = (block_offset / kFilterBase); assert(filter_index \u003e= filter_offsets_.size()); // filter 是一个接一个构造的, 对应的索引数组也是对应着逐渐增长的, // 而非一次性构造好往里面填, 毕竟不知道要生成多少个 filters while (filter_index \u003e filter_offsets_.size()) { // 这里虽然是个循环, 但是因为每次生成 filter // 都会清空相关状态(keys_, start_ 等等), // 所以下个循环并不会再生成 filter 了, 具体见 // GenerateFilter() 的 if 部分. GenerateFilter(); } } StartBlock() 方法有个循环调用 GenerateFilter() 方法的地方, 比较绕, 这部分逻辑要结合 GenerateFilter() if 部分和 FilterBlockReader::KeyMayMatch() 的 limit 计算部分一起看: // 由于即将生成的 block 不能与当前已写入 table 文件的 block // 的 keys 共用 filter 了, 所以为当前已写入 table 文件的 block 的 // keys_ 生成一个 filter. 生成完毕后清空当前 FilterBlockBuilder // 相关相关状态以为下个 filter 计算所用. void FilterBlockBuilder::GenerateFilter() { // keys_ 为空, 无须生成新的 filter. const size_t num_keys = start_.size(); if (num_keys == 0) { // 没有 key 需要计算 filter, 则直接把上一个 filter // 的结束地址(每个 filter 都是一个字符串, 所以保存到 result_ // 时候既有起始地址又有结束地址)填充到 filter-offset 数组中, // 这么做一方面为了对齐(方便 FilterBlockReader::KeyMayMatch() // 直接通过移位计算 filter 索引), 另一方面方便计算 filter 结束偏移量( // 就是 FilterBlockReader::KeyMayMatch() 计算 limit 的步骤). filter_offsets_.push_back(result_.size()); return; } // 将扁平化的 keys_ 转换为一个 key 列表. // 将 keys_ 大小放到 start_ 中作为最后一个 key 的结束地址, // 这样下面可以直接用 start_[i+1] - start_[i] 计算 // 每个 key 长度. start_.push_back(keys_.size()); tmp_keys_.resize(num_keys); // 将字符串 keys_ 保存的每个 key 提取出来封装 // 成 Slice 并放到 tmp_keys 列表中 for (size_t i = 0; i \u003c num_keys; i++) { // 第 i 个 key 在 keys 中的起始地址 const char* base = keys_.data() + start_[i]; // 第 i 个 key 的长度 size_t length = start_[i+1] - start_[i]; // 将第 i 个 key 封装成 Slice 并保存到 tmp_keys // 用于后续计算 filter tmp_keys_[i] = Slice(base, length); } // 为当前的 key 集合生成 filter. // 先将新生成的 filter 在 result_ 中的 // 起始偏移量保存到 filter_offsets_. filter_offsets_.push_back(result_.size()); // 根据 tmp_keys_ 计算 filter 并追加到 result policy_-\u003eCreateFilter(\u0026tmp_keys_[0], static_cast\u003cint\u003e(num_keys), \u0026result_); // 重置当前 FilterBlockBuilder 相关的状态以方便为 // 下个 data block 计算 filter 使用. tmp_keys_.clear(); keys_.clear(); start_.clear(); } 与 FilterBlockBuilder 相反, 将一个 filter block 解析出来, 然后用来查询某个 key 是否在某个 block 中. 其成员 data_ 指向 filte","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#213-写-metafilter-block"},{"categories":null,"content":"2.1.4 写 meta-index block这部分比较简单, 其在 TableBuilder 的 Finish() 方法里完成: // 3 filter block 就是 table_format.md 中提到的 // meta block, 写完 meta block 该写它对应的索引 // metaindex block 到文件中了. if (ok()) { // 虽然 meta block 有独立的 FilterBlockBuilder, // 但是其对应的 index block 用的还是通用的 // BlockBuilder. BlockBuilder meta_index_block(\u0026r-\u003eoptions); if (r-\u003efilter_block != nullptr) { // 如果 meta block 存在, 则将其对应的 key // 和 BlockHandle 写入 metaindex block, // 具体为 \u003cfilter.Name, filter 数据地址\u003e. std::string key = \"filter.\"; key.append(r-\u003eoptions.filter_policy-\u003eName()); std::string handle_encoding; filter_block_handle.EncodeTo(\u0026handle_encoding); meta_index_block.Add(key, handle_encoding); } // 将 metaindex block 写入文件 WriteBlock(\u0026meta_index_block, \u0026metaindex_block_handle); } ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#214-写-meta-index-block"},{"categories":null,"content":"2.1.5 写 data-index block这部分比较简单, 其在 TableBuilder 的 Finish() 方法里完成: // 4 将 index block 写入 table 文件, 它里面保存的 // 都是 data block 对应的 BlockHandle. if (ok()) { // 最后构建的 data block 对应的 index block entry 还没有写入 if (r-\u003epending_index_entry) { r-\u003eoptions.comparator-\u003eFindShortSuccessor(\u0026r-\u003elast_key); std::string handle_encoding; r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // 写入最后构建的 data block 对应的 index block entry r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); r-\u003epending_index_entry = false; } WriteBlock(\u0026r-\u003eindex_block, \u0026index_block_handle); } ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#215-写-data-index-block"},{"categories":null,"content":"2.1.6 写 footerfooter 是 sstable 文件的入口, 结构比较简单: // Footer 封装一个固定长度的信息, 它位于每个 table 文件的末尾. // // 在每个 sstable 文件的末尾是一个固定长度的 footer, // 它包含了一个指向 metaindex block 的 BlockHandle // 和一个指向 index block 的 BlockHandle 以及一个 magic number. class Footer { public: Footer() { } void EncodeTo(std::string* dst) const; Status DecodeFrom(Slice* input); // Footer 长度编码后的长度. 注意, 它就固定这么长. // Footer 包含了一个 metaindex_handle、一个 index_handle、以及一个魔数. enum { // Footer 长度, 两个 BlockHandle 最大长度 + 固定的 8 字节魔数 kEncodedLength = 2*BlockHandle::kMaxEncodedLength + 8 }; private: BlockHandle metaindex_handle_; BlockHandle index_handle_; }; 其在 TableBuilder 的 Finish() 方法里完成写入: // 5 最后将末尾的 Footer 写入 table 文件 if (ok()) { Footer footer; footer.set_metaindex_handle(metaindex_block_handle); footer.set_index_handle(index_block_handle); std::string footer_encoding; footer.EncodeTo(\u0026footer_encoding); r-\u003estatus = r-\u003efile-\u003eAppend(footer_encoding); if (r-\u003estatus.ok()) { r-\u003eoffset += footer_encoding.size(); } } ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:6","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#216-写-footer"},{"categories":null,"content":"2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#22-sstable-文件反序列化"},{"categories":null,"content":"2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Footer, 它是 sstable 的入口. 根据已解析的 Footer, 解析出 (data) index block 存储到 Table::rep_. 根据已解析的 Footer, 解析出 meta block 存储到 Table::rep_. 注意, Open 方法并未去解析 data block 部分, 仅仅是解析了它对应的 index block 部分 和 meta block(它包含的是过滤器, 用来快速确认 key 是否存在). 那什么时候才解析 data block 呢? 答案是调用 InternalGet() 时候: // 在 table 中查找 k 对应的数据项. // 如果 table 具有 filter, 则用 filter 找; // 如果没有 filter 则去 data block 里面查找, // 并且在找到后通过 saver 保存 key/value. Status Table::InternalGet(const ReadOptions\u0026 option","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#221-总干事-table-类"},{"categories":null,"content":"2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size Read(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-options = options; rep-file = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-metaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-index_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-cache_id = (options.block_cache ? options.block_cache-NewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-filter_data = nullptr; rep-filter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-ReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Footer, 它是 sstable 的入口. 根据已解析的 Footer, 解析出 (data) index block 存储到 Table::rep_. 根据已解析的 Footer, 解析出 meta block 存储到 Table::rep_. 注意, Open 方法并未去解析 data block 部分, 仅仅是解析了它对应的 index block 部分 和 meta block(它包含的是过滤器, 用来快速确认 key 是否存在). 那什么时候才解析 data block 呢? 答案是调用 InternalGet() 时候: // 在 table 中查找 k 对应的数据项. // 如果 table 具有 filter, 则用 filter 找; // 如果没有 filter 则去 data block 里面查找, // 并且在找到后通过 saver 保存 key/value. Status Table::InternalGet(const ReadOptions\u0026 option","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#2211-table-类的小助手之一-rep"},{"categories":null,"content":"2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size Read(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-options = options; rep-file = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-metaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-index_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-cache_id = (options.block_cache ? options.block_cache-NewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-filter_data = nullptr; rep-filter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-ReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Footer, 它是 sstable 的入口. 根据已解析的 Footer, 解析出 (data) index block 存储到 Table::rep_. 根据已解析的 Footer, 解析出 meta block 存储到 Table::rep_. 注意, Open 方法并未去解析 data block 部分, 仅仅是解析了它对应的 index block 部分 和 meta block(它包含的是过滤器, 用来快速确认 key 是否存在). 那什么时候才解析 data block 呢? 答案是调用 InternalGet() 时候: // 在 table 中查找 k 对应的数据项. // 如果 table 具有 filter, 则用 filter 找; // 如果没有 filter 则去 data block 里面查找, // 并且在找到后通过 saver 保存 key/value. Status Table::InternalGet(const ReadOptions\u0026 option","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#2212-table-类的小助手之二-block"},{"categories":null,"content":"2.2.2 读 footerfooter 解析比较简单, 主要就是获取 meta-index block 和 data-index block 分别在文件中的地址和大小: // 从 input 指向内存解码出一个 Footer, // 先解码最后 8 字节的魔数(按照小端模式), // 然后一次解码两个 BlockHandle. Status Footer::DecodeFrom(Slice* input) { // 1 按照小端模式解析末尾 8 字节的魔数 const char* magic_ptr = input-\u003edata() + kEncodedLength - 8; const uint32_t magic_lo = DecodeFixed32(magic_ptr); const uint32_t magic_hi = DecodeFixed32(magic_ptr + 4); const uint64_t magic = ((static_cast\u003cuint64_t\u003e(magic_hi) \u003c\u003c 32) | (static_cast\u003cuint64_t\u003e(magic_lo))); if (magic != kTableMagicNumber) { return Status::Corruption(\"not an sstable (bad magic number)\"); } // 2 解析 meta-index block 的 handle // (包含 meta index block 起始偏移量及其长度) Status result = metaindex_handle_.DecodeFrom(input); if (result.ok()) { // 3 解析 index block 的 handle // (包含 index block 起始偏移量及其长度) result = index_handle_.DecodeFrom(input); } if (result.ok()) { // 4 跳过 padding // meta-index handle + data-index handle + padding + 魔数. // 此时 input 包含的数据只剩下可能的 padding 0 了, 跳过. // end 为 footer 尾部 const char* end = magic_ptr + 8; // 第二个参数为值为 0. 生成下面这个 slice 后面没有使用. *input = Slice(end, input-\u003edata() + input-\u003esize() - end); } return result; } ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#222-读-footer"},{"categories":null,"content":"2.2.3 读 data-index block拿到 data-index block 地址和大小后就可以解析它了: /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } 上面 ReadBlock() 最后一个输出型参数即为 data-index block 内容, 可以将其反序列化为 Block 对象: Block* index_block = new Block(index_block_contents); ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#223-读-data-index-block"},{"categories":null,"content":"2.2.4 读 meta-index block解析出来的 meta-index block handle 放到了 footer 对象中, 根据它就可以解析 meta-index block 和 meta block 了: /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); 这个过程统一在 ReadMeta() 完成, 具体读取 meta block 会有专用的 ReadFilter() 完成: // 解析 table 的 metaindex block(需要先解析 table 的 footer); // 然后根据解析出来的 metaindex block 解析 meta block(目前 meta block // 仅有 filter block 一种). // 这就是我们要的元数据, 解析出来的元数据会被放到 Table::rep_ 中. void Table::ReadMeta(const Footer\u0026 footer) { // 如果压根就没配置过滤策略, 那么无序解析元数据 if (rep_-\u003eoptions.filter_policy == nullptr) { return; } /** * 根据 Footer 保存的 metaindex BlockHandle * 解析对应的 metaindex block 到 meta 中 */ ReadOptions opt; if (rep_-\u003eoptions.paranoid_checks) { // 如果开启了偏执检查, 则校验每个 block 的 crc opt.verify_checksums = true; } BlockContents contents; // 1 根据 handle 读取 metaindex block (从 rep_ 到 contents) if (!ReadBlock(rep_-\u003efile, opt, footer.metaindex_handle(), \u0026contents).ok()) { // 由于 filter block 不是必须的, 没有 filter 最多就是读得慢一些; // 所以出错也不再继续传播, 而是直接返回. return; } // 这个变量叫 metaindex 更合适 Block* meta = new Block(contents); // 为 metaindex block 创建一个迭代器 Iterator* iter = meta-\u003eNewIterator(BytewiseComparator()); // 具体见 table_format.md // metaindex block 有一个 entry 包含了 FilterPolicy name // 到其对应的 filter block 的映射 std::string key = \"filter.\"; // filter-policy name 在调用方传进来的配置项中 key.append(rep_-\u003eoptions.filter_policy-\u003eName()); // 在 metaindex block 搜寻 key 对应的 meta block 的 handle iter-\u003eSeek(key); if (iter-\u003eValid() \u0026\u0026 iter-\u003ekey() == Slice(key)) { // 2 找到了, 迭代器对应的 value 即为 meta block handle, // 根据其解析对应的 filter block(就是 meta block), 解析出来的 // 内容会放到 rep_ 中. ReadFilter(iter-\u003evalue()); } delete iter; delete meta; } ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#224-读-meta-index-block"},{"categories":null,"content":"2.2.5 读 meta block前一节提到了读取 meta(filter) block 读取: // 2 找到了, 则解析对应的 filter block, 解析出来的 // 内容会放到 rep_ 中. ReadFilter(iter-\u003evalue()); 具体读取由 ReadFilter() 方法完成: // 解析 table 的 filter block(需要先解析 table 的 metaindex block) // 解析出的数据放到了 table.rep.filter void Table::ReadFilter(const Slice\u0026 filter_handle_value) { Slice v = filter_handle_value; BlockHandle filter_handle; // 解析出 filter block 对应的 blockhandle, 它包含 filter block // 在 sstable 中的偏移量和大小 if (!filter_handle.DecodeFrom(\u0026v).ok()) { return; } ReadOptions opt; if (rep_-\u003eoptions.paranoid_checks) { opt.verify_checksums = true; } BlockContents block; // 读取 filter block(从 rep_ 到 block) if (!ReadBlock(rep_-\u003efile, opt, filter_handle, \u0026block).ok()) { return; } // 如果 blockcontents 中的内存是从堆分配的, // 需要将其地址赋值给 rep_-\u003efilter_data 以方便后续释放(见 ~Rep()) if (block.heap_allocated) { rep_-\u003efilter_data = block.data.data(); } // 反序列化 filter block (从 block.data 到 FilterBlockReader) rep_-\u003efilter = new FilterBlockReader(rep_-\u003eoptions.filter_policy, block.data); } ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#225-读-meta-block"},{"categories":null,"content":"2.2.6 读 block 内容的通用方法每个 block(data block, meta block, meta-index block, data-index block 四类) 在写入 sstable 后都会紧接着追加一字节长的压缩类型和四字节长的 crc(它是 block + 压缩类型一起算出来的), 负责读取解析 block + 压缩类型 + crc 的方法为位于 table/format.cc 中的 ReadBlock() 方法: // 从 file 去读 handle 指向的 block: // - 读取整个块, 包含数据+压缩类型(1 字节)+crc(4 字节) // - 校验 crc: 重新计算 crc 并与保存 crc 比较 // - 解析压缩类型, 根据压缩类型对数据进行解压缩 // - 将 block 数据部分保存到 BlockContents 中 // 失败返回 non-OK; 成功则将数据填充到 *result 并返回 OK. Status ReadBlock(RandomAccessFile* file, const ReadOptions\u0026 options, const BlockHandle\u0026 handle, BlockContents* result) { result-\u003edata = Slice(); result-\u003ecachable = false; result-\u003eheap_allocated = false; /** * 解析 block. * 读取 block 内容以及 type 和 crc. * 具体见 table_builder.cc 中构造这个结构的代码. */ // 要读取的 block 的大小 size_t n = static_cast\u003csize_t\u003e(handle.size()); // 每个 block 后面紧跟着它的压缩类型 type (1 字节)和 crc (4 字节) char* buf = new char[n + kBlockTrailerSize]; Slice contents; // handle.offset() 指向对应 block 在文件里的起始偏移量 Status s = file-\u003eRead(handle.offset(), n + kBlockTrailerSize, \u0026contents, buf); if (!s.ok()) { delete[] buf; return s; } if (contents.size() != n + kBlockTrailerSize) { delete[] buf; return Status::Corruption(\"truncated block read\"); } /** * 校验 type 和 block 内容加在一起对应的 crc */ const char* data = contents.data(); if (options.verify_checksums) { // 读取 block 末尾的 crc(始于第 n+1 字节) const uint32_t crc = crc32c::Unmask(DecodeFixed32(data + n + 1)); // 计算 block 前 n+1 字节的 crc const uint32_t actual = crc32c::Value(data, n + 1); // 比较保存的 crc 和实际计算的 crc 是否一致 if (actual != crc) { delete[] buf; s = Status::Corruption(\"block checksum mismatch\"); return s; } } /** * 解析 type, 并根据 type 解析 block data */ // type 表示 block 的压缩状态 switch (data[n]) { case kNoCompression: if (data != buf) { delete[] buf; result-\u003edata = Slice(data, n); result-\u003eheap_allocated = false; result-\u003ecachable = false; // Do not double-cache } else { result-\u003edata = Slice(buf, n); result-\u003eheap_allocated = true; result-\u003ecachable = true; } // Ok break; case kSnappyCompression: { size_t ulength = 0; // 获取 snappy 压缩前的数据的大小以分配内存 if (!port::Snappy_GetUncompressedLength(data, n, \u0026ulength)) { delete[] buf; return Status::Corruption(\"corrupted compressed block contents\"); } char* ubuf = new char[ulength]; // 将 snappy 压缩过的数据解压缩到上面分配的内存中 if (!port::Snappy_Uncompress(data, n, ubuf)) { delete[] buf; delete[] ubuf; return Status::Corruption(\"corrupted compressed block contents\"); } delete[] buf; result-\u003edata = Slice(ubuf, ulength); result-\u003eheap_allocated = true; result-\u003ecachable = true; break; } default: delete[] buf; return Status::Corruption(\"bad block type\"); } return Status::OK(); } ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:6","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#226-读-block-内容的通用方法"},{"categories":null,"content":"2.3 Table 和两级迭代器的结合前面讲过了, 打开 sstable 文件后会生成对应的 Table 对象, 该对象会被放到 TableCache 缓存中. 如果要访问其内容, 需要一个迭代器, 该工作通过 leveldb::Iterator *leveldb::Table::NewIterator 完成: // 先为 data-index block 数据项构造一个迭代器 index_iter, // 然后基于 index_iter 查询时, 为其指向的具体 data block // 构造一个迭代器 data_iter, 进而可以迭代该 data block 里 // 的全部数据项. // 这样就构成了一个两级迭代器, 从而实现遍历全部 data blocks 的数据项. Iterator* Table::NewIterator(const ReadOptions\u0026 options) const { return NewTwoLevelIterator( rep_-\u003eindex_block-\u003eNewIterator(rep_-\u003eoptions.comparator), \u0026Table::BlockReader, const_cast\u003cTable*\u003e(this), options); } 返回的迭代器为一个 leveldb::\u003cunnamed\u003e::TwoLevelIterator, 该迭代器处于匿名的命名空间所以未直接对外暴露, 仅能通过返回的指针访问其从 class leveldb::Iterator 继承的方法. 每个 sstable 文件对应一个两级迭代器, 然后将全部 sstable 对应的两级迭代器级联起来, 就相当于为整个 leveldb 构造了一个迭代器(见 leveldb::Version::AddIterators(), 后续会详解该类), 从而实现在整个 leveldb 上轻松实现迭代或者查询. –End– ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#23-table-和两级迭代器的结合"},{"categories":null,"content":"本文基于内部分享 \u003c“抄\"能力养成系列 – MapReduce: 分布式计算系统设计与实现\u003e 整理. 2003 年开始 Google 陆续放出三套系统的设计(GFS/MapReduce/Bigtable), 在互联网届掀起云计算狂潮一直影响至今. MapReduce 作为老二出场, 因为它的实现依赖于之前分享的 GFS 作为存储. 该论文一出, 便直接催生了 Hadoop 另一个重量级同名框架 MapReduce 的诞生. 时光荏苒, 虽然后面又出现了 spark/flink, 但是 MapReduce 在批处理领域的地位至今牢固. 下面就让我们一起看看 MapReduce 的设计, 希望为各位后续系统研发提供灵感. (Salute to Jeff). ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#"},{"categories":null,"content":"1 简要介绍这个模型说起来真是简单至极, 非常符合直觉, 就是说给非互联网行业的人, 也能听明白. 该模型跟函数式语言中的 map-reduce 理念基本一样, 不过这个是分布式的. map 用于处理原始记录, 输出中间 \u003ck, v\u003e; reduce 基于 k 把中间数据合并, 输出 \u003ck, List\u003cv\u003e\u003e. 并行化 容错 数据分发 负载均衡 最主要的容错机制就是支持重跑任务. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#1-简要介绍"},{"categories":null,"content":"2 编程模型用户要写的就是 Map 函数和 Reduce 函数. Map 负责将输入加工成中间 kv; MapReduce 库负责将同一个 k 的全部 v 收集好发给 Reduce; Reduce 接收中间数据, 然后基于 k, 合并 v, 一般输出一个或零个值. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#2-编程模型"},{"categories":null,"content":"2.1 举例以单词计数为例, 用户需要干的就是实现自己的 map 函数和 reduce 函数. 剩下的事情由框架负责. // key: 文档名 // value: 文档内容 map(String key, String value): // 遍历文档, 每个词输出一个键值对 for each word w in value: EmitIntermediate(w, \"1\"); // key: 单词 // value: 计数值构成的列表 reduce(String key, Iterator values): // 累加器 int result = 0; // 遍历每个计数值, 将其累加起来 for each v in values: result += ParseInt(v); // 得到每个单词出现的次数 Emit(AsString(result)); ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#21-举例"},{"categories":null,"content":"2.2 输入输出类型 // 输入为两个参数, k1 类型的键, v1 类型的值; // 返回值是一个键值对, 每个键值对是一个 \u003ck2 类型的键, v2 类型的值\u003e, // 整体效果上看相当于输出了一个键值对列表. map (k1,v1) → list(k2,v2) // 输入为两个参数, k2 类型的键, 以及其对应的 v2 类型的值的列表; // 返回值是 v2 类型的值, 效果上看相当于输出了一个列表. reduce (k2,list(v2)) → list(v2) ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#22-输入输出类型"},{"categories":null,"content":"2.3 使用场景举例 分布式 grep map 函数处理模式匹配, 一旦匹配输出一行; reduce 函数是一个等价函数(啥也不干), 将中间结果拷贝到输出. URL 访问计数 map 函数负责处理每个页面的请求日志, 每处理一行便输出 \u003cURL, 1\u003e. reduce 函数负责将 URL 一样的值累加, 返回的是 \u003cURL, 累加值\u003e web 站点链接反转 map 函数针对每个在叫 source 的页面中发现的链接 target, 输出 \u003ctarget, source\u003e. reduce 将每个 target 对应的 source 收集为一个列表并输出, 形如 \u003ctarget, list\u003csource\u003e\u003e. Term-Vector per Host 背景: 检索词向量是对一个文档或者文档集合中最重要的单词及其词频的统计, 形式为 \u003cword, frequency\u003e 列表. map 函数针对每个输入文档, 输出 \u003chostname, term vector\u003e, 其中 hostname 是从文档对应的 URL 中抽取得到. reduce 函数负责处理给定 host 的每个文档的检索词向量, 它将这些词向量加在在一起去除低频检索词, 输出 \u003chostname, term vector\u003e 键值对. 倒排索引 map 函数解析每个文档, 输出一各 \u003cword, document ID\u003e 序列. reduce 函数接受给定单词的全部键值对, 将对应的 document IDs 排序, 输出一个 \u003cword, list(document ID)\u003e 键值对. 分布式排序 map 函数从每个 record 抽取 key, 输出一个 \u003ckey, record\u003e 键值对. reduce 函数原封不动地输出键值对. 该计算场景依赖于后面将要描述的分区设施和排序属性. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#23-使用场景举例"},{"categories":null,"content":"3 实现模型很简单, 具体实现取决于硬件环境. 以 Google 为例(快二十年前的数据了): 双核 x86 处理器, 运行 Linux, 2-4GB 内存. 普通商用网络, 100Mb/s. 几百上千台上述机器构成的集群. 数据就保存在计算节点上, 普通的 IDE 磁盘. 不过这些数据由 GFS 管理, 确保高可用. 用户将 job 提交到调度系统. 每个 job 由多个 tasks 构成, 每个 job 被调度器映射到集群内的一组机器. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#3-实现"},{"categories":null,"content":"3.1 执行概览系统自动将输入数据自动切割成 M 份, 然后在对应机器上部署多个 Mapper, 每个 Mapper 负责处理若干份数据. Mapper 处理输入生成中间数据, 通过分区函数(比如 hash(key) mod R)将中间数据的键空间分成 R 份, 并在其之上部署 Reducer. 具体的分区函数和分成几份, 由用户负责指定. Figure 1 显示了一个 MapReduce 操作的执行概览. 用户程序调用 MapReduce 函数, 然后接下来框架内部陆续发生如下动作: MapReduce 将输入切分成 M 份, 并在一组机器上启动多个用户程序拷贝(fork). 上一步的 fork, 其中有 M 个 map workers, R 个 reduce workers, 还有一个特殊的作为 Master 负责分配任务. map worker 负责读取和解析输入的 key/value 并传给用户定义的 Map 函数, 后者输出中间状态的 key’/value’, 这些中间数据起初被缓存到内存中. map worker 缓存的中间数据会被周期性的写到本地磁盘, 同时会被划分成 R 个分区(如 hash(key’) mod R), 注意由于分区函数无法保证原空间和像空间一一映射, 所以每个分区的 key’ 可能不唯一(比如 R 为 7, 则 key’ 为 70 和 700 的落在同一个分区内). 这些中间数据 key’/value’ 的位置会被上报给 Master, 它会负责把这些位置信息转发给 reduce workers, 每个 reduce worker 负责一个分区. 尤其注意一点, 这一步的中间数据会被写到 map tasks 的本地磁盘, 而不是 GFS. 当 reduce worker 收到上面提到的位置信息的时候, 它发起一个 RPC 读取那个 map workers 磁盘缓存的数据. 当数据都被读取过来之后, reduce worker 根据中间 key’ 对数据进行排序, 于是相同的 keys 就会被排列到一起. 之所以需要排序, 是因为会有不同的 keys 落到同一个 reduce worker(毕竟像 hash(key’) mod R 这种算法无法保证原空间和像空间是一一映射). 如果数据大到无法装进内存, reduce worker 就会采用外部排序算法. reduce worker 迭代排序后的数据, 针对每个唯一 key’, 它会把其连同对应的一组 value’ 传给用户编写的 Reduce 函数, 该函数输出会被追加到当前 reduce 分区的文件中. 注意, 不同于 map tasks, reduce worker 的输出是写到 GFS. 当全部 map tasks 和 reduce tasks 执行完成后, master 就会唤醒用户程序. MapReduce 调用返回至用户代码. 执行成功后, mapreduce 结果保存到了 R 个输出文件中(每个 reduce 任务一个输出文件). 一般用户无需合并这 R 个文件, 因为这些文件会被作为下个阶段的 MapReduce 调用的输入, 或者作为其它可以处理多个输入文件的分布式应用的输入. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#31-执行概览"},{"categories":null,"content":"3.2 Master 节点的作用master 保存着 map tasks 和 reduce tasks 的状态信息以及它们对应的机器 id. master 是一个将中间文件位置信息从 map 传递到 reduce 的中介. master 保存 map tasks 生成的 R 个中间文件区域的位置信息和大小, 因为 map task 成功完成而产生的这些信息的更新都会被 master 接收并增量推送给正在执行的 reduce tasks. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#32-master-节点的作用"},{"categories":null,"content":"3.3 容错","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#33-容错"},{"categories":null,"content":"3.3.1 worker 故障(这里的 worker 指的是机器.) master 周期性 ping 各个 worker 来检活, 如果故障了, 则 master 会在其它机器上重新调度其上跑的 tasks. 故障机器上运行中的 map task 或者 reduce task 会被被重置为 idle 状态, 因此可以在其它 workers 上再次被调度. 失败的 map tasks 会在其它机器上调度重新执行一遍, 因为它们的输出都在故障机器本地磁盘上, 所以这些数据就丢了; 但是 reduce tasks 失败后在其它机器上被调度后无需从头重新执行, 因为它们的输出在类似 GFS 的分布式文件系统中, 继续从失败处继续运行即可. 如果 map task 换机器重新执行, 那么这个情况会被告知给全部 reduce workers, 毕竟这个 map task 输出的中间数据可能会覆盖全部 reduce workers 对应的分区. MapReduce 可以容忍大批机器集体故障几分钟, 只需将故障机器上跑的任务重新在其它机器上重新调度执行就可以保证进度进行下去. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#331-worker-故障"},{"categories":null,"content":"3.3.2 master 故障除了心跳以外, 周期性 checkpoint 是提升容错的另一个利器. master 可以周期性的 checkpointing 自己的状态, 如果失效, 则从最后一个 checkpoint 重启新的 master 即可. 即使是单一 master 架构, 但也容易失效, 如果失效, 客户端可以选择重试 MapReduce 计算. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#332-master-故障"},{"categories":null,"content":"3.3.3 故障存在场景下的语义保证当用户提供的 map 和 reduce 算子是其输入值的确定性函数时(绝大多数计算场景都这样)，即输入确定则输出也是确定的, 我们的分布式实现产生的输出与单体程序的无故障顺序执行所产生的输出相同。但达成这一点依赖于 map 和 reduce 的输出能原子化地提交, 下面详述. 每个进行中的 task 会生成自己的私有临时文件: 每个 reduce task 会生成一个文件; 每个 map task 会生成 R 个文件, 每个文件对应一个 reduce task. 其中, map task 被重新调度会丢弃之前的输出会重新从头计算, 成功完成后会将自己生成的 R 个文件的名字上报给 master, master 会将其记录到本地. reduce task 完成后会将自己的临时输出文件重命名为最终输出文件, 这个重命名过程是原子化的, 看过之前 GFS 分享的应该很清楚. 如果同样的 reduce task 因为故障被在多个机器上先后执行, 那么同一个最终输出文件会被重命名多次. 但由于输出都一样, 所以文件内容也都一样. 我们依赖底层文件系统如 GFS 提供的原子化重命名操作来保证最终的文件状态与同样的 reduce task 只运行一次结果相同. 前面说的都是算子是确定性函数的情形, 如果算子具有不确定性呢? 针对不确定性情形, 我们提供了比较弱但是仍合理的语义. 当存在不确定算子时, 某个 reduce task $R_{1}$ 的输出等价于一个不确定程序顺序执行时的输出, 而另一个 reduce task $R_{2}$ 的输出可能对应前述不确定程序的另一个顺序执行的输出. 考虑 map task $M$ 和 reduce tasks $R_{1}$ 和 $R_{2}$, 令 $e(R_{i})$ 代表 $R_{i}$ 的执行过程(一次恰好仅有一个该执行过程, 因为只有 task 故障了才会执行另一个). 因为 $e(R_{1})$ 可能读取了 $M$ 某次执行的输出, 而 $e(R_{2})$ 可能读取了 $M$ 的另一次执行的输出, 所以弱语义就保证了. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#333-故障存在场景下的语义保证"},{"categories":null,"content":"3.4 数据局部性由 GFS 管理的输入数据就保存在 MapReduce 集群的磁盘上. MapReduce master 在调度 map 任务时会把输入文件的位置信息也考虑进来, 尽量把 map 任务调度到对应数据副本所在机器上, 如果该项尝试失败, 则将 map 任务调度到离着输入数据比较近的(同一局域网或同一个交换机连接的网络)机器上. 在大型 MapReduce 计算过程中, 数据局部性可以极大地减少网络消耗. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#34-数据局部性"},{"categories":null,"content":"3.5 任务颗粒度map 任务数 M 和 reduce 任务数 R 加起来要远大于 worker 机器数. 一般一个 worker 同时执行多个任务, 这可以提升动态负载均衡. master 要做出 $O(M + R)$ 调度策略, 在内存中持有 $O(M * R)$ 个状态(每个 map/reduce 对对应状态大约一个字节). 用户一般想要控制 R 的大小, 因为每个 reduce 任务单独输出一个文件, 控制 R 可以控制最终文件个数. 我们倾向于选择大的 M 以使得每个 map 任务处理的数据量在 16MB 到 64MB 之间, 令 R 为 workers 的一个很小的倍数. 比如, 如果有 2,000 workers, 那么 R 选为 5,000, 而 M 选为 200,000. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:5:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#35-任务颗粒度"},{"categories":null,"content":"3.6 后备任务拖长 mapreduce 计算时间的就是最后完成 map 或者 reduce 任务的机器. 原因一般是机器某些硬件比较差, 比如磁盘 IO 很慢; 或者集群调度系统(除了调度 MR 任务也调度其它的)把很多任务调度到了最后几个任务所在机器上导致资源争用严重. 我们有一个通用的缓解拖后腿问题的机制: 当一个 mapreduce 操作接近完成的时候 master 就会针对仍处于执行阶段的任务调度对应的后备任务, 不管主任务还是后备任务结束, 相关任务就会被标记为完成. 该机制大幅减少了大型 mapreduce 任务的时间消耗, 而资源消耗仅增加几个点. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:6:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#36-后备任务"},{"categories":null,"content":"4 调优尽管对大部分需求, 写写 Map 和 Reduce 函数就够了, 但还是发现了一些可以优化的地方. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#4-调优"},{"categories":null,"content":"4.1 分区函数用户在指定 reduce tasks 个数(也即输出文件个数) R 的值的时候, 也可以指定分区函数, 即如何根据中间数据 key 将数据分散到这 R 个文件. 默认的分区函数就是 hash(key) mod R. 用户可以基于具体需求指定分区函数, 比如当中间 key 是 URL 时候, 如果想把同一个网站的数据刚到同一个文件, 则可以这样 hash(Hostname(urlkey)) mod R. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#41-分区函数"},{"categories":null,"content":"4.2 顺序保证前面讲了, reduce task 在调用 Reduce 函数之前会将本分区数据就行排序, 所以可以保证一个分区内的中间数据会按照升序处理. 这使得为每个分区生成有序输出文件变得简单, 而且也使得针对在每个分区文件进行随机 key 查询变得高效(有序就可以二分查找了). ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#42-顺序保证"},{"categories":null,"content":"4.3 合并函数(combiner function)在某些情况下, Reduce 函数满足交换性和结合性, 比如 word counting, 此时可以在每个中间 kv 通过网络传输给 Reduce 函数之前做一件事情, 即允许用户指定一个可选的 Combiner 函数, 它负责在网络传输前先对部分数据进行合并再发送, 这可以大幅减少网络数据交互量. Combiner 函数在执行 map task 的机器上运行, 因为它针对的是 map 生成的数据. 一般情况下, Combiner 函数代码与 Reduce 函数代码就是同一份. 这两者唯一不同就是 MapReduce 库如何处理它们的输出上. Reduce 函数的输出被写到最终输出文件上, 但是 Combiner 函数输出被写到一个中间文件, 该文件内容将会被发给 reduce task. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#43-合并函数combiner-function"},{"categories":null,"content":"4.4 输入输出类型MapReduce 库支持读取多种不同的文件类型. 比如 “text” 类型, 把每一行当作一个 key/value 对: key 是文件偏移量, value 是偏移处一行文件内容. 不管哪种文件类型, 库里对应的代码都知道如何把数据切分成有意义的范围给对应的 map task 去处理. 当然用户也可以实现相应接口来定制支持特定的文件类型. MapReduce 对输出类型的支持也像输入一样灵活. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#44-输入输出类型"},{"categories":null,"content":"4.5 跳过坏记录有时候用户编写的代码或者依赖的第三方库有 bugs, 导致 mapreduce 任务处理不了某些数据总是挂掉, 第一种情况还好说, 第三方库的就不好搞了. 这时候你可能希望能跳过这些记录. 怎么做到呢? 每个工作进程可以安装一个信号处理器用来捕获段错误或者总线错误. 在调用用户编写的 Map 或者 Reduce 之前, MapReduce 库存储一个序列号到全局变量中. 如果用户代码生成一个信号, 那么前面提到的信号处理器就会发送一个包含前述序列号的 UDP 包给 master. 如果 master 发现, 针对某个记录已经收到不止一次上报了, 它就会在重新执行挂掉的 Map/Reduce task 时跳过这条记录. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:5:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#45-跳过坏记录"},{"categories":null,"content":"4.6 状态信息master 除了做任务调度, 还提供了一个 HTTP server, 用户可以访问它来获取整个集群的状态统计信息. 比如计算进展, 每个 task 的输出等等. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:6:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#46-状态信息"},{"categories":null,"content":"4.7 全局计数器MapReduce 库提供了计数器设施, 用户可以利用它在 Map/Reduce 函数中针对一些事件进行统计, 比如在单词计数应用中针对大写的单词进行统计: Counter* uppercase; uppercase = GetCounter(\"uppercase\"); map(String name, String contents): for each word w in contents: // 如果当前单词大写, 则将全局计数器加 1 if (IsCapitalized(w)): uppercase-\u003eIncrement(); EmitIntermediate(w, \"1\"); 这些计数值会周期性地随心跳响应传递到 master, 然后 master 进行聚合, 聚合时 master 会去重(比如重复调度执行的任务或为了加速完成而启动的后备任务都会造成重复计数). 用户可以从前面提到的 http server 页面查看值的变化. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:7:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#47-全局计数器"},{"categories":null,"content":"5 性能下面以排序程序为例, 说明下各阶段数据传输速率比较以及后备任务对性能的影响. 如上图所示, 横向分为三部分, 分别是正常执行情况, 无后备任务执行情况, 手动干掉 200 个进程的执行情况. 其中, 每种执行情况纵向列出三个指标, 分别是输入数据速率, 排序数据速率, 输出数据速率. 具体每个图, 横轴是时间, 纵轴是速率. a 列上图是数据读取速率, 显著快于下面的排序和输出, 这全都拜前面提到的数据局部性所赐. a 列中图是排序, 可以看到第一个 map task 完成后即启动了排序. 第一个高峰是 1700 个 reduce tasks 执行盛况(这个 sort 程序用了 1700 台机器, 每个机器执行不超过 1 个 reduce task.), 大约 300 秒后第一批数据排序完成, 然后对剩下数据进行排序, 大约 600 秒时完成全部排序任务. 从该图可以看出数据传输速率高于下面的输出, 原因是下面输出要写到 GFS 多副本, 比较耗时. a 列下图是输出, 输出就是 reduce tasks 将数据写入到 GFS. 可以看到第一批数据排序完成到开始输出有一个延迟, 原因是这段时间内机器忙着排序中间数据. b 列下图显示最终完成时间要显著多余 a 列, 原因是最后 5 个 reduce tasks 严重拖后腿了, 整体耗时增长 44%. 这可以看出后备任务的对性能的显著提升. c 列上图显示显示手动干掉 200 个进程(机器还正常运行)后速率变成负的了, 原因是部分 map tasks 丢了, 需要重跑. 集群调度器快速在这些机器上重新运行相关任务, 最后 c 列下图显示仅仅比正常情况多了 5% 耗时. –End– ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#5-性能"},{"categories":null,"content":"迭代器的设计和实现是 leveldb 的精华之一. 前几篇文章都多少提到了迭代器的使用, 本篇让我们深入一下迭代器的设计实现, 也为接下来的几篇剖析打下基础. ","date":"2021-02-05","objectID":"/leveldb-annotations-4-iterator/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之四: 迭代器设计与实现","uri":"/leveldb-annotations-4-iterator/#"},{"categories":null,"content":"1 迭代器接口设计迭代器接口类为 leveldb::Iterator, 位于 include/leveldb/iterator.h 和 table/iterator.cc. (实现位于 table 目录, 是因为接下来要介绍的 sstable 是迭代器重度用户.) 迭代器接口定义比较简洁, 主要方法为指向合法性判断, 前后移动, 定位(开头/末尾/任意), 提取数据项 key/value 等等. 唯一的数据成员为清理函数列表头节点. 具体如下: class LEVELDB_EXPORT Iterator { public: Iterator(); // 禁用复制构造 Iterator(const Iterator\u0026) = delete; // 禁用赋值构造 Iterator\u0026 operator=(const Iterator\u0026) = delete; virtual ~Iterator(); // 一个迭代器要么指向 key/value 对, 要么指向非法位置. // 当且仅当第一种情况才为 valid. virtual bool Valid() const = 0; // 将迭代器移动到数据源的第一个 key/value 对. // 当前仅当数据源不空时, 调用完该方法再调用 Valid() 为 true. virtual void SeekToFirst() = 0; // 将迭代器移动到数据源的最后一个 key/value 对. // 当前仅当数据源不空时, 调用完该方法再调用 Valid() 为 true. virtual void SeekToLast() = 0; // 将迭代器指向移动到数据源 target 位置或之后的第一个 key. // 当且仅当移动后的位置存在数据项时, 调用 Valid() 才为 true. virtual void Seek(const Slice\u0026 target) = 0; // 将迭代器移动到数据源下一个数据项. // 当且仅当迭代器未指向数据源最后一个数据项时, 调用完该方法后调用 Valid() 结果为 true. // 注意: 调用该方法前提是迭代器当前指向必须 valid. virtual void Next() = 0; // 将迭代器移动到数据源前一个数据项. // 当且仅当迭代器未指向数据源第一个数据项时, 调用完该方法后调用 Valid() 结果为 true. // 注意: 调用该方法前提是迭代器当前指向必须 valid. virtual void Prev() = 0; // 返回当前迭代器指向的数据项的 key, Slice 类型, 如果使用迭代器进行修改则会反映到 // 已返回的 key 上面. // 注意: 调用该方法前提是迭代器当前执行必须 valid. virtual Slice key() const = 0; // 返回当前迭代器指向的数据项的 value, Slice 类型, 如果使用迭代器进行修改则会反映到 // 已返回的 value 上面. // 注意: 调用该方法前提是迭代器当前执行必须 valid. virtual Slice value() const = 0; // 发生错误返回之; 否则返回 ok. virtual Status status() const = 0; // 我们允许调用方注册一个带两个参数的回调函数, 当迭代器析构时该函数会被自动调用. using CleanupFunction = void (*)(void* arg1, void* arg2); // 我们允许客户端注册 CleanupFunction 类型的回调函数, 在迭代器被销毁的时候会调用它们(可以注册多个). // 注意, 跟前面的方法不同, RegisterCleanup 不是抽象的, 客户端不应该覆写他们. void RegisterCleanup(CleanupFunction function, void* arg1, void* arg2); private: // 清理函数被维护在一个单向链表上, 其中头节点被 inlined 到迭代器中. // 该类用于保存用户注册的清理函数, 一个清理函数对应一个该类对象, 全部对象被维护在一个单向链表上. struct CleanupNode { // 清理函数及其两个参数 CleanupFunction function; void* arg1; void* arg2; // 下个清理函数 CleanupNode* next; // 判断清理函数是否为空指针. bool IsEmpty() const { return function == nullptr; } // 运行调用方通过 Iterator::RegisterCleanup 注册的清理函数 void Run() { assert(function != nullptr); (*function)(arg1, arg2); } }; // 清理函数列表的头节点 CleanupNode cleanup_head_; } Iterator 本身实现了三个方法, 分别是构造方法, 析构方法, 以及清理函数注册方法. 下面是非抽象方法的实现: // 构造方法, 初始化唯一数据成员 Iterator::Iterator() { cleanup_head_.function = nullptr; cleanup_head_.next = nullptr; } Iterator::~Iterator() { // 析构时调用已注册的清理函数 if (!cleanup_head_.IsEmpty()) { // 线性的, 如果在该迭代器上注册的清理函数太多了应该会影响性能, 但总要做释放操作, 时间总归省不了. cleanup_head_.Run(); for (CleanupNode* node = cleanup_head_.next; node != nullptr; ) { node-\u003eRun(); CleanupNode* next_node = node-\u003enext; delete node; node = next_node; } } } // 将用户定制的清理函数挂到单向链表上, 待迭代器销毁时挨个调用(见 ~Iterator()). void Iterator::RegisterCleanup(CleanupFunction func, void* arg1, void* arg2) { assert(func != nullptr); CleanupNode* node; if (cleanup_head_.IsEmpty()) { node = \u0026cleanup_head_; } else { node = new CleanupNode(); // 新节点插到 head 后面 node-\u003enext = cleanup_head_.next; cleanup_head_.next = node; } node-\u003efunction = func; node-\u003earg1 = arg1; node-\u003earg2 = arg2; } ","date":"2021-02-05","objectID":"/leveldb-annotations-4-iterator/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之四: 迭代器设计与实现","uri":"/leveldb-annotations-4-iterator/#1-迭代器接口设计"},{"categories":null,"content":"1.1 迭代器实现一例下面以 sstable 的 block 为例示意一下迭代器的实现. 关键部分就是将 block 作为迭代器数据源, 基于 block 构造和查询原理实现迭代器的前后移动, 定位等操作. 里面涉及了一些成员看不懂也不用管, 在介绍 sstable 时会解释. 具体类为 leveldb::Block::Iter, 代码位于 table/block.cc 文件: class Block::Iter : public Iterator { private: // 迭代时使用的比较器 const Comparator* const comparator_; // 指向 block 的指针 const char* const data_; // block 的 restart 数组 // (每个元素 32 位固定长度, 保存着每个 restart 在 block 里的偏移量) // 在 block 里的起始偏移量 uint32_t const restarts_; // restart 数组元素个数(每个元素都是 uint32_t 类型) uint32_t const num_restarts_; // current_ 表示当前数据项在 data_ 里的偏移量, // 如果迭代器无效则该值大于等于 restarts_ // 即 restart array 在 block 的起始偏移量 // (restart array 位于 block 后部, 数据项在 block 前半部分) uint32_t current_; // restart block 的索引值, current_ 指向的数据项落在该 block uint32_t restart_index_; // current_ 所指数据项的 key std::string key_; // current_ 所指数据项的 value Slice value_; // 当前迭代器对应的状态 Status status_; inline int Compare(const Slice\u0026 a, const Slice\u0026 b) const { return comparator_-\u003eCompare(a, b); } // 返回 current_ 所指数据项的下一个数据项的偏移量. // 根据 Block 布局我们可以知道, value 位于每个数据项最后, // 所以 value 之后第一个字节即为下一个数据项起始位置. inline uint32_t NextEntryOffset() const { return (value_.data() + value_.size()) - data_; } // 返回索引值为 index 的 restart 在 block 中的起始偏移量 uint32_t GetRestartPoint(uint32_t index) { assert(index \u003c num_restarts_); return DecodeFixed32(data_ + restarts_ + index * sizeof(uint32_t)); } // 将迭代器移动到索引值为 index 的 restart 对应的偏移量位置. // 注意, 此方法只调整了 current_ 对应的 value_, 此时两者不再保持一致; // current_ 与 key_ 仍然保持一致性. void SeekToRestartPoint(uint32_t index) { key_.clear(); restart_index_ = index; // current_ 和 key_ 指向后续会被 ParseNextKey() 校正. // ParseNextKey() 从 value_ 末尾开始, 所以这里需要设置好, 为何从 value_ // 末尾开始呢? 根据 Block 布局我们可以知道, value 位于每个数据项最后, // 所以 value 之后第一个字节即为下一个数据项起始位置. uint32_t offset = GetRestartPoint(index); // 将 value 数据起始地址设置为 offset 对应的 restart 起始位置, // value_ 这么设置是为了方便 ParseNextKey(). value_ = Slice(data_ + offset, 0); } public: Iter(const Comparator* comparator, const char* data, uint32_t restarts, uint32_t num_restarts) : comparator_(comparator), data_(data), restarts_(restarts), num_restarts_(num_restarts), current_(restarts_), restart_index_(num_restarts_) { assert(num_restarts_ \u003e 0); } virtual bool Valid() const { return current_ \u003c restarts_; } virtual Status status() const { return status_; } virtual Slice key() const { assert(Valid()); return key_; } virtual Slice value() const { assert(Valid()); return value_; } virtual void Next() { // 向后移动前提是当前指向合法 assert(Valid()); ParseNextKey(); } // 将 current 指向当前数据项前一个数据项. // 如果 current 指向的已经是 block 第 0 个数据项, 则无须移动了; virtual void Prev() { // 向前移动前提是当前指向合法 assert(Valid()); // 倒着扫描, 直到 current_ 之前的一个 restart point. // current_ 大于等于所处 restart 段起始地址, 下面要做的 // 是寻找 current_ 之前的一个 restart point. // 把 current_ 当前取值作为原点. const uint32_t original = current_; // 下面循环干一件事, 定位 current_ 前一个数据项, 具体分两种情况： // - 如果 current_ 大于所处 restart 段起始地址, 不进行循环, // 到下面去直接定位 current_ 前一个数据项即可. // - 如果 current_ 等于所处 restart 段起始地址, // - 如果当前 restart 不是 block 的首个 restart, // 则 current_ 前一个数据项肯定位于前一个 restart 最后一个位置 // - 如果当前 restart 是 block 的首个 restart, // 则 current_ 就是 block 首个数据项, 所以没有所谓前一个数据项了 // - 没有其它情况. // 循环能够执行的唯一条件就是相等 while (GetRestartPoint(restart_index_) \u003e= original) { // 倒到开头的 restart point 了, 没法再向前倒了, 也就是没有 pre 了. if (restart_index_ == 0) { // current_ 置为同 restarts_, // 即使得它位于 block 首个 restart 的首个数据项处. current_ = restarts_; // 将 restart_index_ 置为 restart point 个数, // 这个索引是越界的. restart_index_ = num_restarts_; return; } // 倒车, 请注意. restart_index_--; } // 粗粒度移动, 即先将 current_ 移动到指定 restart 分段 SeekToRestartPoint(restart_index_); do { // 细粒度移动, 将 current_ 移动到 original (current_ 移动之前的值)的前一个数据项 } while (ParseNextKey() \u0026\u0026 NextEntryOffset() \u003c original); } // 寻找 block 中第一个 key 大于等于 target 的数据项. // 先通过二分法在 restart 段级定位查找目标段, 存在 // key \u003c target 且是最后一个 restart 段; // 然后在目标段进行线性查找找到第一个 key 大约等于 target 的数据项. // 如果存在则 current_ 指向该目标数据项; 否则 current_ 指向 // 一个非法数据项. // 调用者需要检查返回结果以确认是否找到了. virtual void Seek(const Slice\u0026 target) { // 在 restart array 中进行二分查找, 找到最后一个 // 存在 key 小于 target 的 restart, 注意是小于,","date":"2021-02-05","objectID":"/leveldb-annotations-4-iterator/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之四: 迭代器设计与实现","uri":"/leveldb-annotations-4-iterator/#11-迭代器实现一例"},{"categories":null,"content":"2 双层迭代器设计双层迭代器, 对应的类为 class leveldb::\u003cunnamed\u003e::TwoLevelIterator, 位于 table/two_level_iterator.cc 文件. 它的父类为 leveldb::Iterator, 所以表现出来的性质是一样的. 该类设计比较巧妙, 这主要是由 sstable 文件结构决定的. 具体地, 要想在 sstable 文件中找到某个 key/value 对, 肯定先要找到它所属的 data Block, 而要找到 data Block 就要先在 index block 找到其对应的 BlockHandle. 双层迭代器就是这个寻找过程的实现. 该类包含两个迭代器封装： 一个是 index_iter_, 它指向 index block 数据项. 针对每个 data block 都有一个对应的 entry 包含在 index block 中, entry 包含一个 key/value 对, 其中： key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 的第一个 key 的(比较拗口)字符串; value 是指向一个对应 data block 的 BlockHandle. 另一个是 data_iter_, 它指向 data block 包含的数据项. 至于这个 data block 是否与 index_iter_ 所指数据项对应 data block 一致, 那要看实际情况, 不过即使不一致也无碍. 示意图如下: 这两个迭代器, 可以把 index_iter 看作钟表的时针, 指向具体小时, 可以把 data_iter_ 看作更精细的分针, 指向当前小时的具体分钟. 两个指针一起配合精确定位到我们要查询的数据. 这么说其实就能大体上猜出来, 迭代器前后移动, 定位等等这些方法是如何实现的了, 简单说就是先移动 index_iter_ 再移动 data_iter_. 以 Seek() 方法举例来说: // 根据 target 将 index_iter 和 data_iter 移动到对应位置 void TwoLevelIterator::Seek(const Slice\u0026 target) { // 因为 index block 每个数据项的 key 是对应 data block 中最大的那个 key, // 所以 index block 数据项也是有序的, 不过比较\"宏观\" . // 先找到目标 data block index_iter_.Seek(target); // 根据 index_iter_ 设置 data_iter_ InitDataBlock(); // 然后在目标 data block 找到目标数据项 if (data_iter_.iter() != nullptr) data_iter_.Seek(target); // data_iter_.iter() 为空则直接向前移动找到第一个不为空的 // data block 的第一个数据项. SkipEmptyDataBlocksForward(); } 可以看出, 双层迭代器设计具有分形的思想, 迭代器是由迭代器构成的. 其它方法实现原理类似, 不再赘述. –End– ","date":"2021-02-05","objectID":"/leveldb-annotations-4-iterator/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之四: 迭代器设计与实现","uri":"/leveldb-annotations-4-iterator/#2-双层迭代器设计"},{"categories":null,"content":"本文基于内部分享 \u003c“抄\"能力养成系列 – GFS 设计\u003e 整理. 2003 年开始 Google 陆续放出三套系统的设计(GFS/MapReduce/Bigtable), 在互联网届掀起云计算狂潮一直影响至今. 该论文一出, 便催生了 Hadoop 中的 HDFS 的诞生. GFS 作为发轫, 目前许多业界知名的分布式系统设计仍然有着它的影子. 下面就让我们一起看看 GFS 的设计, 希望为各位后续系统研发提供灵感。(Salute to Jeff). ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:0:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#"},{"categories":null,"content":"1 GFS 诞生背景 1, 组件失效是常态而非例外 2, 按传统标准, 现实世界的文件太大了, 一般都几个 GB, 个数多了单机存不下. 3, 大多数文件通过 append 而非覆盖进行修改, 所以追加操作是性能优化和需要原子保证的焦点. 4, 将上层应用和文件系统 API 联合设计可增加灵活性, 让整个系统都受益. 比如通过放松 GFS 的一致性要求以大规模简化文件系统还不给应用增加负担. 同时也引入了一个原子化的 append 操作以让多个客户端针对同一个文件并发执行 append 操作而无需额外的同步措施. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:0:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#1-gfs-诞生背景"},{"categories":null,"content":"2 GFS 设计概览","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:0:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#2-gfs-设计概览"},{"categories":null,"content":"2.1 几点假设 硬件便宜爱故障, 监控完善恢复快. 文件个数适中, 以大文件为主, 支持但不优化小文件存储(tradeoff). 负载主要是大数据量的流式读取和小数据量的随机读取, 后者可以合并重排减少随机. 另一个重要负载是大数据量顺序 append 数据到文件, 也支持随机写但没做优化, 文件一旦写完几乎不再修改. 设计良好且高效实现的单文件并发 append 操作. GFS 文件经常用于生产者消费者队列或多路合并. 持续稳定的高带宽比时延更加重要. 大多数应用更看重高吞吐而非某个读写操作的响应时间. (long-fat network) ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#21-几点假设"},{"categories":null,"content":"2.2 接口 GFS 支持 create/delete/open/close/read/write 操作但不支持 POSIX 规范(GlusterFS 支持). Snapshot: 就是以一个很低的消耗创建一个文件或者目录树的拷贝. record append 操作: 允许多个客户端并发地向同一个文件追加数据, 每个追加操作都是原子的, 这为实现生产者-消费者队列或者多路合并提供了便利, 因为多个客户端同时写同一个文件无需加锁. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#22-接口"},{"categories":null,"content":"2.3 架构Figure 1 为 GFS 架构一览. 一个 GFS 集群由 1 个 master 和多个 chunkserver 构成, 集群可同时被多个 client 访问. Client 代表应用和 master 还有 chunkserver 通信. 其中文件相关的有: 文件被切成固定大小的 chunks(每个 chunk 还会被切成 blocks, 后述). 每个chunk 都有一个全局不可变的 64 bit 的 chunk handle, 这个 handle 由 master 在 chunk 创建时分配. 读写 chunk 时需要指定 handle 和字节范围. 为了高可用, 每个 chunk 会被在多个 chunkserver 上保存各保存一个副本, 默认三副本. master 相关的有: master 保存整个文件系统的元信息, 包括命名空间/访问控制信息/从 files 到 chunks 的映射/chunks 当前位置. master 也控制系统层面的活动如 chunk 租约管理/孤儿 chunks 的垃圾回收/chunkservers 之间的 chunk 迁移. master 和各个 chunkservers 通信(心跳)以发送指令和收集状态信息. client 和 chunkserver 都不会缓存文件数据: 就 client 而言, 一方面原因是应用程序一般流式拉取巨大文件或者数据太大无法进行缓存, 另一方面因为无需处理缓存一致性问题可以简化 client 和整个系统的复杂度. 针对 chunkserver 而言, 因为数据本来就存在 chunkserver 本地磁盘, 可以直接使用 linux 的 buffer 来实现经常被访问的数据的缓存, 单机的缓存就交给单机操作系统来处理了. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#23-架构"},{"categories":null,"content":"2.4 单 master 集群各组件交互重点注意, GFS 一个集群只有一个 Master(关于它的高可用后面描述). master 和 chunkserver 之间周期性的心跳交互. client 读取时先问 master 数据存在哪个 chunkserver, 拿到信息后缓存到本地(有超时时间限制), 然后直接和 chunkserver 通信. client 把上层应用指定的文件名和偏移量翻译成 chunk index, 发给 master. master 返回 handle 和 location, client 用文件名和 index 做 key 缓存该信息. 直到元信息缓存失效或者应用重新打开文件, client 都无需和 master 交互, 而是直接和 chunkserver 交互. 这可以大幅降低 master 负载. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#24-单-master-集群各组件交互重点"},{"categories":null,"content":"2.5 chunk 大小设计 chunk 大小非常关键, GFS 选的是 64MB. 比传统的文件块大多了. 采用 lazy space 分配策略, 可以避免如此大的 chunk size 导致的空间浪费. 每个 chunk 都作为一个普通的文件存储在 chunkserver. chunk size 选的大有如下好处: 1, 大幅减少了 client 和 master 的交互, 因为典型的应用就是顺序读取大文件, chunk 大则要访问的 chunks 就少, 就不用频繁与 master交互. client 侧可以为数 TB 数据集数据缓存它们对应的 chunk 元信息. 2, 因为 chunk 很大,可以覆盖很多操作, 这样 client 跟其所在 chunkserver 长时间维持一个持久 TCP 连接而无需频繁与多个 chunkserver 新建链接, 这就减少了网络开销. 3, 因为 chunk 很大, 个数就会减少, 则对应的元信息也相应减少, 这样 master 可以在内存缓存全部元信息. 这么大的 chunk size 也有缺点, 就是对小文件不友好: 因为一个小文件可能只对应一个 chunk, 如果针对小文件并发操作很多, 那么它就会成为热点(redis hotkey 与之类似.). 因为实际应用中以大文件读写为主所以问题不严重. 目前解决办法就是针对热点小文件提升其副本因子, 并且把访问该文件的客户端交错启动. 长远的解决办法是允许客户端能从其它客户端而不仅仅是从 chunkserver 读取同样的数据. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:5:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#25-chunk-大小设计"},{"categories":null,"content":"2.6 元信息master 保存三种类型的元信息: 文件和 chunk 命名空间 file-to-chunk mapping 每个 chunk 副本的位置信息 master 把全部元信息都保存在的内存中: 前两种元信息会被 master 通过本地日志持久化变更同时备份到远程机器确保可用性. chunk 位置信息不会被持久化, 而是在 master 启动以及 chunkserver 加入集群时询问 chunkserver 有关 chunk 的位置信息. chunkserver 保存的元信息有两个: 每个 block 的校验和, 以及 chunk 版本号. 在 master 上, 每个文件对应的元信息大约 100 字节, 这也符合 master 内存不会成为系统容量瓶颈的预期. 而且这 100 字节大部分是文件名(已经过前缀压缩). 其它元信息还有文件归属/权限, 从文件到 chunks 的映射, chunk 副本的位置信息, 每个 chunk 的当前版本. 针对 chunk 还会保存一个引用计数用于实现 COW(copy-on-write). master 和 chunkserver 每个都保存大约 50 到 100 MB 数据, 加载非常快, 但是由于 master 启动时要拉取chunk location 所以启动需要额外 30 到 60 秒. 自从 master 内存结构改成二叉搜索树以后, 命名空间搜索不再是瓶颈. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:6:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#26-元信息"},{"categories":null,"content":"2.6.1 元信息都保存在内存中master 把元信息存在内存, 操作就会很快. master 可以在后台高效地周期性扫描整个状态空间以实现: chunk GC, 应对 chunkserver 失效重新分配副本, 为实现负载和磁盘空间均衡在 chunkservers 间迁移 chunks. master 把全部信息保存到内存有个不好的地方就是集群数据量受限于 master 内存大小. 不过因为每个 chunk(64MB 大) 对应元信息不超过 64 字节, 所以实践中不是啥问题. 大多数 chunks 都是满的因为大多数文件各自都包含多个 chunks, 可能仅每个文件对应的最后一个 chunk 不满, 这就让元信息性价比非常高. 同样, 针对每个文件, 它的文件命名空间数据也不超过 64 字节, 因为其存储的文件名通过使用前缀压缩非常紧凑. 当然, 想支持更大的数据集合, 给 master 加内存就行了, 便宜又快捷. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:6:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#261-元信息都保存在内存中"},{"categories":null,"content":"2.6.2 chunk 位置信息虽然 master 不持久化 chunk 位置信息, 但是因为它负责 chunks 布局同时通过心跳监控 chunkserver, 所以它能保持这些位置信息时刻保持更新. 其实我们开始也尝试持久化 chunk 位置信息, 后来发现还是 master 启动后周期性拉取更简单. 这消除了因为 chunkserver 加入或离开集群/改名/失效/重启等等而保持 master 和 chunkservers 数据同步的问题. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:6:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#262-chunk-位置信息"},{"categories":null,"content":"2.6.3 操作日志master 上的操作日志包含了关键元信息的变更历史. 这对 GFS来说至关重要. 这个日志作为逻辑时间线定义了并发操作的顺序. 文件和 chunks 以及它们的版本号, 可以永久通过它们被创建的逻辑时间被唯一识别. 元信息变更被持久化到本地和远程多台机器后, 相关变更才对客户端可见. master 启动时会读取操作日志并进行重放以恢复系统状态, 所以操作日志不能太大否则启动时间会非常长. 为了避免日志文件过大, master 会在日志文件超过一定大小后 checkpoint 自己当前状态, 这样下次启动时只需从本地磁盘加载和重放最近一次 checkpoint 就可以恢复到最新状态. 因为构造 checkpoint 需要花时间, 为了避免阻塞后续处理, 方法如下: master 切换到新的日志文件并在一个单独的线程中创建 checkpoint. 新的 checkpoint 包含日志切换前的全部变更. 大约一分钟左右就可以为一个拥有几百万文件的集群创建一个 checkpoint. 创建完成后它将被写入本地和远程磁盘. master 恢复过程只需最近的 checkpoint 和创建该 checkpoint 后的日志文件. 老的 checkpoints 和日志文件都会被删除. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:6:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#263-操作日志"},{"categories":null,"content":"2.7 GFS 的一致性模型GFS 提供了一个弱一致性模型, 相对简单且容易高效实现. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:7:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#27-gfs-的一致性模型"},{"categories":null,"content":"2.7.1 GFS 提供的保证文件命名空间变更, 如创建文件, 是原子的. 它们均仅由 master 处理. master 的操作日志为这些操作定义了全局顺序. consistent: 针对某个文件区域, 如果全部客户端看到的数据是一致的, 不管它们是从哪个副本读取的数据, 我们就说这个文件区域数据是一致的. defined: 针对经历过数据变更的某个文件区域, 如果它是 consistent 的, 并且客户端能看到前述变更的完整内容, 即可预测, 不会因并发而随机那么我们就说这个文件区域是确定的. 注意, defined 和 consistent 是两个层面的东西, 只要写成功, 那么 GFS 的 2PC 能保证 consistent, 也就是 defined 包含了 consistent. 但如果发生了并发变更, 比如多个客户端针对某个文件区域同一个偏移并发写, 此时不同于追加, 这种写操作是互相覆盖的, 最终结果是 undefined 的, 即我们无法预先知晓该偏移处结果是什么, 但 GFS 可以保证各个副本都是同样的 undefined 状态. 上面 Table 1 展示了 GFS 两种典型的变更操作 write(覆盖写) 和 record append(记录追加)在串行和并发情况下完成后对应的文件区域的状态. 数据变更包括 write(在指定偏移处写入, 如覆盖写)和 record append(即在文件尾部原子地追加记录). GFS 会保证一系列成功变更后的文件是 defined 的, 措施如下: 1, 以同样的顺序将变更应用到 chunk 全部副本. 2, 使用 chunk 版本号来检测过期的副本, 相关 chunkserver可能因为下过线错过某些变更. 过期副本不会对外服务而是会被尽快 GC 掉. master 借助心跳计算每个chunkserver 上数据的校验和以检测数据是否损坏.发现损坏后会尽快从好的副本恢复数据. 如果全部副本丢失, 那么chunk 就不可恢复了. 这种情况下应用得到的响应是数据丢失而非损坏. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:7:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#271-gfs-提供的保证"},{"categories":null,"content":"2.7.2 一致性模型在应用端实现写 依赖于追加而非覆写(相比于覆写, 追加更加高效同时对应用错误更富有弹性), 应用程序一般就是一个文件从头 append 到尾; 要么等数据写完后原子地将文件重命名为一个永久性的名字, 要么周期性地 checkpoint 写了多少字节了同时还可以附加一个应用层校验和. checkpoint 让 writers 可以在重启后增量写入而不用重写全部数据, 同时避免 readers 处理已成功写入但不完整的内容(站在应用层角度). 读 只验证和处理最后一个 checkpoint 之前的文件区域, 这些区域处于 defined 状态. record-append 操作的 append-at-least-once 语义确保了不丢数据, 然后由 readers 负责处理重复数据. 由于 record 包含了一个附加的校验和, 所以 readers 可以此校验 records. readers 还可以通过每个记录的唯一 ID 过滤掉重复的 records. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:7:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#272-一致性模型在应用端实现"},{"categories":null,"content":"2.8 系统交互GFS 在设计的时候就在尽量做到最小化 master 参与各种操作, 给 master 减负. 下面描述 client/master/chunkserver 三者之间如何交互以实现数据变更/原子化记录追加/快照. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:8:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#28-系统交互"},{"categories":null,"content":"2.8.1 lease 和 mutation 顺序chunk 的每个变更都会反映到每个副本上, 具体如下: master 通过选择一个副本颁发一个 lease 将其指定为 primary. 然后 primary 为 chunk 的并发变更选择一个串行的顺序, 全部副本都遵循该顺序应用变更. 因此全局变更顺序首先由 master 授权 lease 的顺序以及每个 lease 生效期 primary 指定序列号时确定. lease 机制可以最小化 master 的管理开销, 因为针对 chunk 的一部分管理工作让 primary 承担了. 每个 lease 都有一个初始的 60 秒超时, 但是只要 chunk 仍在变更, primary 可以发送请求给 master 要求延长 lease 有效期, 这个过程是通过 heartbeat 实现的. 当然 master 可以在 lease 过期之前吊销它, 比如当 mast 想禁止对正在改名的文件进行变更时. 当 master 失去与primary 通信时, master 可以授权新的 lease 给另一个副本. Figure 2 为执行写操作时的控制流, 具体如下: 1, client 询问 master 哪个 chunkserver 持有要访问的 chunk 的 lease 以及其它副本的位置. 如果没人持有 lease, master 会选一个副本授权之. 2, master 返回 primary 的 id 以及和其它副本的位置, client 会缓存这些信息, 仅当 primary 不可用或者 primary 明确告知不再持有 lease 时再和 master 通信. 3, client 以任意顺序将数据推给全部副本. 每个 chunkserver 将数据首先保存到内部的 LRU 缓存. 图中将控制流和数据流解耦, 基于网络拓扑(该拓扑不关心谁是 primary)调度数据流可以改善性能, 具体后述. 4, 当全部副本确认收到数据后, client 再发送一个写请求给 primary,该请求标记了上一步推送给全部副本的数据, primary 会为(可能来自多个 clients 的)全部变更分配连续的序列号,然后 primary 按照此顺序应用这些变更. 5, 应用本次变更后, primary 将该写请求转发给全部 secondary 副本,这些副本以同样的顺序应用变更. 6, secondary 副本回复 primary 自己已完成操作. 7, primary 回复 client 本次写操作成功 or 失败. 如果只有 primary 和部分 secondary 成功, 则再返回第一步重试之前会尝试重复 3-7. 从 3 到 7 其实就是 2PC (two-phase commit). 如果一个写操作跨多个 chunk, 那么客户端就会将这个写操作分裂成多个写操作. 它们都按照前面描述的步骤执行, 但是可能和其它客户端的写操作交织在一起并发执行. 因此同一个文件区域可能被多个客户端相互覆盖写入. 尽管这个文件区域的全部副本最终是 consistent, 但是最终结果我们无法预知是什么, 所以最终是 consistent 但 undefined. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:9:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#281-lease-和-mutation-顺序"},{"categories":null,"content":"2.8.2 数据流将数据流和控制流解耦, 让我们可以更高效地利用网络. 下面看看 GFS 是怎么做的. 就像控制流是从 client-\u003eprimary-\u003esecondaries 管道化传输, 数据流从 client 到各个 replicas 也是管道化传输. 具体地, client 挑选离自己最近的 chunkserver (注意这里根本不管谁是 primary, 只关注网络拓扑), 将数据发给它, 然后 这个 chunk server 一边接收 client 的数据一边转发给离自己最近的 chunkserver, 依此类推 … 从而链式完成数据从 client 到每个副本所在 chunkserver 的发送. 最近距离计算: 这个管道化传输过程中, 各个节点通过 IP 地址计算前面提到的 “最近”距离的计算, 比如同一个局域网跟定比跨网段的要更近一些. 管道化传输好处: 这种管道化传输尽可能地利用了每个 chunkserver 的出站带宽, 也最小化了 TCP 连接的时延. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:9:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#282-数据流"},{"categories":null,"content":"2.8.3 原子化的记录追加操作传统的写操作, 需要指定要写入到的文件偏移量. 并发执行该类操作写同样的区域并不是串行化的, 被写入区域最后状态包含多个客户端的数据片段. GFS 提供了原子化追加操作, 叫 record append, GFS 会确保至少一次写的语义, 将数据原子化地追加到文件末尾. 这类似于在 unix 编程中, 采用 O_APPEND 模式打开文件而且没有竟态条件. record append 作为一种 mutation, 执行流程同 Figure 2, 但在 primary 那有些许不同: 1, 首先 client 肯定是将数据 push 给文件的最后一个 chunk(因为是 appende 嘛). 2, primary 检查追加这个记录后 chunk 是否超过 64MB, 2.1, 如果超过, 则将这个chunk 填满无效数据, 同时告诉 secondaries 也这么干, 然后告诉客户端说满了, 请将数据写入下个chunk. 为了避免产生过多这类因填充导致的空间碎片, 我们要求 record 大小最多为 chunk 的四分之一. 2.2, 如果不超过, primary 将数据写入本地副本,然后告知secondaries 将数据写入到同样的偏移处, 然后响应客户端. 上述过程, 在任何副本写失败, 客户端就要重试该写入操作. 当然,这个重试会导致 chunk 不同副本数据不一致, 但是 GFS 保证的不是字节级别的一致性, 而是记录级别的, 它保证至少一次 append, 就能保证各副本数据一致, 虽然重复记录但是可以通过 id 去重. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:9:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#283-原子化的记录追加操作"},{"categories":null,"content":"2.9 Snapshot 快照snapshot 操作用于制作当前文件和目录的快照, 速度非常快. snapshot 用的是 COW (copy-on-write) 技术实现, 具体如下: 当master 收到 snapshot 请求时, 它首先吊销要进行 snapshot 的文件相关 chunks 的 lease, 这样后续针对这些 chunks 的写操作强迫 client 先和 master 交互, 这就为创建快照争取了时间. lease 吊销后, master 将 snapshot 操作记录到日志中, 然后将其应用到内存状态上制作快照. 新创建的 snapshot 文件指向源文件同样的 chunks, 计数加一. 此时如果客户端要写这些 chunks, master 就会察觉引用计数大于 1, 此时触发 COW 操作. COW 过程: master 指示每个拥有 chunk C 副本的 chunkserver 都新建一个 chunk C‘, 而且 C’ 创建在同 C 一样的 server 上, 这可以使得 COW 在本地而非通过网络进行. 创建完 C‘ 并拷贝完数据后, 集群就有两组一样的 chunk 副本了, 剩下的处理流程就同之前一样了, master 针对 C’ 授权 lease 给某个副本并响应客户端, 客户端开始写入数据. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:10:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#29-snapshot-快照"},{"categories":null,"content":"3 Master 行为Master 执行全部和命名空间有关的操作. 另外, master 管理着整个系统的 chunk 复制: chunk 布局决策 创建新 chunks 和其副本 协调多种系统级别的活动以保障 chunks 副本健全, 各个 chunkserver 的负载均衡, 回收未使用的存储空间 下面挨个讨论上述话题. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:0:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#3-master-行为"},{"categories":null,"content":"3.1 命名空间管理和锁机制GFS 允许多个操作并发, 通过锁来将其串行化. 读锁, 防止目标被删除/重命名/snapshotted; 写锁, 防止并发创建同名目标文件. 同 Unix 不同, GFS 没有为每个目录设计一个保存其文件列表的数据结构. GFS 的命名空间可以看作一个速查表, 保存了从全路径名(文件名或目录名, 可以看作一个前缀树)到元数据的映射. 通过前缀压缩, 可以使得整个表被保存到内存中. 命名空间树上面的每个节点都关联了一个读写锁. 由于没有真正的目录结构, 所以这使得我们可以并发地在同一个目录下创建文件, 只要获取这个所谓的目录的读锁(防止目标被修改)同时获取目标文件的写锁(防止在同一个目录下生成同名文件)即可. 为避免死锁, 加锁顺序全局一致: 先按照命名空间树不同层次对锁排序, 如果在同一层则按照字典序对锁排序. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#31-命名空间管理和锁机制"},{"categories":null,"content":"3.2 chunk 副本布局GFS 的分布式体现了多个层次: 1, 首先 GFS 集群一般涉及上百台 chunkservers 2, 这些 chunkservers 一般分布在多个机架. 所以同一个 chunk 的副本布局不但要考虑多 chunkservers 还要考虑这些 chunkservers 不能集中在同一个机架, 这么做就为了实现: 可靠性 可用性 最大化网络带宽使用率(机架进出带宽可能小于这个机架上的全部机器各自网卡带宽加总.), 比如热点副本集中到同一个 chunkserver 或者同一个机架, 并发读写就会出现瓶颈, 这在后面测量部分会再细述. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#32-chunk-副本布局"},{"categories":null,"content":"3.3 副本的新建, 重复制与再平衡","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#33-副本的新建-重复制与再平衡"},{"categories":null,"content":"3.3.1 副本新建三种情况下新建副本: 1, 新建 chunk 时 2, 重复制 3, 重平衡 新建 chunk 时新副本安置在哪儿, 主要考虑下面几点: 1, 尽量把新副本放在磁盘使用率低于平均水平的 chunkserver 上, 这样各个机器会逐渐平均. 2, 虽然副本创建操作本身消耗不多, 但它预示着大量的写流量即将到达. 所以新建副本时会尽量让每个 chunkserver 近期的副本(不区分 chunk)新建数尽量的差不多, 避免写流量涌入少数 chunkserver. 3, 就像前一节讨论的, 让副本尽量分布在多个机架上. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#331-副本新建"},{"categories":null,"content":"3.3.2 副本重复制当 chunk 副本因子低于设定时, master 就会触发重复制. 如果有多个 chunk 满足条件, 则执行优先级就是: 1, 哪个 chunk 距离目标副本因子越远就谁优先重复制; 2, 还有就是活跃的文件 chunks 优先级高于被删除文件的 chunks. 3, 另外就是优先那些当前阻塞住客户端的 chunks. 重复制后的副本放到哪个 chunkserver, 原则同副本新建所描述: 1, 让每个磁盘空间使用率尽量相同; 2, 限制同一个 chunkserver 上同时活跃的 clone 操作; 3, 另副本尽量分散到各个机架. 为了避免副本 clone 流量压倒客户端流量, master 会限制同时活跃的 clone操作个数. 同时, 每个 chunkserver 会限制自己用于 clone 的带宽, 方法就是限制自己到其它源 chunkserver 的读请求数目. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#332-副本重复制"},{"categories":null,"content":"3.3.3 副本重平衡master 会为了更好的磁盘使用率和负载均衡而周期性地做副本 rebalance. master 做 rebalance 时布局标准同重复制. rebalance 选择要移除哪个 chunkserver 的副本时倾向于那些空闲空间低于平均值的 chunkserver, 目的也是最终拉平各个 chunkserver 的磁盘使用. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#333-副本重平衡"},{"categories":null,"content":"3.4 垃圾回收","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#34-垃圾回收"},{"categories":null,"content":"3.4.1 惰性回收一个文件被删除后, GFS 不会立即回收它所占用地物理空间, 而是通过周期性地 GC 在文件和 chunk 两个层次进行惰性回收. 我们发现这么做使得整个系统更加简单也更加可靠. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#341-惰性回收"},{"categories":null,"content":"3.4.2 回收机制文件层面: 同其他操作, 删除操作会先记录到 master 日志. 然后被删除文件被标记为包含删除时间戳的隐藏名. master 周期性扫描文件系统命名空间时移除那些被标记超过三天(可配置)的文件. 真正移除之前这些文件仍可读, 甚至可以将名字改成正常名字表示不再删除. 当隐藏文件被移除后, 它对应的内存中的元数据也会被擦除, 这相当于断开了这个文件同其 chunks 的连接. Chunk 层面: 同样地, 针对 chunk 命名空间的周期性扫描, master 会将从任何文件都不可达的 chunks 标记为孤儿 chunks. chunkserver 在和 master 周期性的心跳消息中上报自己持有的 chunks, master 会回复那些不再存在的 chunks 标识, chunkserver 收到后自主删除这些 chunks 对应的副本. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#342-回收机制"},{"categories":null,"content":"3.4.3 几点讨论虽然在编程语言中, 分布式垃圾回收非常难, 但是针对 GFS 却很简单: 我们可以很容易地识别到 chunks 的引用: 它们就在 file-to-chunk mappings 中, 这些信息由 master 专门维护. 我们也可以很容易识别全部 chunk 副本: 它们就是 chunkserver 指定目录下的 linux 文件. 这些副本对 master 来说都不是垃圾. GC 相比 eager deletion 的好处: 1, 在组件容易故障的分布式系统中更简单可靠. 如果发删除消息则可能丢失, 需要重发送, 维护起来很复杂. GC 提供了统一可靠的清理不再有用的副本的方式. 2, 将 GC 纳入到周期性后台任务(其它还有命名空间扫描和心跳), 批量处理摊销了消耗. 而且这种后台任务仅当 master 相对空闲时候才做可以保证对客户端的响应. 3, GC 的惰性也避免了因误删除(意外且不可逆)导致的数据安全问题. 最大的缺点: 不能在存储紧张时候及时回收. 但 GFS 支持连续两次删除立即回收空间. 另外, GFS 支持为不同的命名空间指定副本策略(如副本因子)和删除策略. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#343-几点讨论"},{"categories":null,"content":"3.5 过期副本检测chunkserver 挂掉或者下线就会导致其上的 chunks 过期. master 维护着一个 chunk 版本号来识别最新和过期副本. master 授权一个新的 lease 时就会递增 chunk 版本号并通知其它副本, 并且 master 和这些副本会持久化这个版本号. 当某个副本挂掉重启后, 它会上报自己的 chunks 和版本号给 master, master就能检测到过期. 如果 master 发现有版本号比自己记录的还要大, 则认为自己授权 lease 时候出错并将该版本号作为最新版本号. master 在响应客户端哪个 chunkserver 持有它所请求的 chunk 时会在响应中包含版本号以让客户端进行校验; master 在指示某个 chunkserver 去另一个 chunkserver 拷贝数据时也会告诉它当前的版本号, 供其校验. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:5:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#35-过期副本检测"},{"categories":null,"content":"4 容错与检测设计这个系统最大的挑战时应对频繁的组件失效. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:0:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#4-容错与检测"},{"categories":null,"content":"4.1 高可用","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#41-高可用"},{"categories":null,"content":"4.1.1 保持高可用的两个手段 1, 快速恢复,不管之前如何下线的, master 和 chunkservers 可以几秒内即可重启并恢复数据. 2, 多副本, 默认三副本. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#411-保持高可用的两个手段"},{"categories":null,"content":"Master 复制master 的操作日志和 checkpoint 会被保存到多台机器. 一个更新操作仅当其被记录到 master 本地和远程机器上才算被提交. master 挂了, 监控系统会立即在其它机器上启动一个 master 并快速从日志恢复状态. clients 用的是域名访问 master, 所以可以快速感知这个变化. 另外 GFS 还提供了影子 masters, 它们只提供了读操作, 而且数据可能会稍微落后 primary master 一秒. 针对那些不怎么变动的文件或者应用不太在乎稍微过期的数据的时候, 这些影子 masters 可以为 primary master 分担一些读请求. 影子 masters 会拉取日志副本并应用保持自己更新, 而且它们也会在启动时查询 chunkservers 获取 chunk 副本位置信息(因为这些信息不会被持久化到日志只能自己去主动去查), 也和 chunkservers 保持心跳交换信息以监控它们的状态. 当然副本布局/删除等等变更操作还是由 primary master 负责, 影子 masters 只读. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#master-复制"},{"categories":null,"content":"4.2 数据完备性chunkserver 靠校验和来检测数据是否损坏, 而不是靠比对各个副本,那不太可行. 每个 chunk 被切分成多个 64KB 大小的 blocks. 每个 block 都有一个对应的 32 bit 校验和. 校验和也会被保存到内存中同时会和日志一起持久化. chunkserver 响应请求者数据之前会计算校验和(有点性能消耗)并和存储的校验和比对, 如果不一致则响应错误并上报给 master, 请求者会去其它副本读数据, 同时 master 会指示 chunkserver去从其他副本恢复数据, 然后删掉损坏的副本. 空闲时, chunkserver 会扫描和校验不活跃的 chunks, 检测损坏的 chunks 并上报. master 就会指示创建新的 chunks 并删除损坏的 chunks. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#42-数据完备性"},{"categories":null,"content":"4.3 诊断工具诊断问题只能靠日志, GFS 诊断日志记录了 chunkserver 上下线等重大事件和全部 RPC 请求响应. 可以通过日志来重建完整的交互历史来诊断问题. 日志的开销很小, 尤其和得到的好处比起来. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#43-诊断工具"},{"categories":null,"content":"5 测量接下来, 我们看看 GFS 架构和实现中的瓶颈, 以及真实集群中的若干数字. 注意, 这都是 2003 年的数据, 我们要观察的是 GFS 这么大系统的度量方法以及它如何在当年那种相比现在硬件条件那么差的情况下发挥其价值的. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:0:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#5-测量"},{"categories":null,"content":"5.1 测量用的微基准测试集群由 1 master, 2 master replicas, 16 chunkservers, 16 clients 构成. 这么搭就为了方便测试, 真实的集群有几百个 chunkservers 和几百个 clients 构成. 所有机器都是双核 1.4GHz, 2GB 内存, 两个每分钟 5400 转的 80GB 磁盘, 一个 100Mbps 的全双工以太网. master 和 chunkserver 共 19 台机器都连接到同一个 HP 2524 交换机上, 16 个 clients 连接到另一个交换机上, 两个交换机通过 1Gbps 链路连接. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#51-测量用的微基准"},{"categories":null,"content":"5.1.1 reads 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.a 显示 N 个 clients 的读取速率聚合以及它的理论上限. 当两个交换机之间的 1Gbps 跑满的时候达到理论上限 125MB/s; 当每个 client 的 100Mbps 跑满时, 单个 client 机器达到理论上限 12.5MB/s. 当仅有一个 client 读取时, 观察到的读取速率为 10MB/s, 大约是每个 client 上限的 80%(即 10/12.5). 当 16 个 clients 一起读取时, 观察到的聚合读取速率为 94MB/s, 大约为理论上限的 75%(即 94/125); 或者 每个 client 大约 6MB/s. 从 80% 降到 75% 的原因是: 当读取客户端变多, 多个客户端同时从同一个 chunkserver 读取的概率也变大, 从而导致 chunkserver 的网卡带宽被多个客户端争用的更厉害了. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#511-reads-测试"},{"categories":null,"content":"5.1.2 writes 测试N 个 clients 同时写多个 N 个不同的文件. 每个 client 写 1GB 数据到一个新文件, 每次写入 1MB. 聚合写入速率和理论上限如 3.b 所示. 理论上限为 67MB/s, 因为我们需要将每个字节写到 16 个 chunkservers 中的 3 个里, 每个 chunkserver 入口带宽上限为 12.5MB/s. 如果单个 client 写入, 则观察到的写入速率为 6.3MB/s, 大约是上限的一半. 罪魁祸首是网络协议栈, 因为它与我们把数据 push 给 chunk 副本的管道化方案不太搭配. 数据从一个副本传播给另一个副本延迟降低了整体的写入速率. 如果是 16 个 clients 一起写入, 观察到的聚合写入速率为 35MB/s(每个 client 平均 2.2MB/s), 大约是理论上限的一半(即 35/67). 就像读取测试中, 随着执行写操作的 clients 增多, 则同一个 chunkserver 被多个 clients 并发写的几率增大, 此 chunkserver 的入口带宽被更多 clients 争用, 而且 16 个 clients 的写要比读竞争更激烈, 因为一个字节要写到三个 chunkserver. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#512-writes-测试"},{"categories":null,"content":"5.1.3 record appends 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.c 显示了记录追加操作的性能. N 个 clients 同时向同一个文件追加. 性能被存储最后一个 chunk 的 chunkservers 的网络带宽限制住了, 与 clients 个数无关. 当只有一个 client 写入时, 速率为 6.0MB/s, 当 16 个 clients 一起写入时, 速率降到了 4.8MB/s, 主要原因是网络拥塞和抖动. 实际使用中, 我们的应用程序倾向于并发生成若干前述文件而不是一个. 换句话说, N 个 clients 同时向 M 个共享文件追加, N 和 M 都是几千级别的. 因此前面提到的网络拥塞在实际中不是啥大问题, 因为 client 写入一个 chunkserver 时, 当一个文件忙的时候, 可以写另一个文件. –End– ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#513-record-appends-测试"},{"categories":null,"content":"本文基于内部分享 \u003c“抄\"能力养成系列 – Gorilla 的设计和实现\u003e 整理. Gorilla 是 Facebook 于 2015 年开放的一个快速, 可扩展的, 内存式时序数据库. 它的一些设计理念影响了后来的 Prometheus. 本文就其设计和实现进行深入分析希望能为各位后续在系统研发中提供灵感. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#"},{"categories":null,"content":"1 Gorilla 诞生背景 针对一个大型系统, 监控和分析上千万个打点数据(measurements)是一件很有挑战的事情. 解决这个事情, 一个有效手段就是将这些打点数据存储到时序数据库(TSDB) 中. 而设计一个 TSDB 的关键挑战是如何让效率(efficiency)/扩展性(scalability)/可靠性(reliability)三者达成一个有效平衡. Facebook 的 Gorilla 就是一个达成这种平衡的 TSDB. Gorilla 的设计基于这样一个洞察: 监控系统的用户不会太关注一个个单独的数据点, 而是关注聚合分析; 对于分析一个现有的问题, 最近的数据点比老的数据更有价值. Gorilla 为了读写的高可用而优化, 即使以丢失部分写操作为代价. 为了提升查询效率, fb 激进地采用压缩技术, 如 delta-of-delta 和浮点数 XOR, 来压缩存储大小, 以实现将 Gorilla 数据保存到内存中. 最终效果是将查询时延减少 73X, 同时提升查询吞吐 14X, 两项均为和传统的基于 HBase 的时序数据库比较. 这个结果解锁了很多监控和调试功能, 比如可以在 Gorilla 上执行时序关联搜索. Gorilla 也能很优雅地处理从单点到整个集群故障. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#1-gorilla-诞生背景"},{"categories":null,"content":"2 Gorilla 简介Figure 1 是 Facebook 内部的监控系统, 名叫 ODS(Operational Data Store, fb 内部广泛使用的一个老的监控系统), 其中 Gorilla 作为一个 write-through(即 cache 和 back store 同时修改保证多用户一致性) cache. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#2-gorilla-简介"},{"categories":null,"content":"2.1 需要满足的几个限制 写密集. 每秒可以写入上千万数据点, 一个查询可以毫秒级响应. 读比写少几个数量级, 主要是一些自动化监控系统或者用户查询. 快速识别系统的重大状态变迁. 出现任何问题时都会导致系统状态发生变化, Gorilla 支持针对很小的窗口(几十秒)进行聚合, 以快速识别重大状态变化并触发自动化修复. 高可用. 即使多个数据中心通信断开, 每个 DC(位于不同 region 的 DataCenter, 下同) 都能本地实例读写. 容错. 写入数据被复制到多个 DC, 确保某个 DC 挂掉数据仍在. 以上限制 Gorilla 均满足, 而且可以做到绝大多数查询在几十个毫秒内返回. 另外, 统计发现, 针对 ODS 的至少 85% 的查询涉及过去 26 个小时的数据. 这就暗示我们如果将之前基于磁盘的存储改为内存式将会更好地服务用户. 再进一步, 我们将这个内存式数据库作为持久性磁盘存储系统的 cache, 我们就既可以获得高速插入速率, 同时还能获得数据持久性. 2015 年春天, fb 的监控系统就生成了超过二十亿个 counter 类型的时间序列, 每秒产生大约 1200 万个数据点, 每天产生 1 万亿个数据点. 每个数据点 16 字节, 那就占用 16TB 内存, 这太多了. 但是通过采用基于 XOR 的浮点数压缩技术, 平均每个数据点 1.37 字节, 大小减少 12x. 为了满足可靠性, 我们在不同 region 的 DC 都部署了 Gorilla 实例, 每个数据点都会写入每个 DC 的实例, 但是这多个副本并不保证一致性. 查询请求会被路由到最近的 DC. 以上基于一个观察, 即独立的数据点丢失不会对数据聚合结果产生大的影响, 除非不同 Gorilla 实例数据差异很大. 目前 Gorilla 在 fb 用于生产环境, 而且和其它系统如 hive/scuba 一起检测和分析问题. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:1:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#21-需要满足的几个限制"},{"categories":null,"content":"2.2 现状(注: 2015)fb 内部有几百个系统, 分布在多个 DC, 这些系统的健康状况和性能是需要监测的, ODS 就是 fb 监控系统的重要组成部分. ODS 由 TSDB, 查询服务以及检测和告警服务构成; 其中 TSDB 是构建在 HBase 上的. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#22-现状注-2015"},{"categories":null,"content":"2.3 已有监控系统的查询性能问题早在 2013 年, fb 的监控团队就意识到基于 hbase 的 TSDB 无法扩展应对将来的查询负载. 其中 90 分位数长达几秒, 这对于依赖 tsdb 的自动化监控系统非常不友好. 一个针对稀疏数据的大点的查询甚至会超时, 因为 hbase 是针对写操作优化的. 虽然 hbase 表现不行, 但是也不能整个替换掉, 因为 ODS 的 hbase 存了 2PB 的数据. 而且 fb 的数据仓库, hive 也不能胜任, 因为它的查询比 ODS 还要慢几个数量级, 而查询时延和效率是我们主要关注的. 接下来能做的就是内存式 cache 了. (ODS 其实本来就有 write-through cache, 只不过是用于制图系统.) 开始也考虑过基于 Memcached 来做, 但是因为针对已有时间序列追加新数据需要一个读写周期, 会导致 memcached 服务器产生极其高的流量, 所以否掉了这个方案. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:3:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#23-已有监控系统的查询性能问题"},{"categories":null,"content":"3 Gorilla 的设计目标 通过唯一 key 可以识别 20 亿时间序列. 每分钟可追加 7 亿数据点(时间戳+具体值). 可保存 26 个小时的数据. 可支持每秒 4000 查询峰值. 一个毫秒内完成读操作. 支持 15 秒粒度的时间序列(即一分钟四个数据点). 两个内存式副本(不能部署在同一个地方, 以应对故障). 即使挂掉一个实例, 仍然可以正常支持查询. 可以快速扫描整个内存数据的能力. 支持每年至少 2x 的负载增长. Gorilla 聚焦如何实时收集和存储海量数据. Gorilla 可以作为其它 tsdb 的 write-through cache. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#3-gorilla-的设计目标"},{"categories":null,"content":"4 Gorilla 与现有的 TSDB 比较(注:2015年)","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#4-gorilla-与现有的-tsdb-比较注2015年"},{"categories":null,"content":"4.1 OpenTSDB 它基于 HBase, 无降采样功能. 数据模型丰富, 针对一个时序有一组 key-value 对即 tags, Gorilla 仅有一个字符串 key 而且依赖更高级的工具抽取和识别时序的元数据. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:1:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#41-opentsdb"},{"categories":null,"content":"4.2 Whiper(Graphite) 数据存储在本地磁盘, 所以查询速度不够快. 数据格式为 whisper, 即 RRD 风格. 该文件格式要求时间序列数据都带固定间隔的时间戳. 如果时间戳间隔固定, Gorilla 表现更好(压缩率更高), 但是 Gorilla 也可以处理随机变化的时间间隔. 每个时间序列保存到一个单独的文件中, 一段时间后新采集的数据会覆盖老的数据, 毕竟是 Round-Robin Database. Gorilla 也采用类似的方式, 仅在内存保存最近的数据. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#42-whipergraphite"},{"categories":null,"content":"4.3 InfluxDB 比 OpenTSDB 数据类型更加丰富, 每个数据点都有一组丰富的标签, 这也导致它更占磁盘. 无需 hbase/hadoop 就能做到水平扩展. 数据保存到本地磁盘, 所以查询不如内存式的快. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:3:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#43-influxdb"},{"categories":null,"content":"5 Gorilla 架构","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#5-gorilla-架构"},{"categories":null,"content":"5.1 概况 内存式, 作为后端 hbase 的 write-through cache. 每个数据点是一个三元组\u003c字符串形式的 key, 64 比特的时间戳, 双精度浮点数类型的测量值\u003e. 所采用的压缩算法, 可以将 16 字节的数据点压缩到平均 1.37 字节, 减少 12x. 内存数据结构既支持快速的全量数据扫描, 也支持高效地查询单个时序. key 作为时序唯一标识, 写入时也是基于此在众多 Gorilla 实例之间做 sharding. 所以仅仅通过增加新机器就能实现 Gorilla 集群的水平扩展, 新写入的数据会被 sharding 到新机器上. 之所以这么简单, 就是因为 Gorilla 整体是一个 share-nothing 架构, 专注于水平扩展能力.(疑问, 一致性哈希咋做的? 论文没讲, 但提到了 ShardManager, 应该是它负责的). Gorilla 可以处理单点故障, 网络分区甚至整个 DC 挂掉, 方法就是每个时间序列都被写入到两个不同地理 regions 中的实例中. 一旦检测到宕机, 针对目标序列的全部查询会被自动切换到另一个 region 的实例, 这个过程用户感觉不出来. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:1:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#51-概况"},{"categories":null,"content":"5.2 时间序列压缩Gorilla 对压缩算法的诉求: 支持针对浮点数进行压缩而非整数 支持在数据流上进行压缩, 而不是针对静态完整的数据集 无损, 保持整个时序精度不变 Gorilla 的压缩算法受科学计算中的浮点数压缩方案启发, 该方案利用当前值与前一个值的 XOR 比较来生成 delta 编码. Gorilla 不支持跨时间序列进行压缩, 而是在每个时序内进行压缩. 时间戳和具体数值各自独立进行压缩, 压缩时都用到了前一个数据点的信息. Figure 2 是 Gorilla 的压缩过程图示. 2.a 显示了一个数据流, 每个点由两个 64 比特的数构成, 一个是时间戳一个是具体测量值. Gorilla 基于时间将数据流分成 block, 划分时会按照每两小时进行对齐. 可以看到每一个 block 以一个简单的 header 开始, 它含有所在 block 对齐的时间戳, 例子中是凌晨两点. 2.b 是基于 delta-of-delta 压缩后的数据, 可以看到 delta-of-delta 等于 60-62=-2, 用 ‘10’ 两个比特表示(后面详述表示规则), 接下来 7 个比特存储测量值, 一共用了 9 比特. 2.c 显示的是使用 XOR 算法压缩的浮点数, 可以看到当前值与前一个值 XOR 后的结果, 它有 11 个前导零, 整个结果仅有一个有效位 1, 这可以编码为 2 比特的 ‘11’. 存储当前浮点数测量值 24 共用了 14 比特. 具体编码细节后面详述. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#52-时间序列压缩"},{"categories":null,"content":"5.2.1 时间戳压缩 我们注意到, 在 ODS 中, 绝大多数数据点都是以固定时间间隔到达的. 后面可以看到这是一个非常重要的洞察. 我们不会将时间戳完整存储, 而是采用 delta-of-delta. 假设一个 delta 序列为 60, 60, 59, 61, 那么 delta-of-delta 就是 0, -1, 2. 然后我们按照下面的算法使用变长编码来编码这些 delta-of-delta: block header 存储起始时间戳, 记为 $t_{-1}$, 它以两小时时间窗口来对齐. 当前 block 中第一个数据点的时间戳记为 $t_0$, 真正存储的是它减去 $t_{-1}$ 得到的 delta(只有第一个时间戳对应的存储值为 delta 而非 delta-of-delta, 因为前面就一个起始时间戳), 以 14 比特存储. 接下来针对每个时间戳 $t_n$: 计算其对应的 delta-of-delta: $D=(t_n-t_{n-1})-(t_{n-1}-t_{n-2})$ 如果 $D$ 为 0, 则用一个比特存储 ‘0’ 如果 $D$ 在$[-63, 64]$ 之间, 用 2 个比特存储 ‘10’ , 然后接下来 7 比特存储 $D$ 的具体值. 如果 $D$ 在$[-255, 256]$ 之间, 用 3 个比特存储 ‘110’, 然后接下来 9 比特存储 $D$ 的具体值. 如果 $D$ 在$[-2047, 2048]$ 之间, 用 4 个比特存储 ‘1110’, 然后接下来 12 比特存储 $D$ 的具体值. 其它情况, 用 4 个比特存储 ‘1111’, 然后接下来 32 比特存储 $D$ 的具体值. 上面的取值范围都是通过从生产系统统计出来的, 这几个范围可以帮助达到最大压缩率. 从 Figure 3 观察压缩效果: 可以发现大约 96% 的时间戳可以压缩到 1 个比特. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#521-时间戳压缩"},{"categories":null,"content":"5.2.2 测量值压缩 Gorilla 只允许存储双精度浮点数的测量值. 根据统计, 时间序列中相邻的数据点大多数时候相差不大, 如果相邻的数据点对应的测量值的符号部分/指数部分/小数部分前半截都差不多一样, 那么我们就可以计算当前值和前一个值的 XOR 来存储而不是采用 delta 编码方案. 当前值和前一个值进行 XOR 后按照下面变长编码方案来存储: 第一个值不压缩 如果 XOR 结果为 0, 则仅存 1 比特的 ‘0’ 如果 XOR 结果非 0, 计算其前导零和后缀零个数, 存储 1 比特 ‘1’, 然后后面跟着下面的 a 或者 b: a. 首先是 1 比特的控制位 ‘0’, 如果当前 XOR 结果的前导零个数和后缀零个数与前一个 XOR 结果的对应部分一样, 仅存储当前 XOR 结果的中间有效位部分(该部分开头和结尾都为 1). 正是因为前述的特性, 可以让我们省去存储前导零长度和后缀零长度的空间, 根据前一个 XOR 结果就能复原当前这个. b. 首先是 1 比特的控制位 ‘1’, 接下来 5 比特为前导零个数, 接下来 6 比特为当前 XOR 结果的有效位长度(该部分开头和结尾都为 1), 接下来为 XOR 结果的有效位部分. 从上面方案来看, 测量值压缩不但用到了当前测量值和前一个测量值, 也用到了当前 XOR 结果和前一个 XOR 的结果, 因为计算出来的 XOR 序列, 相邻两个经常有相似的前导零和后缀零. 这可以通过 Figure 4 来观察. 针对测量值的编码方案, 从 Figure 5 统计可以看到: 大约 59.06% 数据点被压缩到仅剩 1 比特 大约 28.3% 以控制位 ‘10’ 开头, 平均长度为 26.6 比特 大约 12.64% 以控制位 ‘11’ 开头, 平均长度为 39.6 比特, 相比控制位 ‘10’ 方案多出来的 13 比特用于存储前导零长度和有效位部分长度了. 测量值方案潜在的问题: 就是时序对应的数据流按多大的窗口划分 block 才能更好的利用这个压缩方案. 由于中间每个值压缩都与自己的前驱密切相关, block 肯定越大越好, 如果不划分 block 是最佳的. 但是如果 block 太大, 但是用户只想查询一个很小的时间窗口的数据, 就会涉及非常大的计算量来解压缩. 这就牵扯到 trade-off. 从 Figure 6 可以看到 120 分钟也就是两个小时的 block 大小可以达到一个比较理想的压缩率, 平均每个数据点占 1.37 字节. 窗口再大, 压缩率变化不大了. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#522-测量值压缩"},{"categories":null,"content":"6 Gorilla 的内存数据结构Figure 7 是针对 Gorilla 的内存数据结构相互关系和各个部分的图示. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#6-gorilla-的内存数据结构"},{"categories":null,"content":"6.1 TSmapGorilla 实现涉及的核心数据结构是 Timeseries Map(TSmap), 如 Figure 7 中间部分所示. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:1:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#61-tsmap"},{"categories":null,"content":"6.1.1 TSmap 由两部分构成 一个 C++ 标准库的 vector, 每个元素是一个 shared-pointer, 每个 shared-pointer 指向一个时间序列 TS. 该结构主要用于针对全量数据进行高效地分页扫描. 一个从时间序列名称(大小写不敏感但是保留大小写)到指向时间序列 TS 的 shared-pointer 的无序 map. 该结构主要是用于在常数时间内根据名称查询某个时间序列. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:1:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#611-tsmap-由两部分构成"},{"categories":null,"content":"6.1.2 TSmap 的并发与性能关于 TSmap 的并发控制: 有一个专用的 Read-Write 锁保护 map 和 vector 的访问 每个 TS 有一个 1 字节的 spin lock, 由于每个时间序列写吞吐低(秒级或分钟级写一次), 所以针对 spin lock 的争用并不激烈. C++ shared-pointer 使得扫描操作可以在微秒时间内拷贝 vector, 这避免了长时间锁住临界区(critical section)从而影响写入数据流. 当删除一个时间序列时, 其对应的在 vector 的元素会被标记为墓碑(相关内存不会返回系统而是等待重用), 对应的索引被放到池子里待新的时间序列重用之. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:1:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#612-tsmap-的并发与性能"},{"categories":null,"content":"6.2 ShardMapGorilla 为了做到分布式存储采用了分片机制, 如 Figure 7 左边部分所示. ShardMap 名字里有 map, 但它实际是一个 vector, 每个元素是一个指向 TSmap 的 unique_ptr. 那它为什么叫 map 呢? 因为每个时间序列根据其名称先做 hash(具体算法同 TSmap 的 map 结构)后做 shard(即其被分到哪个 TSmap). shards 即 TSmap 个数固定, 只有几千个. 就像 TSmap, 针对 ShardMap 的访问也由一个 read-write spin lock 控制. 因为 ShardMap 这一层做了 shard 了, 所以 TSmap 内部的 map 就比较小了, C++ 标准库里的 unordered-map 性能足够了. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#62-shardmap"},{"categories":null,"content":"6.3 TS说完了最外层的 ShardMap, 也说完了中间的 TSmap, 下面说说最里面的 TS 构成. TS (即 time series), 为 Gorilla 存储每个时间序列的数据结构, 如 Figure 7 右边部分所示. 每个时间序列的数据结构由一系列 closed blocks(每一个两个小时大小)和一个 open block (保存最近两个小时的数据). open block 仅追加压缩过的(压缩算法前面讲过了) \u003c时间戳, 测量值\u003e 数据点. 超过 2 小时, open block 就会转成 closed block, 后者不可更改直至从内存被删除. 一旦 closed, block 就会被拷贝到另外的内存(这些内存从大块的 slabs 分配)从而减少内存碎片. open block 经常因大小变化而重分配内存, 前述的拷贝过程可以减少 Gorilla 整体的内存碎片. 当外部查询时, Gorilla 直接拷贝包含目标数据的 blocks 给客户端, 客户端自己去解压缩. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:3:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#63-ts"},{"categories":null,"content":"7 Gorilla 的磁盘存储结构Gorilla 其中一个设计目标就是高可用, 不能因为单个 host 挂掉而导致集群不可用. Gorilla 通过 GlusterFS 实现数据持久性, 该分布式文件系统支持三副本, 并且兼容 POSIX. 当然你也可以用 Hadoop 或者其它分布式文件系统. 注意这里是单个 host 上数据的持久性, 它们对 host 内存数据对应. 这些数据不用于响应客户查询, 而是用于 host 内存数据持久化以便在 host 挂掉后被重新加载到内存恢复对应数据结构. Gorilla 每个 host 持有多个 shards, 每个 shard 一个目录. 每个目录中保存着如下四类信息: key list 文件. key list 文件就是一个简单的 map, 即从时间序列的字符串类型的 key 到一个整数 ID 的映射. 这个整数 ID 就是对应时间序列在 Tsmap-\u003evector 的索引. 新的 keys 会被追加到 key list 中, Gorilla 会周期性地扫描每个 shard 对应的全部 keys 以重写该文件. 一组 append-only logs. 每个 shard 一个 append-only log. 类似 WAL(write-ahead log), 但因为它不保障 ACID 所以不是真正的 WAL. 新到达的数据点都是先写到这个文件. 由于每个 shard 一个 append-only log, 所以哈希到同一个 shard 的多个时间序列的数据点会交织在一起. 数据编码同内存格式, 但多了一个 32 比特的整数 ID 标识数据点属于哪个时间序列. 每当攒够 64KB 数据刷盘一次, 大约是一两秒的数据, 这可能因为宕机丢失一小部分数据, 不过为了获取更高的磁盘写入效率, 这个 trade-off 就不得不做了. 一组完整的 block 文件. 每隔两小时 Gorilla 会拷贝内存中压缩过的 block 数据到磁盘, 它比 append-only log 文件小多了. 一个 block 文件就是两个小时的数据, 它包含两段: 一组连续的 64kB slabs(就是它们在内存中的样子), 一个 \u003ctime series ID, data block pointer\u003e 列表. checkpoint 文件. 每当一个完整的 block 文件生成, Gorilla 就会创建一个新的 checkpoint 文件同时删除对应的 append-only log. 这个 checkpoint 文件用于标记什么时候一个完整的 block 文件被刷入了磁盘. 如果因为进程崩溃导致 block 文件写入失败, 新进程启动后会发现对应的 checkpoint 文件不存在, 新进程就会认为这个 block 文件有问题, 转而仅从 append-only log 读取数据. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#7-gorilla-的磁盘存储结构"},{"categories":null,"content":"8 Gorilla 如何实现高可用Gorilla 容忍两种故障: 单机故障 单集群故障 以上两种都可以通过多副本机制做到快速转移和处理, 也对频繁的版本升级很友好. 多副本机制: 针对同一份数据, Gorilla 会在不同 region 的 DC 里面部署相互独立的集群, 每次写操作同时写到这两个地方, 当然这个双写不保证一致性(还是回到前面讲到的, 时序数据不太注重单个数据点或者一小块数据, 关注的是整体), 一个挂了另一个还能用, 挂了的那个数据恢复到最近 26 个小时大小就继续对外提供服务. 单机挂掉后 Gorilla 处理流程: ShardManager(一个基于 paxos 强一致系统)会将其负责的 shards 重分配给当前集群其它机器. 重分配期间, 写客户端会自己缓冲这些 shards 的数据一分钟. 一般 node 挂掉 30 秒后就启动重分配, 一分钟就可以完成, 如果超过一分钟, 缓冲的数据就会用新的覆盖老的. shards 重分配时, 分到这些 shards 的 nodes 会去 GlusterFS 拉取数据, 一般五分钟内就可以全部恢复. 注意, 前述过程可能会丢失部分数据, 这是可以容忍的. 注意, node 失效期间, 因为部分 shards 不可用, 读响应可能会被标记为 partial, 客户端转而请求另一个 DC 的同样数据, 如果两个 DC 相关均不可用, 则两个 DC 的部分结果和一个表示错误的标识会被返回给客户端. 最后, 如果 Gorilla 都挂了怎么办? 这时就靠 HBase 的 long-term 存储来响应客户请求了, 直到 Gorilla 集群恢复. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#8-gorilla-如何实现高可用"},{"categories":null,"content":"9 Gorilla 带来的新分析工具","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#9-gorilla-带来的新分析工具"},{"categories":null,"content":"9.1 关联分析引擎 Gorilla 提供的关联搜索功能支持用户一次在一百万个时间序列上做交互式地暴力搜索. 关联分析引擎支持针对某个序列和一组其它时间序列做 PPMCC(皮尔逊积矩相关系数), 从而找出形状相似地时间序列, 进而帮助做根因分析(root-cause analysis). 计算 PPMCC 时, 测试时间序列会被广播到多个机器, 这些机器基于目标时序的 key 来确定, 每个机器独立计算相关时序, 然后基于 PPMCC 绝对值排序求出 topN. 最后返回结果. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:1:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#91-关联分析引擎"},{"categories":null,"content":"9.2 降采样在 Gorilla 上线之前, fb 在 hbase 上允许 map-reduce 任务来计算降采样数据. 有了 Gorilla, 后台进程每两个小时扫一遍全量数据来计算降采样, 不用再去全表扫描 HBase 了.\u000b–End– ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#92-降采样"},{"categories":null,"content":"memtable 可以看作是 log 文件的内存形式, 但是格式不同. 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable(所以可能同时有两个 memtable 存在) 以及磁盘上的各个 level 包含的文件构成了数据全集. memtable 的本质就是一个 SkipList. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#"},{"categories":null,"content":"1 核心文件和核心类 核心类 所在文件 用途 leveldb::MemTable db/memtable.cc; db/memtable.h 本文主角 leveldb::Arena util/arena.cc; util/arena.h 负责内存管理(仅分配和释放, 无回收) leveldb::SkipList\u003cKey, Comparator\u003e db/skiplist.h 作为 MemTable 类底层存储(基于内存) ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#1-核心文件和核心类"},{"categories":null,"content":"2 MemTable 核心成员变量 字段 类型 用途 comparator_ struct leveldb::MemTable::KeyComparator 用于 SkipList 比较 key refs_ int 该 memtable 引用计数, memtable 的拷贝构造和赋值构造都是禁用的, 只能通过增加引用计数复用 arena_ class leveldb::Arena 内存池, 给 SkipList 分配 Node 时候使用 table_ typedef leveldb::SkipList\u003cconst char *, leveldb::MemTable::KeyComparator\u003e leveldb::MemTable::Table SkipList, 存储 memtable 里的数据 ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#2-memtable-核心成员变量"},{"categories":null,"content":"3 MemTable 核心方法 方法 用途 explicit MemTable(const InternalKeyComparator\u0026 comparator) 传入一个自定义或数据库提供的比较器, 生成一个 MemTable 实例. void Ref() 递增引用计数, MemTable 是基于引用计数的, 每个 MemTable 实例初始引用计数为 0, 调用者必须至少调用一次 Ref() 方法. void Unref() 递减引用计数, 计数变为 0 后删除该 MemTable 实例. size_t ApproximateMemoryUsage() 返回当前 memtable 中数据字节数的估计值, 当 memtable 被修改时调用该方法也是安全的, 该方法底层实现直接用的 arena_ 持有内存的字节数. Iterator* NewIterator() 返回一个迭代器, 该迭代器可以用来遍历整个 memtable 的内容. void Add(SequenceNumber seq, ValueType type, const Slice\u0026 key, const Slice\u0026 value) 根据指定的序列号和操作类型将 user_key 转换为 internal_key 然后和 value 一起向 memtable 新增一个数据项. 该数据项是 key 到 value 的映射, 如果操作类型 type==kTypeDeletion 则 value 为空. 最后数据项写入了底层的 SkipList 中. 每个数据项编码格式为 [varint32 类型的 internal_key_size, internal_key, varint32 类型的 value_size, value] internal_key 由 [user_key, tag] 构成. bool Get(const LookupKey\u0026 key, std::string* value, Status* s) 如果 memtable 包含 key 对应的 value, 则将 value 保存在 *value 并返回 true. 如果 memtable 包含 key 对应的 deletion, 则将 NotFound 错误存在 *status, 并返回 true; 其它情况返回 false. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#3-memtable-核心方法"},{"categories":null,"content":"3.1 MemTable 写方法当需要向 memtable 加入一个 \u003ckey, value\u003e 时, 可以调用 void Add(SequenceNumber seq, ValueType type, const Slice\u0026 key, const Slice\u0026 value) 达成目标. 该方法负责将用户 key 转换为一个 InternalKey, 然后连同 value 一起进行编码, 组成一个形如下述的数据项: +------------------------------+---------+--------+--------+---------------+-----+ | internal key size | seq num |op type |user key| value size |value| | (varint32 type) |(7 bytes)|(1 byte)| |(varint32 type)| | |(user key + seq num + op type)| | | | | | +------------------------------+---------+--------+--------+---------------+-----+ 其中该数据项前四个部分构成了 InternalKey, 该结构是 MemTable 做排序的依据. 最后将该数据项插入到底层的 SkipList 中. 具体处理流程见下面代码注释: void MemTable::Add(SequenceNumber s, ValueType type, const Slice\u0026 key, const Slice\u0026 value) { // 插入到 memtable 的数据项的数据格式为 // (注意, memtable key 的构成与 LookupKey 一样, 即 user_key + 序列号 + 操作类型): // [varint32 类型的 internal_key_size, // user_key, // 序列号 + 操作类型, // varint32 类型的 value_size, // value] // 其中, internal_key_size = user_key size + 8 size_t key_size = key.size(); size_t val_size = value.size(); size_t internal_key_size = key_size + 8; // 编码后的数据项总长度 const size_t encoded_len = VarintLength(internal_key_size) + internal_key_size + VarintLength(val_size) + val_size; // 分配用来存储数据项的内存 char* buf = arena_.Allocate(encoded_len); // 将编码为 varint32 格式的 internal_key_size 写入内存 char* p = EncodeVarint32(buf, internal_key_size); // 将 user_key 写入内存 memcpy(p, key.data(), key_size); p += key_size; // 将序列号和操作类型写入内存. // 注意, 序列号为高 7 个字节; // 将序列号左移 8 位, 空出最低 1 个字节写入操作类型 type. EncodeFixed64(p, (s \u003c\u003c 8) | type); p += 8; // 将编码为 varint32 格式的 value_size 写入内存 p = EncodeVarint32(p, val_size); // 将 value 写入内存 memcpy(p, value.data(), val_size); assert(p + val_size == buf + encoded_len); // 将数据项插入跳跃表 table_.Insert(buf); } 可以看到由于模块化抽象做得好, 整体来说还是比较简单的. 后面章节会重点说一下上述处理中有关内存分配(arena_.Allocate())和数据项插入到 SkipList (table_.Insert()) 的相关操作. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#31-memtable-写方法"},{"categories":null,"content":"3.2 MemTable 读方法bool Get(const LookupKey\u0026 key, std::string* value, Status* s) 用于实现针对 memtable 的读操作. 当用户调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 1 先查询当前在用的 memtable, 查到返回, 未查到下一步 2 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 3 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. 其中上面 1,2 两步骤查询 memtable 使用的即为本节开头提到的 Get 方法. // 如果 memtable 包含 key 对应的 value, 则将 value 保存在 *value 并返回 true. // 如果 memtable 包含 key 对应的 deletion(memtable 的删除不是真的删除, 而是一个带有删除标记的 Add 操作, // 而且 key 对应的 value 为空), 则将 NotFound 错误存在 *status, 并返回 true; // 其它情况返回 false. // LookupKey 就是 memtable 数据项的前半部分, 布局同 internal key. bool MemTable::Get(const LookupKey\u0026 key, std::string* value, Status* s) { // LookupKey 结构同 internal key. // 注意由于比较时, 当 userkey 一样时会继续比较序列号, // 而且序列号越大对应 key 越小, 所以外部调用 Get() // 方法前用户组装 LookupKey 的时候, 传入的序列号不能小于 // MemTable 每个 key 的序列号. 外部具体实现是采用 DB 记录的最大 // 序列号. Slice memkey = key.memtable_key(); // 为底层的 skiplist 创建一个临时的迭代器 Table::Iterator iter(\u0026table_); // 返回第一个大于等于 memkey 的数据项(数据项在 iter 内部存着), // 这里的第一个很关键, 当 userkey 相同但是序列号不同时, 序列号 // 大的那个 key 对应的数据更新, 同时由于 internal key 比较规则 // 是, userkey 相同序列号越大对应 key 越小, 所以 userkey 相同时 // 序列号最大的那个 key 肯定是第一个. iter.Seek(memkey.data()); // iter 指向有效 node, 即 node 不为 nullptr if (iter.Valid()) { // 每个数据项格式如下: // klength varint32 // userkey char[klength-8] // 源码注释这里有误, 应该是 klength - 8 // tag uint64 // vlength varint32 // value char[vlength] // 通过比较 user_key 部分和 ValueType 部分来确认是否是我们要找的数据项. // 不去比较序列号的原因是上面调用 Seek() 的时候已经跳过了非常大的序列号 // (internal_key 比较逻辑是序列号越大 internal_key 越小, 而我们 // 通过 Seek() 寻找的是第一个大于等于某个 internal_key 的节点). // // 注意, memtable 将底层的 SkipList 的 key(确切应该说是数据项) // 声明为了 char* 类型. 这里的 entry 是 SkipList.Node 里包含的整个数据项. const char* entry = iter.key(); uint32_t key_length; // 解析 internal_key 长度 const char* key_ptr = GetVarint32Ptr(entry, entry+5, \u0026key_length); // 比较 user_key. // 因为 internal_key 包含了 tag 所以任意两个 internal_key // 肯定是不一样的, 而我们真正在意的是 user_key, // 所以这里调用 user_comparator 比较 user_key. if (comparator_.comparator.user_comparator()-\u003eCompare( Slice(key_ptr, key_length - 8), key.user_key()) == 0) { // 解析 tag, 包含 7 字节序列号和 1 字节操作类型(新增/删除) const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8); // 解析 tag 中的 ValueType. // leveldb 删除某个 user_key 的时候不是通过一个插入墓碑消息实现的吗? // 那怎么确保在 SkipList.Seek() 时候返回删除操作对应的数据项, // 而不是之前同样 user_key 对应的真正的插入操作对应的数据项呢? // 机巧就在于 internal_key 的比较原理 user_key 相等的时候, // tag 越大 internal_key 越小, // 这样后执行的删除操作的 tag(序列号递增了, 即使不递增, 但由于 // 删除对应的 ValueType 大于插入对应的 ValueType 也可以确保 // 后执行的删除操作的 tag 大于先执行的插入操作的 tag) // 这里有个比较技巧的地方. switch (static_cast\u003cValueType\u003e(tag \u0026 0xff)) { // 找到了 case kTypeValue: { Slice v = GetLengthPrefixedSlice(key_ptr + key_length); value-\u003eassign(v.data(), v.size()); return true; } // 找到了, 但是已经被删除了 case kTypeDeletion: *s = Status::NotFound(Slice()); return true; } } } return false; } 以上代码重要地方有如下几点: 参数 LookupKey 结构如 internal key, 前文介绍过具体结构不再赘述. 要强调的是, 构造其实例时, user key 部分采用用户传入的, 但是序列号要用数据库目前最大值, 这样可以确保作为比较环节的序列号不会影响我们的查找过程. 在底层 SkipList 查找之前, 构造了一个临时的迭代器, 迭代器结构体待后文介绍 SkipList 时详细介绍. 迭代器的 Seek 方法返回的是第一个大于等于目标 key 的节点, 这个第一个非常非常重要, 这也是为什么我们前面强调 LookupKey 的序列号必须足够达到不污染查找过程. Leveldb 的删除会插入一个墓碑消息, 其标识记录到 key 的 tag 部分, 所以查到数据的时候先不要高兴太早, 需要确认下 tag 的操作类型字段值. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#32-memtable-读方法"},{"categories":null,"content":"3.3 MemTable 迭代器我们知道, MemTable 和磁盘上的 sstables 文件一起构成了数据全集, 而且 leveldb 支持迭代整个数据库. 这就要求 MemTable 也是可迭代的, 这样才能和磁盘文件迭代器串联在一起构成一个超级迭代器供用户迭代. 从 MemTable 实例返回一个迭代器的方法为: Iterator* MemTable::NewIterator() { return new MemTableIterator(\u0026table_); } MemTable 的迭代器实现很简单, 它提供了一个 class leveldb::MemTableIterator, 但整个类仅仅是一个 wrapper. 因为 MemTable 依赖的 SkipList 提供了一个完整的迭代器, MemTableIterator 仅仅在内部封装了一个 leveldb::SkipList\u003cKey, Comparator\u003e::Iterator 实例即实现了对 MemTable 的迭代, 所以它的实现就很简单了: // 该类全部方法都是对 leveldb::SkipList\u003cKey, Comparator\u003e::Iterator 方法的二次封装 class MemTableIterator: public Iterator { public: explicit MemTableIterator(MemTable::Table* table) : iter_(table) { } virtual bool Valid() const { return iter_.Valid(); } virtual void Seek(const Slice\u0026 k) { iter_.Seek(EncodeKey(\u0026tmp_, k)); } virtual void SeekToFirst() { iter_.SeekToFirst(); } virtual void SeekToLast() { iter_.SeekToLast(); } virtual void Next() { iter_.Next(); } virtual void Prev() { iter_.Prev(); } virtual Slice key() const { return GetLengthPrefixedSlice(iter_.key()); } virtual Slice value() const { Slice key_slice = GetLengthPrefixedSlice(iter_.key()); return GetLengthPrefixedSlice(key_slice.data() + key_slice.size()); } // 该迭代器默认返回 OK virtual Status status() const { return Status::OK(); } private: // 这个是真正干活的家伙 MemTable::Table::Iterator iter_; // 用于 EncodeKey 方法存储编码后的 internal_key std::string tmp_; // No copying allowed MemTableIterator(const MemTableIterator\u0026); void operator=(const MemTableIterator\u0026); }; 下文介绍 SkipList 时会着重介绍迭代器. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#33-memtable-迭代器"},{"categories":null,"content":"3.4 MemTable 内存估计MemTable 提供了一个方法 ApproximateMemoryUsage() 来返回当前 MemTable 占用的内存空间大小. 它的实现依托底层的 Arena, 实现很简单: size_t MemTable::ApproximateMemoryUsage() { return arena_.MemoryUsage(); } Arena 每次分配内存的时候, 都会将分配的内存字节数累加到计数器上. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#34-memtable-内存估计"},{"categories":null,"content":"4 MemTable 比较器之 KeyComparatorMemTable 既然是有序的, 那么任何操作就需要一个比较器. Memtable 内部定义了一个结构体 struct leveldb::MemTable::KeyComparator, 它封装了 MemTable 的比较逻辑, 具体如下: // 一个基于 internal_key 的比较器 // 注意下它的函数调用运算符重载方法的参数类型, 都是 char*, // 原因就是 memtable 底层的 SkipList 的 key 类型就是 char*, // 而类 KeyComparator 对象会被传给 SkipList 作为 key 比较器. struct KeyComparator { const InternalKeyComparator comparator; explicit KeyComparator(const InternalKeyComparator\u0026 c) : comparator(c) { } // 注意, 这个操作符重载方法很关键, 该方法的会先从 char* 类型地址 // 获取 internal_key, 然后对 internal_key 进行比较. // 该方法未在 memtable 直接调用, 而是被底层的 SkipList 使用了. int operator()(const char* a, const char* b) const; }; 该结构体除了在 MemTable::Get() 直接使用比较 user key 以外, 还会被用于构造 SkipList 实例. 其中函数调用运算符的定义如下: int MemTable::KeyComparator::operator()(const char* aptr, const char* bptr) const { // 提取数据项的前半部分, 即 internal_key Slice a = GetLengthPrefixedSlice(aptr); Slice b = GetLengthPrefixedSlice(bptr); // 对 internal_key 进行比较 return comparator.Compare(a, b); } 代码注释中提到的 internal key 在前面章节介绍过了, 不再赘述. 该结构体真正干活的是其封装的 comparator, 它是一个 class leveldb::InternalKeyComparator 实例, 比较器很重要的有两点: 它有一个唯一的名字, 非后向兼容的比较器如果重名则会导致数据库混乱. 毕竟 leveldb 是有序数据库, 顺序至关重要, 一旦被破坏就会混乱. 核心是 Compare 方法, 它决定了什么叫做顺序. InternalKeyComparator 用于比较 internal key, 而 internal key 包含 user key, 而 user key 是用户自己定义的所以比较器也由用户提供. 如果用户不提供, 则采用默认(见 leveldb::Options::Options() 定义)的 class leveldb::\u003cunnamed\u003e::BytewiseComparatorImpl, 它是一个基于字典序进行逐字节比较的内置 comparator 实现. InternalKeyComparator 比较时采用如下处理: 如果 user_key 相等, 则序列号越小 internal_key 越大; 如果序列号也相等, 则操作类型(更新/删除)越小 internal_key 越大(因为 leveldb 可以确保序列号单调递增且唯一, 所以实际上用不到该字段). ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#4-memtable-比较器之-keycomparator"},{"categories":null,"content":"5 MemTable 内存管理之 ArenaLeveldb 专门为了管理内存定义了一个类 class leveldb::Arena, 可以把该类看作是 leveldb 的内存池实现, 但是它只负责对完分配, 并不会做回收重利用. 默认情况下, Arena 维护的内存块都是 4KB 大小. 注意, 同 MemTable 类一样, Arena 类全部操作需要调用者确保线程安全. 该类核心字段如下: 字段 类型 用途 alloc_ptr_ char* 指向 arena 当前空闲字节起始地址 alloc_bytes_remaining_ size_t arena 当前空闲字节数 blocks_ std::vector\u003cchar*\u003e 存放通过 new[] 分配的全部内存块 memory_usage_ port::AtomicPointer arena 持有的全部内存字节数 同 MemTable, Arena 也不支持拷贝构造和赋值构造. 该类核心方法如下: 方法 用途 Arena() 默认构造方法, 无任何限制. char* Allocate(size_t bytes) 从该 arena 返回一个指向大小为 bytes 的内存的指针. bytes 必须大于 0, 具体见实现. char* AllocateAligned(size_t bytes) 返回一个对齐后的可用内存的地址. 具体对齐逻辑见实现. size_t MemoryUsage() const 返回该 arena 持有的全部内存的字节数的估计值(未采用同步设施). size_t MemoryUsage() const 返回该 arena 持有的全部内存的字节数的估计值(未采用同步设施). ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#5-memtable-内存管理之-arena"},{"categories":null,"content":"5.1 如何向 Arena 申请指定大小的内存可以通过方法 Arena::Allocate 申请指定大小的内存, 该方法会被 memtable 的 Add() 方法直接调用. 如果当前 Arena 内存池剩余内存足够则直接分配, 否则向操作系统进行申请. 具体处理流程如下: inline char* Arena::Allocate(size_t bytes) { // 如果我们允许分配 0 字节内存, 那该方法返回什么的语义就有点乱, // 所以我们不允许返回 0 字节(我们内部也没这个需求). assert(bytes \u003e 0); // arena 剩余内存够则直接分配 if (bytes \u003c= alloc_bytes_remaining_) { char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } // arena 可用内存不能满足用户需求, 向系统申请新内存 return AllocateFallback(bytes); } 上面的 AllocateFallback() 根据用户需求决定向系统申请的内存大小, 具体处理如下: // 当 arena 可用内存不够时调用该方法来申请新内存 char* Arena::AllocateFallback(size_t bytes) { // 如果用户要申请的内存超过默认内存块大小(4KB)的 1/4, // 则单独按照用户指定大小分配一个内存块, 否则单独分配一个默认 // 大小(4KB)的内存块给用户会造成太多空间浪费. // 新分配的内存块会被加入 arena 然后将其起始地址返回给用户, // 所分配的内存不会纳入 Arena 管理, 但是占用空间会被纳入计数. if (bytes \u003e kBlockSize / 4) { char* result = AllocateNewBlock(bytes); return result; } // 直接分配一个默认大小内存块给用户, 此处所分配 // 内存会被纳入 Arena 管理, 等待用户后续内存申请. // alloc_ptr_ 被覆盖之前指向的空间被浪费了. alloc_ptr_ = AllocateNewBlock(kBlockSize); alloc_bytes_remaining_ = kBlockSize; // result 是返回给用户的 char* result = alloc_ptr_; // 移动 alloc_ptr_ 指向未被占用内存 alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } 上述处理中还涉及一个辅助方法 AllocateNewBlock, 它负责向系统申请固定大小内存并将其纳入 Arena 管理. 具体处理如下: // 分配一个大小为 block_bytes 的块并将其加入到 arena char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); // 调用者确保线程安全, 这里无需强制同步. memory_usage_.NoBarrier_Store( reinterpret_cast\u003cvoid*\u003e(MemoryUsage() + block_bytes + sizeof(char*))); return result; } ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#51-如何向-arena-申请指定大小的内存"},{"categories":null,"content":"5.2 如何向 Arena 申请对齐后的内存除了上面提到的 Arena::Allocate(size_t bytes) 方法, Arena 还提供另一个内存分配方法, 它支持按照特定字节大小将所分配的内存进行对齐, 它的签名为 char* Arena::AllocateAligned(size_t bytes), 该方法没有被 memtable 代码直接调用, 而是由其底层依赖的 SkipList 所使用. 后面分析 SkipList 代码时还会再次说明. // 按照 8 字节或者指针长度进行内存对齐, 然后返回对齐后分配的内存起始地址 char* Arena::AllocateAligned(size_t bytes) { // 如果机器指针的对齐方式超过 8 字节则按照指针的对齐方式进行对齐; // 否则按照 8 字节进行对齐. const int align = (sizeof(void*) \u003e 8) ? sizeof(void*) : 8; // 确保当按照指针大小对齐时, 机器的指针大小是 2 的幂 assert((align \u0026 (align-1)) == 0); // 求 alloc_ptr 与 align 的模运算的结果, // 以确认 alloc_ptr 是否恰好为 align 的整数倍. size_t current_mod = reinterpret_cast\u003cuintptr_t\u003e(alloc_ptr_) \u0026 (align-1); // 如果 alloc_ptr 的值恰好为 align 整数倍, // 则已经满足对齐要求, 可以从该地址直接进行分配; // 否则, 需要进行手工对齐, 比如 align 为 8, alloc_ptr 等于 15, // 则需要将 alloc_ptr 增加 align - current_mod = 8 - 7 = 1 个字节. size_t slop = (current_mod == 0 ? 0 : align - current_mod); // 虽然用户申请 bytes 个字节, 但是因为对齐要求, // 实际消耗的内存大小为 bytes + slop size_t needed = bytes + slop; char* result; // 如果 arena 空闲内存满足要求则直接分配 if (needed \u003c= alloc_bytes_remaining_) { // 将 result 指向对齐后的地址(对齐会导致前 slop 个字节被浪费掉) result = alloc_ptr_ + slop; alloc_ptr_ += needed; alloc_bytes_remaining_ -= needed; } else { // 否则从堆中申请新的内存块, 注意重新分配 // 内存块时 malloc 会保证对齐, 无序再如上做手工对齐. result = AllocateFallback(bytes); } assert((reinterpret_cast\u003cuintptr_t\u003e(result) \u0026 (align-1)) == 0); return result; } ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#52-如何向-arena-申请对齐后的内存"},{"categories":null,"content":"6 MemTable 底层存储之 SkipListMemTable 是有序的, 为了确保查询和插入迅速, 它采用了 SkipList 作为存储结构. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6-memtable-底层存储之-skiplist"},{"categories":null,"content":"6.1 SkipList 原理简介开始之前先大体介绍下 SkipList 这个数据结构. 目前该数据结构在开源项目中用的越来越多代替红黑树等存在, 这里包括 Redis 都用到了该数据结构. 你在学校可能没学过这个数据结构, 但是你肯定在操作系统课上学过多级页表. SkipList 加速原理跟多级页表差不多, 当然后者如此设计也为了节省存储. 整个内存字节空间分成若干页面, 然后把全部页面分成若干组, 然后把上一步若干组进一步分为多个二级组 …; 查找时从最外层找起, 从外向里索引粒度逐渐减小, 最后一下子定位到内存空间某个字节位置, 这里虽然没说排序, 但是由于每个字节已经被编码所以其实也是有序的. SkipList 与之类似, 每个元素所在节点在一个链表上按序排列, 如果我们把每个节点看作内存空间一个字节位置, 那么接下来我们就要为它们建索引了: 第 0 级索引就是原始链表, 一个接一个, 共 $n$ 个节点 第 $1$ 级索引就是在上一级基础上间隔一个选取一个, 这样构成第 $1$ 级索引, 共 $\\frac{n}{2}$ 个节点 第 $2$ 级索引也是在上一级基础设间隔一个取一个, 这样构成第 $2$ 级索引, 共 $\\frac{n}{2^2}$ 个节点 … … 最后从上一级只能提取两个节点了, 这就是最后一级索引了因为没必要再往下分了, 假设共有 h 级, 那么这就是第 h-1 级索引, 该级索引共 $\\frac{n}{2^{h-1}}$ 个节点 我们调过头反着看, 每一层索引节点数都是其上一层索引的 $\\frac{1}{2}$, 这不就是个平衡二叉树吗? 总的节点数为 $2n$ 个. 每次查询时, 从最外层索引向里找, 每次在每一层只需查询一次, 最多查询 h 次也就是 $\\log_2^{2n} = 1 + \\log_2^n \\approx \\log_2^n$ 即可找到或则确认不存在. 我们把最外层看作最高层, 最里层看作最底层, 则 SkipList 的高度为 h. 可以看出时间复杂度同平衡二叉树如红黑树是一样的, 只是空间复杂度要多一倍. 那为什么不用红黑树呢, 嘿嘿, 你手写一次就知道了. 那么 SkipList 是如何确保平衡的呢? 方法强大又朴素, 抛硬币, 五五分. 每次插入元素时, 自上而下定位到其插入位置, 在第 $0$ 级插入元素时, 通过抛硬币的办法决定是否将该元素拔擢到第 $1$ 级索引中, 如果拔擢成功则再次抛硬币决定是否将其继续拔擢到第 $2$ 级索引, 以此类推, 除非上一级拔擢成功则继续抛硬币, 否则停止拔擢过程, 索引插入的时间复杂度也是 $\\log_2^n$. 删除类似, 从外层到内层, 查找到该元素后, 则从该层开始递进删除每一层中的该元素, 时间复杂度依然是 $\\log_2^n$. 好了, 我们来看看 leveldb 是如何实现 SkipList 的. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#61-skiplist-原理简介"},{"categories":null,"content":"6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#62-skiplist-实现"},{"categories":null,"content":"6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#621-skiplist-核心数据成员"},{"categories":null,"content":"6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template\u003cclass Key, class Comparator\u003e struct leveldb::Node\u003cKey, Comparator\u003e 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList\u003cKey, Comparator\u003e::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast\u003cNode*\u003e(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast\u003cNode*\u003e(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template\u003ctypename Key, class Comparator\u003e typename SkipList\u003cKey,Comparator\u003e::Node* SkipList\u003cKey,Comparator\u003e::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template\u003ctypename Key, class Comparator\u003e int SkipList\u003cKey,Comparator\u003e::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template\u003ctypename Key, class Comparator\u003e void SkipList\u003cKey,Comparator\u003e::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大约等于目标 key 的节点, 一会会把 key // 插到这个节点前面. // 如果为 nullptr 表示当前 SkipList 节点都比 key 小. Node* x = FindGreaterOrEqual(key, prev); // 虽然 x 是我们找到的第一个大于等于目标 key 的节点, // 但是 leveldb 不允许重复插入 key 相等的数据项. assert(x == nullptr || !Equal(key, x-\u003ekey)); // 确定待插入节点的最大索引层数 int height = RandomHeight(); // 更新 SkipList 实例维护的最大索引层数 if (height \u003e GetMaxHeight()) { // 如果最大索引层数有变, 则当前节点将是索引层数最多的节点, // 需要将前面求得","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#622-skiplist-核心方法"},{"categories":null,"content":"6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 templatestruct leveldb::Node表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n = 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n = 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n = 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n = 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. templatetypename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-AllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. templateint SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height 0); assert(height void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大约等于目标 key 的节点, 一会会把 key // 插到这个节点前面. // 如果为 nullptr 表示当前 SkipList 节点都比 key 小. Node* x = FindGreaterOrEqual(key, prev); // 虽然 x 是我们找到的第一个大于等于目标 key 的节点, // 但是 leveldb 不允许重复插入 key 相等的数据项. assert(x == nullptr || !Equal(key, x-key)); // 确定待插入节点的最大索引层数 int height = RandomHeight(); // 更新 SkipList 实例维护的最大索引层数 if (height GetMaxHeight()) { // 如果最大索引层数有变, 则当前节点将是索引层数最多的节点, // 需要将前面求得","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6223-辅助数据结构-node"},{"categories":null,"content":"6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 templatestruct leveldb::Node表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n = 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n = 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n = 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n = 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. templatetypename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-AllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. templateint SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height 0); assert(height void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大约等于目标 key 的节点, 一会会把 key // 插到这个节点前面. // 如果为 nullptr 表示当前 SkipList 节点都比 key 小. Node* x = FindGreaterOrEqual(key, prev); // 虽然 x 是我们找到的第一个大于等于目标 key 的节点, // 但是 leveldb 不允许重复插入 key 相等的数据项. assert(x == nullptr || !Equal(key, x-key)); // 确定待插入节点的最大索引层数 int height = RandomHeight(); // 更新 SkipList 实例维护的最大索引层数 if (height GetMaxHeight()) { // 如果最大索引层数有变, 则当前节点将是索引层数最多的节点, // 需要将前面求得","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6224-用于为新数据分配存储的-newnode-方法"},{"categories":null,"content":"6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 templatestruct leveldb::Node表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n = 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n = 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n = 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n = 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. templatetypename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-AllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. templateint SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height 0); assert(height void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大约等于目标 key 的节点, 一会会把 key // 插到这个节点前面. // 如果为 nullptr 表示当前 SkipList 节点都比 key 小. Node* x = FindGreaterOrEqual(key, prev); // 虽然 x 是我们找到的第一个大于等于目标 key 的节点, // 但是 leveldb 不允许重复插入 key 相等的数据项. assert(x == nullptr || !Equal(key, x-key)); // 确定待插入节点的最大索引层数 int height = RandomHeight(); // 更新 SkipList 实例维护的最大索引层数 if (height GetMaxHeight()) { // 如果最大索引层数有变, 则当前节点将是索引层数最多的节点, // 需要将前面求得","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6225-用于确定某个节点索引层数的-randomheight-方法"},{"categories":null,"content":"6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 templatestruct leveldb::Node表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n = 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n = 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n = 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n = 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. templatetypename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-AllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. templateint SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height 0); assert(height void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大约等于目标 key 的节点, 一会会把 key // 插到这个节点前面. // 如果为 nullptr 表示当前 SkipList 节点都比 key 小. Node* x = FindGreaterOrEqual(key, prev); // 虽然 x 是我们找到的第一个大于等于目标 key 的节点, // 但是 leveldb 不允许重复插入 key 相等的数据项. assert(x == nullptr || !Equal(key, x-key)); // 确定待插入节点的最大索引层数 int height = RandomHeight(); // 更新 SkipList 实例维护的最大索引层数 if (height GetMaxHeight()) { // 如果最大索引层数有变, 则当前节点将是索引层数最多的节点, // 需要将前面求得","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6226-用于插入数据的-insert-方法"},{"categories":null,"content":"6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 templatestruct leveldb::Node表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n = 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n = 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n = 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n = 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. templatetypename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-AllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. templateint SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height 0); assert(height void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大约等于目标 key 的节点, 一会会把 key // 插到这个节点前面. // 如果为 nullptr 表示当前 SkipList 节点都比 key 小. Node* x = FindGreaterOrEqual(key, prev); // 虽然 x 是我们找到的第一个大于等于目标 key 的节点, // 但是 leveldb 不允许重复插入 key 相等的数据项. assert(x == nullptr || !Equal(key, x-key)); // 确定待插入节点的最大索引层数 int height = RandomHeight(); // 更新 SkipList 实例维护的最大索引层数 if (height GetMaxHeight()) { // 如果最大索引层数有变, 则当前节点将是索引层数最多的节点, // 需要将前面求得","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6226-用于查找数据是否存在的-contains-方法"},{"categories":null,"content":"6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 templatestruct leveldb::Node表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n = 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n = 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n = 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n = 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. templatetypename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-AllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. templateint SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height 0); assert(height void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大约等于目标 key 的节点, 一会会把 key // 插到这个节点前面. // 如果为 nullptr 表示当前 SkipList 节点都比 key 小. Node* x = FindGreaterOrEqual(key, prev); // 虽然 x 是我们找到的第一个大于等于目标 key 的节点, // 但是 leveldb 不允许重复插入 key 相等的数据项. assert(x == nullptr || !Equal(key, x-key)); // 确定待插入节点的最大索引层数 int height = RandomHeight(); // 更新 SkipList 实例维护的最大索引层数 if (height GetMaxHeight()) { // 如果最大索引层数有变, 则当前节点将是索引层数最多的节点, // 需要将前面求得","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6227-读写都要依赖的辅助方法-findgreaterorequal"},{"categories":null,"content":"6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 templatestruct leveldb::Node表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n = 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n = 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n = 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n = 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. templatetypename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-AllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. templateint SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height 0); assert(height void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大约等于目标 key 的节点, 一会会把 key // 插到这个节点前面. // 如果为 nullptr 表示当前 SkipList 节点都比 key 小. Node* x = FindGreaterOrEqual(key, prev); // 虽然 x 是我们找到的第一个大于等于目标 key 的节点, // 但是 leveldb 不允许重复插入 key 相等的数据项. assert(x == nullptr || !Equal(key, x-key)); // 确定待插入节点的最大索引层数 int height = RandomHeight(); // 更新 SkipList 实例维护的最大索引层数 if (height GetMaxHeight()) { // 如果最大索引层数有变, 则当前节点将是索引层数最多的节点, // 需要将前面求得","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6228-辅助数据结构-iterator"},{"categories":null,"content":"7 番外: C++ 的内存排序本节内容主要参考 cppreference. Leveldb 内部多处用到同步设施, 这一块内容比较多, 后续会单独开辟一片文章介绍 leveldb 的原子类型实现. std::memory_order 指定了如何围绕原子操作对内存访问（包括常规的、非原子的内存访问）进行排序。当无任何顺序限制的时候, 在多核系统中, 当多个线程同时读写多个变量时, 一个线程观测到的某个值的变化顺序可能与负责写的线程的写顺序不同. 实际上, 甚至多个读线程之间各自观测到的变化顺序也互不相同. 此类情况哪怕在单核系统上也会发生, 因为只要内存模型允许, 编译器会在编译程序时进行指令重排序. 库中的全部原子操作的默认行为提供了顺序一致的排序(具体见后文). 这可能会造成性能损失, 但是可以给库中的原子操作提供额外的 std::memory_order 参数来指定具体的顺序限制, 除了保障原子性, 编译器和处理器必须执行这类限制确保顺序性. memory_order_relaxed Relaxed 操作: 仅保证该操作的原子性, 不会针对其它读写操作施加同步或者顺序限制(此即 Relaxed ordering) memory_order_consume 带有此限制的 load 操作针对受影响内存执行一个 consume 行为: 执行 load 操作的当前线程中后续针对该变量的读写操作不会被重排序到 load 操作之前, 从而确保后续读写用的都是最新加载的数据. 其它线程中, 如果先发生过针对同一个原子变量的写操作, 那么在它们 release 该原子变量之前针对该原子变量依赖的其它变量的写操作, 在当前线程 load 该原子变量时也都是可见的. 在大多数平台上, 这仅仅会影响编译器优化(此即 Release-Consume ordering). memory_order_acquire 带有此限制的 load 操作针对受影响内存执行一个 acquire 行为: 执行 load 操作的当前线程中后续依赖该变量的读写操作不会被重排序到该 load 操作之前, 从而确保后续读写用的都是最新加载的数据. 其它线程中, 如果先发生过针对同一个原子变量的写操作, 那么在它们 release 该原子变量之前针对其它变量的写操作(无论该原子变量是否依赖这些变量, 这比 consume 语义更强烈), 在当前线程 load 该原子变量时都是可见的(此即 Release-Acquire ordering). memory_order_release 带有此限制的 store 操作执行一个 release 行为: 执行 store 操作的当前线程中其它针对该变量的读写操作都不会被重排序到当前 store 操作之后, 从而确保之前的读写用的都是本次修改之前的值. 在此 store 之前的针对其它变量的全部写操作对 acquire 该原子变量的其它线程也都是可见的(此即 Release-Acquire ordering), 同时针对该原子变量所依赖的变量的写操作针对 consume 该原子变量的其它线程也都是可见的(此即 Release-Consume ordering). memory_order_acq_rel 带有此限制的 read-modify-write 操作既是一个 acquire 操作也是一个 release 操作. 执行 read-modify-write 操作的当前线程针对同一个变量的其它读写操作不能被重排序到该 store 操作之前或之后. 其它线程中针对该原子变量的写操作在 release 之后针对当前线程的 modify 是可见的, 同理当前线程的 modify 在 release 之后对其它 acquire 该原子变量的其它线程也是可见的. memory_order_seq_cst 带有此限制的 load 操作执行一个 acquire 行为, 带有此限制的 store 操作执行一个 release 行为, 带有此限制的 read-modify-write 操作既执行一个 acquire 行为也执行一个 release 行为, 此外, 该限制确保了一个单一全序, 也就是说, 全部线程观测道德全部修改行为都是一个顺序(此即 Sequentially-consistent ordering). 该限制语义最强. –End– ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#7-番外-c-的内存排序"},{"categories":null,"content":"我们先简单回顾下 log 文件相关的基础知识点, 具体请见 Leveldb 源码详解系列之一: 接口与文件. log 文件(*.log)保存着数据库最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead logging). 当前在用的 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 每个更新操作都被追加到当前的 log 文件和 memtable 中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件对应的 memtable 就会被转换为一个 sorted string table 文件落盘然后一个新的 log 文件就会被创建以保存未来的更新操作. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#"},{"categories":null,"content":"log 文件结构log 文件内容是一系列 blocks, 每个 block 大小为 32KB(有时候最后一个 block 可能装不满). 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] type 取值如下: FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#log-文件结构"},{"categories":null,"content":"读 log下面分析读 log 相关的类和方法. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#读-log"},{"categories":null,"content":"核心文件与核心类与读 log 相关的代码定义在下面两个文件中: db/log_reader.h db/log_reader.cc 核心类为 class leveldb::log::Reader. 下面针对这个类核心方法进行分析. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#核心文件与核心类"},{"categories":null,"content":"Reader 构造方法 // 创建一个 Reader 来从 file 中读取和解析 records, // 读取的第一个 record 的起始位置位于文件 initial_offset 或其之后的物理地址. // 如果 reporter 不为空, 则在检测到数据损坏时汇报要丢弃的数据估计大小. // 如果 checksum 为 true, 则在可行的条件比对校验和. // 注意, file 和 reporter 的生命期不能短于 Reader 对象. Reader(SequentialFile* file, Reporter* reporter, bool checksum, uint64_t initial_offset) ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#reader-构造方法"},{"categories":null,"content":"Reader 读取方法bool ReadRecord(Slice* record, std::string* scratch) 方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 所做的事情, 概括地讲就是从文件读取下一个 record 到 *record 中. 如果读取成功, 返回 true; 遇到文件尾返回 false. 如果当前读取的 record 没有被分片, 那就用不到 *scratch 参数来为 *record 做底层存储了; 其它情况需要借助 *scratch 来拼装分片的 record data 部分, 最后封装为一个 Slice 赋值给 *record. 具体处理流程见下面详细注释: bool Reader::ReadRecord(Slice* record, std::string* scratch) { // last_record_offset_ 表示上一次调用 ReadRecord 方法返回的 // record 的起始偏移量, 注意这个 record 是逻辑的. // initial_offset_ 表示用户创建 Reader 时指定的在文件中寻找第一个 record 的起始地址. // 如果条件成立, 表示当前方法是首次被调用. if (last_record_offset_ \u003c initial_offset_) { // 跳到我们要读取的第一个 block 起始位置 if (!SkipToInitialBlock()) { return false; } } scratch-\u003eclear(); record-\u003eclear(); // 指示正在处理的 record 是否被分片了, // 除非逻辑 record 对应的物理 record 类型是 full, 否则就是被分片了. bool in_fragmented_record = false; // 记录我们正在读取的逻辑 record 的起始偏移量. 初值为 0 无实际意义仅为编译器不发警告. // 为啥叫逻辑 record 呢？ // 因为 block 大小限制, 所以 record 可能被分成多个分片(fragment). // 我们管 fragment 叫物理 record, 一个或多个物理 record 构成一个逻辑 record. uint64_t prospective_record_offset = 0; Slice fragment; while (true) { // 从文件读取一个物理 record 并将其 data 部分保存到 fragment, // 同时返回该 record 的 type. const unsigned int record_type = ReadPhysicalRecord(\u0026fragment); // 计算返回的当前 record 在 log file 中的起始地址 // = 当前文件待读取位置 // - buffer 剩余字节数 // - 刚读取的 record 头大小 // - 刚读取 record 数据部分大小 // end_of_buffer_offset_ 表示 log file 待读取字节位置 // buffer_ 表示是对一整个 block 数据的封装, 底层存储为 backing_store_, // 每次执行 ReadPhysicalRecord 时会移动 buffer_ 指针. uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size() - kHeaderSize - fragment.size(); // resyncing_ 用于跳过起始地址不符合 initial_offset_ 的 record, // 如果为 true 表示目前还在定位第一个满足条件的逻辑 record 中. // 与 initial_offset_ 的比较判断在上面 ReadPhysicalRecord 中进行. if (resyncing_) { // 只要数据没有损坏或到达文件尾, 而且返回的 record_type 只要 // 不是 kBadRecord(返回该类型其中一个情况就是起始地址不满足条件) // 就说明当前 record 起始地址已经大于 initial_offset_ 了, // 但是如果当前 record 的 type 为 middle 或者 last, // 那么逻辑上这个 record 仍然与不符合 initial_offset_ 的 // 类型为 first 的 record 同属一个逻辑 record, // 所以当前 record 也不是我们要的. if (record_type == kMiddleType) { continue; } else if (record_type == kLastType) { resyncing_ = false; continue; continue; } else { // 如果是 full 类型的 record, 而且这个 record 起始地址 // 不小于 inital_offset_(否则 ReadPhysicalRecord 返回的 // 类型就是 kBadRecord 而非 full), // 满足条件了, 关掉标识. // 如果返回 kBadRecord/kEof(没什么可读了)/ // 未知类型(但是起始位置满足要求), 也会关掉该标识. resyncing_ = false; } } // 注意, 下面 switch 有的 case 是 return, 有的是 break. switch (record_type) { case kFullType: if (in_fragmented_record) { // 早期版本 writer 实现存在 bug. // 即如果上一个 block 末尾保存的是一个 FIRST 类型的 header, // 那么接下来 block 开头应该是一个 MIDDLE 类型的 record, // 但是早期版本写入了 FIRST 类型或者 FULL 类型的 record. if (!scratch-\u003eempty()) { ReportCorruption(scratch-\u003esize(), \"partial record without end(1)\"); } } prospective_record_offset = physical_record_offset; scratch-\u003eclear(); // 赋值构造 // FULL 类型 record 不用借助 scratch 拼装了 *record = fragment; last_record_offset_ = prospective_record_offset; // 读取到一个完整逻辑 record, 完成任务. return true; // 注意, 只有 first 类型的 record 起始地址满足大于 inital_offset_ 的时候 // 才会返回其真实类型 first, 其它情况哪怕是 first 返回也是 kBadRecord. case kFirstType: if (in_fragmented_record) { // 早期版本 writer 实现存在 bug. // 即如果上一个 block 末尾保存的是一个 FIRST 类型的 header, // 那么接下来 block 开头应该是一个 MIDDLE 类型的 record, // 但是早期版本写入了 FIRST 类型或者 FULL 类型的 record. if (!scratch-\u003eempty()) { ReportCorruption(scratch-\u003esize(), \"partial record without end(2)\"); } } // FIRST 类型物理 record 起始地址也是对应逻辑 record 的起始地址 prospective_record_offset = physical_record_offset; // 非 FULL 类型 record 需要借助 scratch 拼装成一个完整的 record data 部分. // 注意只有 first 时采用 assign, first 后面的分片要用 append scratch-\u003eassign(fragment.data(), fragment.size()); // 除了 FULL 类型 record, 都说明当前读取的 record 被分片了, // 还需要后续继续读取. in_fragmented_record = true; // 刚读了 first, 没读完, 继续. break; case kMiddleType: // 都存在 MIDDLE 了, 竟然还说当前 record 没分片, 报错. if (!in_fragmented_record) { ReportCorruption(fragment.size(), \"missing start of fragmented record(1)\"); } else { // 非 FULL 类型 record 需要借助 scratch 拼装成一个 // 完整的 record data 部分, // F","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#reader-读取方法"},{"categories":null,"content":"写 log下面分析写 log 相关的类和方法. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#写-log"},{"categories":null,"content":"核心文件与核心类与写 log 相关的代码定义在下面两个文件中: db/log_writer.h db/log_writer.cc 核心类为 class leveldb::log::Writer. 下面针对这个类核心方法进行分析. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#核心文件与核心类-1"},{"categories":null,"content":"Writer 构造方法 // 创建一个 writer 用于追加数据到 dest 指向的文件. // dest 指向的文件初始必须为空文件; dest 生命期不能短于 writer. explicit Writer(WritableFile *dest); // 创建一个 writer 用于追加数据到 dest 指向的文件. // dest 指向文件初始长度必须为 dest_length; dest 生命期不能短于 writer. Writer(WritableFile *dest, uint64_t dest_length); ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#writer-构造方法"},{"categories":null,"content":"Writer 写方法如果用户想把数据写入 log, 则需要将这些数据封装为 Slice, 然后调用 Writer::AddRecord 将其写入 log 文件. 写入时, 这个 Slice 内容即为 record 的 data 部分, 如果数据量太大导致一个 block(默认 32KB) 装不下, 则这些数据会被分片写入. 也就是说, 这些数据属于一个逻辑 record, 但是因为太大, 被分为若干物理 record 写入到 log 文件. 具体写入流程见源码注释: Status Writer::AddRecord(const Slice\u0026 slice) { const char* ptr = slice.data(); // data 剩余部分长度, 初始值为其原始长度 size_t left = slice.size(); // 如有必要则将 record 分片后写入文件. // 如果 slice 内容为空, 则我们仍将会写入一个长度为 0 的 record 到文件中. Status s; bool begin = true; do { // 当前 block 剩余空间大小 const int leftover = kBlockSize - block_offset_; assert(leftover \u003e= 0); // 如果当前 block 剩余空间不足容纳 record 的 header(7 字节) // 则剩余空间作为 trailer 填充 0, 然后切换到新的 block. if (leftover \u003c kHeaderSize) { if (leftover \u003e 0) { assert(kHeaderSize == 7); // 最终填充多少 0 由 leftover 决定, 最大 6 字节 dest_-\u003eAppend(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover)); } block_offset_ = 0; } // 到这一步, block (可能因为不足 kHeaderSize 在上面已经切换到了下个 block) // 最终剩余字节必定大约等于 kHeaderSize assert(kBlockSize - block_offset_ - kHeaderSize \u003e= 0); // block 当前剩余空闲字节数. // 除了待写入 header, 当前 block 还剩多大空间, 可能为 0; // block 最后剩下空间可能只够写入一个新 record 的 header 了 const size_t avail = kBlockSize - block_offset_ - kHeaderSize; // 可以写入当前 block 的 record data 剩余内容的长度, 可能为 0 const size_t fragment_length = (left \u003c avail) ? left : avail; RecordType type; // 判断是否将 record 剩余内容分片 const bool end = (left == fragment_length); if (begin \u0026\u0026 end) { // 如果该 record 内容第一次写入文件, 而且, // 如果 block 剩余空间可以容纳 record data 全部内容, // 则写入一个 full 类型 record type = kFullType; } else if (begin) { // 如果该 record 内容第一写入文件, 而且, // 如果 block 剩余空间无法容纳 record data 全部内容, // 则写入一个 first 类型 record. // 注意, 此时是 record 第一次写入即它是一个新 record, // 该 block 剩余空间可能只够容纳 header 了, // 则在 block 尾部写入一个 FIRST 类型 header, record data 不写入, // 等下次循环会切换到下个 block, 然后又会重新写入一个 // 非 FIRST 类型的 header (注意下面会将 begin 置为 false) // 而不是紧接着在新 block 只写入 data 部分. type = kFirstType; } else if (end) { // 如果这不是该 record 内容第一写入文件, 而且, // 如果 block 剩余空间可以容纳 record data 剩余内容, // 则写入一个 last 类型 record type = kLastType; } else { // 如果这不是该 record 内容第一写入文件, 而且, // 如果 block 剩余空间无法容纳 record data 剩余内容, // 则写入一个 middle 类型 record type = kMiddleType; } // 将类型为 type, data 长度为 fragment_length 的 record 写入 log 文件. s = EmitPhysicalRecord(type, ptr, fragment_length); ptr += fragment_length; left -= fragment_length; // 即使当前 block 剩余空间只够写入一个新 record 的 FIRST 类型 header, // record 也算写入过了 begin = false; // 写入不出错且 record 再无剩余内容则写入完毕 } while (s.ok() \u0026\u0026 left \u003e 0); return s; } ‘AddRecord写入 record 时依赖的辅助方法EmitPhysicalRecord`. 该方法负责组装 record header, 然后连同 payload 写入文件. Status Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t n) { // data 大小必须能够被 16 位无符号整数表示, 因为 record 的 length 字段只有两字节 assert(n \u003c= 0xffff); // 要写入的内容不能超过当前 block 剩余空间大小 assert(block_offset_ + kHeaderSize + n \u003c= kBlockSize); // buf 用于组装 record header char buf[kHeaderSize]; // 将数据长度编码到 length 字段, 小端字节序 // length 低 8 位安排在低地址位置 buf[4] = static_cast\u003cchar\u003e(n \u0026 0xff); // 然后写入 length 高 8 位安排在高地址位置 buf[5] = static_cast\u003cchar\u003e(n \u003e\u003e 8); // 将 type 编码到 type 字段, type 紧随 length 之后 1 字节 buf[6] = static_cast\u003cchar\u003e(t); // 计算 type 和 data 的 crc 并编码安排在最前面 4 个字节 uint32_t crc = crc32c::Extend(type_crc_[t], ptr, n); crc = crc32c::Mask(crc); // 将 crc 写入到 header 前四个字节 EncodeFixed32(buf, crc); // 写入 header Status s = dest_-\u003eAppend(Slice(buf, kHeaderSize)); if (s.ok()) { // 写入 payload s = dest_-\u003eAppend(Slice(ptr, n)); if (s.ok()) { // 刷入文件 s = dest_-\u003eFlush(); } } // 当前 block 剩余空间起始偏移量. // 注意, 这里不管 header 和 data 是否写成功. block_offset_ += kHeaderSize + n; return s; } –End– ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#writer-写方法"},{"categories":null,"content":"[toc] 最新的 Go Weekly 推送了这篇文章, eBPF 作为新时代的剖析工具正在如火如荼发展, 读完感觉用来入门很好, 就根据自己理解编译了这篇文章. 做实验过程遇到一些问题, 在最后加了一个番外章节可参考. 下面正式开始. 不用重新编译/部署线上程序而是借助 eBPF 即可实现对程序进行调试, 接下来我们会用一个系列文章介绍我们是怎么做的, 这是开篇. 本篇描述了如何使用 gobpf 和 uprobe 来构建一个跟踪 Go 程序函数入口参数变化的应用. 这里介绍的技术可以扩展到其它编译型语言, 如 C++, Rust 等等. 本系列文章后续将会讨论如何使用 eBPF 来跟踪 HTTP/gRPC 数据和 SSL 等等. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:0:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#"},{"categories":null,"content":"介绍当调试程序时, 我们一般对捕获程序的运行时状态非常感兴趣. 因为这可以让我们检查程序在干什么, 并能让我们确定 bug 出现在程序的哪一块. 观察运行时状态的一个简单方式是使用调试器. 比如针对 Go 程序, 我们可以使用 Delve 和 gdb. Delve 和 gdb 在开发环境中做调试表现没得说, 但是我们一般不会在线上使用此类工具. 它们的长处同时也是它们的短处, 因为调试器会导致线上程序中断, 甚至如果在调试过程中不小心改错某个变量的值而导致线上程序出现异常. 为了让线上调试过程的侵入和影响更小, 我们将会探索使用增强版的 BPF(eBPF, Linux 4.x+ 内核可用)和更高级的 Go 库 gobpf 来达成目标. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:0:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#介绍"},{"categories":null,"content":"什么是 eBPF扩展型 BPF(eBPF) 是一项在 Linux 4.x+ 内核可用的技术. 你可以把它看作一个轻量级的沙箱 VM, 它运行在 Linux 内核中并且提供了针对内核内存的可信访问. 就像下面要说的, eBPF 允许内核运行 BPF 字节码. 虽然可用的前端(这里指的是编译器前端)语言多样, 但通常都是 C 语言的真子集. 通常 C 代码先通过 Clang 被编译为 BPF 字节码, 然后字节被验证以确保可以安全执行. 这些严格的验证保证了机器码不会有意或无意地危及 Linux 内核, 同时也确保了 BPF 探针在每次被触发时将会执行有限数目的指令. 这些保证确保了 eBPF 可以被用于性能敏感的应用中, 比如包过滤, 网络监控等等. 从功能上说, eBPF 允许你针对某些事件(如定时器事件, 网络事件或是函数调用事件)运行受限的 C 代码. 当因为一个函数调用事件被触发时, 我们把这些 eBPF 代码叫做探针. 这些探针既可以针对内核函数调用事件被触发(这时叫 kprobe, k 即 kernelspace), 也可以针对用户空间的函数调用事件被触发(这时叫 uprobe, u 即 userspace). 本篇文章讲解如何通过 uprobe 实现函数参数的动态追踪. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:0:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#什么是-ebpf"},{"categories":null,"content":"UprobesUprobes 允许我们通过插入一个 debug trap 指令(在 x86 上就是 int3)触发一个软中断从而实现对运行在用户空间的程序进行拦截. 这也是调试器的工作原理. uprobe 运行过程本质上与其它 BPF 程序一样, 可以总结为下面图示: 用于跟踪的 BPF(来自 Brendan Gregg) 编译和验证过的 BPF 程序作为 uprobe 的一部分被执行, 同时执行结果写入到一个 buffer 中. 下面让我们研究下 uprobes 如何起作用的. 为了演示部署 uprobes 并捕获函数参数, 我们会用到这个简单的 demo 应用. 该 demo 相关部分下面介绍. main() 方法是一个简单的 HTTP server, 它暴露了一个监听 /e 端点的 GET 接口, 该接口通过迭代逼近计算自然常数 e(也叫欧拉数). computeE 方法有一个参数 iters, 它指定了逼近时的迭代次数. 迭代次数越多, 结果越精确, 当然耗费 CPU 也越多. 迭代逼近算法不是我们本次关注重点, 感兴趣的可以自己研究下. 我们仅对追踪调用 computeE 方法时的参数感兴趣. func computeE(iterations int64) float64 { res := 2.0 fact := 1.0 for i := int64(2); i \u003c iterations; i++ { fact *= float64(i) res += 1 / fact } return res } func main() { http.HandleFunc(\"/e\", func(w http.ResponseWriter, r *http.Request) { // ... 省略代码用于从 get 请求中解析 iters 参数, 若为空则使用默认值 w.Write([]byte(fmt.Sprintf(\"e = %0.4f\\n\", computeE(iters)))) }) // 启动 server... } 为了进行后面的实验以及为最后采用 gdb 验证修改生效, 我们采用如下指令编译该代码: $ go build -gcflags \"-N -l\" app.go 为了理解 uprobe 如何工作的, 我们看看可执行文件中要追踪的符号. 既然 uprobes 通过插入一个 debug trap 指令到可执行文件来实现, 我们先要确定要追踪的函数地址是什么. Go 程序在 Linux 上的二进制采用 ELF 格式存储 debug 信息, 该信息甚至在优化过的二进制中也是存在的, 除非 debug 数据被裁剪掉了. 我们可以使用命令 objdump 来检查二进制文件中的符号: # 执行下面命令之前需要你先将上面 go 程序编译为名为 app 的二进制文件. # objdump --syms 可以从可执行程序中导出全部符号, 然后通过 grep 查找 computeE. # 具体输出可能与你机器上不同, 这没什么问题. $ objdump --syms app | grep computeE 00000000000x6600e0 g F .text 000000000000004b main.computeE 从上述输出可以看到, computeE 方法的入口地址为 0x0x6600e0. 为了看一下这个地址附近的指令, 我们可以通过 objdump 来反汇编该二进制文件(通过命令行选项 -d). 反汇编代码如下: $ objdump -d app | grep -A 1 0x6600e0 00000000000x6600e0 \u003cmain.computeE\u003e: 0x6600e0: 48 8b 44 24 08 mov 0x8(%rsp),%rax 从上面汇编代码可以看到当 computeE 方法被调用时会执行哪些指令. 第一条指令是 mov 0x8(%rsp),%rax, 该指令将距寄存器 rsp 保存的地址(栈指针寄存器保存的是 computeE 方法的入口地址)相对偏移量为 0x8 处的内容移动到寄存器 rax 中. 这个被移动的值即为 computeE 方法的入参 iterations 的值. Go 程序的参数通过栈来传递. 好了, 记住上面提到的信息, 我们来看看如何实现针对 computeE 方法的参数追踪. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:0:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#uprobes"},{"categories":null,"content":"构建追踪程序我们给这个追踪程序起个名叫 Tracer. 为了捕获前面提到的事件, 我们需要注册一个 uprobe 函数, 并且还得有个用户态函数负责去读 uprobe 的输出, 具体如下图所示: 我们编写一个叫做 tracer 的应用, 由它负责注册 BPF 代码, 同时读取这些 BPF 代码的输出. 如上图所示, uprobe 将会简单地输出到一个 perf-buffer 中, 该结构体是用于 perf 事件的 linux 内核数据结构. 万事俱备, 我们来看看当我们增加一个 uprobe 时会发生哪些事情. 下面的图显示了 Linux 内核如何使用一个 uprobe 来修改一个已有的二进制程序. 前文提到的软中断 int3 作为第一条指令被插入到 main.computeE 方法中. 这条指令将会在执行时触发一个软中断, 从而允许 Linux 内核来执行 BPF 代码. 然后我们把 computeE 每次被调用时的参数输出到 perf-buffer 中, 这些值会被我们编写的 tracer 应用异步地读取. 就我们这个需求来说, 相应的 BPF 代码很简单, C 代码如下: #include \u003cuapi/linux/ptrace.h\u003eBPF_PERF_OUTPUT(trace); // 该函数将会被注册, 以便每次 main.computeE 被调用时该函数也会被调用 inline int computeECalled(struct pt_regs *ctx) { // main.computeE 的入参保存在了 ax 寄存器里. long val = ctx-\u003eax; trace.perf_submit(ctx, \u0026val, sizeof(val)); return 0; } 我们注册上面代码以便 main.computeE 方法被调用它们也会被执行. 这些代码被执行时, 我们仅仅读取函数参数然后写到 perf-buffer 中. 实现这个功能需要很多样板代码, 为了方便示意这里都省掉了, 完整的例子见这里. 好了, 我们现在有个针对 main.computeE 的功能齐全的端到端参数追踪器了! 执行结果见下面动图: 上述动图执行步骤如下: 1 在 localhost:9090 启动待追踪程序 ./app, 此时我们可以用 curl 访问该应用了, 具体命令为 curl http://localhost:9090/e?iters=10 2 启动 trace 应用, 注意指定参数 sudo ./trace --binary ../app/app, 参数是第一步中待追踪程序对应的二进制文件的路径. 3 不停的执行 curl 命令, 使其 iters 参数取值不同, 则会看到 trace 应用输出你指定的 iters 值. 还有个有意思的事情, 我们真的可以通过 GDB 看到针对二进制文件的修改. 下面我们 dump 出 0x0x6600e0 处的指令, 在我们运行 trace 之前是这样的: $ gdb ./app (gdb) display /4i 0x6600e0 1: x/4i 0x6600e0 0x6600e0 \u003cmain.computeE\u003e: sub $0x20,%rsp 0x6600e4 \u003cmain.computeE+4\u003e: mov %rbp,0x18(%rsp) 0x6600e9 \u003cmain.computeE+9\u003e: lea 0x18(%rsp),%rbp 0x6600ee \u003cmain.computeE+14\u003e: xorps %xmm0,%xmm0 在我们运行 trace 之后, 再次查看: $ gdb ./app (gdb) display /4i 0x65fecf 2: x/4i 0x6600e0 0x6600e0 \u003cmain.computeE\u003e: int3 0x6600e1 \u003cmain.computeE+1\u003e: sub $0x20,%esp 0x6600e4 \u003cmain.computeE+4\u003e: mov %rbp,0x18(%rsp) 0x6600e9 \u003cmain.computeE+9\u003e: lea 0x18(%rsp),%rbp 看到了吗? 0x6600e0 插入了 int3 指令. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:0:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#构建追踪程序"},{"categories":null,"content":"番外下面说一下实验过程遇到的问题以及解决办法. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:0:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#番外"},{"categories":null,"content":"安装 BCC编译前文提到的 trace 应用之前需要安装 bcc. 以 Ubuntu 16.04 为例(其它系统请参考这里): sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD echo \"deb https://repo.iovisor.org/apt/$(lsb_release -cs)$(lsb_release -cs)main\" | sudo tee /etc/apt/sources.list.d/iovisor.list sudo apt-get update sudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r) 如果安装速度慢, 而且你设置了 http_proxy/https_proxy, 请编辑 /etc/sudoers 新增一行 Defaults env_keep = \"http_proxy https_proxy\", 这样速度至少会有百倍提升. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:1:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#安装-bcc"},{"categories":null,"content":"too many arguments 编译错误 # github.com/iovisor/gobpf/bcc ../../../../go/pkg/mod/github.com/iovisor/gobpf@v0.0.0-20200614202714-e6b321d32103/bcc/module.go:98:40: too many arguments in call to _Cfunc_bpf_module_create_c_from_string have (*_Ctype_char, number, **_Ctype_char, _Ctype_int, _Ctype__Bool, nil) want (*_Ctype_char, _Ctype_uint, **_Ctype_char, _Ctype_int, _Ctype__Bool) ../../../../go/pkg/mod/github.com/iovisor/gobpf@v0.0.0-20200614202714-e6b321d32103/bcc/module.go:230:28: too many arguments in call to _C2func_bcc_func_load have (unsafe.Pointer, _Ctype_int, *_Ctype_char, *_Ctype_struct_bpf_insn, _Ctype_int, *_Ctype_char, _Ctype_uint, _Ctype_int, *_Ctype_char, _Ctype_uint, nil) want (unsafe.Pointer, _Ctype_int, *_Ctype_char, *_Ctype_struct_bpf_insn, _Ctype_int, *_Ctype_char, _Ctype_uint, _Ctype_int, *_Ctype_char, _Ctype_uint) 原因为这一行增加的特性 Update bcc_func_load to libbcc 0.11 with hardware offload support, 以及这一行增加的特性 bcc: update bpf_module_create_c_from_string for bcc 0.11.0 (fixes #202). 我没有深究具体是什么导致的(初步怀疑是系统版本), 如果你急着看结果, 可以根据上面报错地址知道到 module.go 文件, 把涉及的两个函数的最后一个 nil 参数去掉就可以顺利编译了. –End– ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:2:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#too-many-arguments-编译错误"},{"categories":null,"content":"从哪里着手分析 leveldb 实现在了解了其基本使用以后, 如果想理解 leveldb 基本原理, 则有两个抓手. 第一个是 include 目录下的头文件, 尤其是 db.h , 第二个就是它的文件类型及其格式. 下面我们就从接口和文件两个方向来切入 leveldb 的设计与实现. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#从哪里着手分析-leveldb-实现"},{"categories":null,"content":"leveldb 常用的接口","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#leveldb-常用的接口"},{"categories":null,"content":"Open /** * 打开一个名为 name 的数据库. * * 打开成功, 会把一个指向基于堆内存的数据库指针存储到 *dbptr, 同时返回 OK; 如果打开失败, * 存储 nullptr 到 *dbptr 同时返回一个错误状态. * * 调用者不再使用这个数据库时需要负责释放 *dbptr 指向的内存. * * @param options 控制数据库行为和性能的参数配置 * @param name 数据库名称 * @param dbptr 存储指向堆内存中数据库的指针 * @return */ static Status Open(const Options\u0026 options, const std::string\u0026 name, DB** dbptr); 该方法在数据库启动时调用, 主要工作由 leveldb::DBImpl::Recover 方法完成, 后者主要做如下事情: 调用其 VersionSet 成员的 leveldb::VersionSet::Recover 方法. 该方法从磁盘读取 CURRENT 文件, 进而读取 MANIFEST 文件内容, 然后在内存重建 level 架构: 读取 CURRENT 文件(不存在则新建)找到最新的 MANIFEST 文件(不存在则新建)的名称 读取该 MANIFEST 文件内容与当前 Version 保存的 level 架构合并保存到一个新建的 Version 中, 然后将这个新的 version 作为当前的 version, 即最新的 level 架构信息. 清理过期的文件 这一步我们可以打开全部 sstables, 但最好等会再打开 将 log 文件块转换为一个新的 level-0 sstable 将接下来的要写的数据写入一个新的 log 文件 遍历数据库目录下全部文件. 筛选出 sorted string table 文件, 验证 VersionSet 包含的 level 架构图有效性; 同时将全部 log 文件筛选换出来后续反序列化成 memtable. 恢复 log 文件时会按照从旧到新逐个 log 文件恢复, 这样新的修改会覆盖旧的, 如果对应 memtable 太大了, 将其转为 sorted string table 文件写入磁盘, 同时将其对应的 table 对象放到 table_cache_ 缓存. 若发生 memtable 落盘表示 level 架构新增文件则将 save_manifest 标记为 true, 表示需要写变更日志到 manifest 文件. 恢复 log 文件主要由方法 leveldb::DBImpl::RecoverLogFile 负责完成. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#open"},{"categories":null,"content":"Put /** * 将 \u003ckey, value\u003e 对写入数据库, 成功返回 OK, 失败返回错误状态. * @param options 本次写操作相关的配置参数, 如果有需要可以将该参数中的 sync 置为 true, 不容易丢数据但更慢. * @param key Slice 类型的 key * @param value Slice 类型的 value * @return 返回类型为 Status */ virtual Status Put(const WriteOptions\u0026 options, const Slice\u0026 key, const Slice\u0026 value) = 0; 该方法主要依赖 leveldb::DBImpl::Write 实现. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#put"},{"categories":null,"content":"Delete /** * 从数据删除指定键为 key 的键值对. 如果 key 不存在不算错. * * @param options 本次写操作相关的配置参数, 如果有需要可以将该参数中的 sync 置为 true, 不容易丢数据但更慢. * @param key 要删除数据项对应的 key * @return */ virtual Status Delete(const WriteOptions\u0026 options, const Slice\u0026 key) = 0; 该方法主要依赖 leveldb::DBImpl::Write 实现. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#delete"},{"categories":null,"content":"Write /** * 对数据库进行批量更新写操作. * * 该方法线程安全, 内部自带同步. * * @param options 本次写操作相关的配置参数, 如果有需要可以将该参数中的 sync 置为 true, 不容易丢数据但更慢. * @param updates 要进行的批量更新操作 * @return */ virtual Status Write(const WriteOptions\u0026 options, WriteBatch* updates) = 0; 该方法是 leveldb::DBImpl::Write 原型. 针对调用 db 进行的写操作, 都会生成一个对应的 struct leveldb::DBImpl::Writer, 其封装了写入数据和写入进度. 新构造的 writer 会被放入一个队列. 循环检查, 若当前 writer 工作没完成并且不是队首元素, 则当前有其它 writer 在写, 挂起当前 writer 等待条件成熟. 当前 writer 如果被排在前面的 writer 给合并写入了, 那么它的 done 就被标记为完成了. 否则会被其它在写入的 writer 调用其 signal 将其唤醒执行写入工作. 当执行写入工作时(被前一个执行写入并完成工作的 writer 唤醒了), 首先确认是否为本次该 writer 写操作分配新的 log 文件, 如果需要则分配. 因为该 writer 成为队首 writer 了, 则它负责将队列前面若干 writers 的 batch 合并为一个(该工作由leveldb::DBImpl::BuildBatchGroup 负责完成), 注意, 被合并的 writers 不出队(待合并写入完成再出队, 具体见后面描述), 所以写 log 期间队首 writer 不变. 具体写入工作由 leveldb::log::Writer::AddRecord 负责, 就是将数据序列化为 record 写入 log 文件. 如果追加 log 文件成功,则将被追加的数据插入到内存中的 memtable 中. 待写入完毕, 该 writer 将参与前述 batch group 写入 log 文件的 writer 都取出来并设置为写入完成, 即将其出队, 将其 done 置为 true, 同时向其发送信号将其唤醒, 被唤醒后它会检查其 done 标识并返回. 最后唤醒队首 writer 执行下一个合并写入. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#write"},{"categories":null,"content":"Get /** * 查询键为 key 的数据项, 如果存在则将对应的 value 地址存储到第二个参数中. * * 如果 key 不存在, 第二个参数不变, 返回值为 IsNotFound Status. * * @param options 本次读操作对应的配置参数 * @param key 要查询的 key, Slice 引用类型 * @param value 存储与 key 对应的值的指针, string 指针类型 * @return */ virtual Status Get(const ReadOptions\u0026 options, const Slice\u0026 key, std::string* value) = 0; 先查询当前在用的 memtable(具体工作由 leveldb::MemTable::Get 负责, 本质就是 SkipList 查询, 速度很快) 如果没有则查询正在转换为 sorted string table 的 memtable 中寻找 如果没有则我们在磁盘上采用从底向上 level-by-level 的寻找目标 key. 针对上述第 3 步, 具体由 db VersionSet 的当前 Version 负责, 因为该结构保存了 db 当前最新的 level 架构信息, 即每个 level 及其对应的文件列表和每个文件的键范围. 对应方法为 leveldb::Version::Get, 具体为: 从低 level 向高 level 寻找. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. 另外需要注意的的是, 参数 options 可以配置一个快照, 快照对应了数据库历史上的一个操作序列号, 查询时仅查询不大于该序列号的操作范围. 针对同样的 key, 如果历史上有多次更新操作, 而用户想查找特定更新, 这就是实现途径. 如果没有配置快照选项, 默认采用当前最大序列号进行查询. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#get"},{"categories":null,"content":"NewIterator /** * 返回基于堆内存的迭代器, 可以用该迭代器遍历整个数据库的内容. * 该函数返回的迭代器初始是无效的(在使用迭代器之前, 调用者必须在其上调用 Seek 方法). * * 当不再使用时, 调用者应该释放该迭代器对应的内存, 而且迭代器必须在数据库释放之前进行释放. * @param options 本次读操作对应的配置参数 * @return */ virtual Iterator* NewIterator(const ReadOptions\u0026 options) = 0; 该方法负责将内存 memtable(可能有两个, 一个在写, 一个写完待存盘) 和磁盘 sorted string table 文件全部数据结构串起来构造一个大一统迭代器, 可以遍历整个数据库. 上述工作其实是由 leveldb::Iterator *leveldb::DBImpl::NewInternalIterator 负责完成的. 该方法实现涉及到 leveldb 特别精巧的迭代器的实现. 这个单独可以写一篇文章来专门介绍. 这里大致说下处理流程: 1 初始化一个列表 2 把当前 memtable 迭代器加入列表中 3 把待写盘 memtable 迭代器追加到列表中 4 将当前 version 维护的 level 架构中每个 sorted string table 文件对应的迭代器追加到列表中. 针对 level-0 和其它 levels 处理方式不同. 由于 level-0 文件之间可能存在重叠, 所以按照文件生成顺序(这极其重要, 其实就是按照 key 从小到大, 只有这样才能确保最后生成的迭代器能够从小到大按序遍历整个数据库) 为每个文件生成一个两级迭代器(TwoLevelIterator, 该结构巧妙地将索引块和数据块结合到了一起)追加到列表中. 针对 level-1 及其以上 level, 按照从低 level 到高 level(这极其重要, 原因同 level-0), 为每个 level 生成一个两级迭代器, 数据结构依然是 TwoLevelIterator, 不过这里把每个 level 的文件列表抽象成了第一级索引, 然后每个文件对应的 table 对象抽象层二级索引. 最后将前述全部迭代器构成的迭代器列表再级联成一个大一统的迭代器 MergingIterator. 这其实也是一个两级迭代器, 第一级指向迭代器列表, 第二级是某个迭代器指向的内容的迭代器. 最后返回给调用者的就是 MergingIterator 实例. 可以调用它的相关方法在整个数据库上寻找目标 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:6","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#newiterator"},{"categories":null,"content":"GetSnapshot /** * 返回当前 DB 状态的一个快照. * 使用该快照创建的全部迭代器将会都指向一个当前 DB 的一个稳定快照. * * 当不再使用该快照时, 调用者必须调用 ReleaseSnapshot 将其释放. * @return */ virtual const Snapshot* GetSnapshot() = 0; 用数据库当前最新的更新操作对应的序列号创建一个快照. 快照最核心的就是那个操作序列号, 因为查询时会把 用户提供的 key(我们叫做 user_key)和操作序列号一起构成一个 internal_key(数据库存储的 key 就是它), 针对 user_key 相等的情况比如针对 hello 这个 user_key Put 多次, 则每次序列号就不一样, 于是根据特定序列号可以查询到特定的那次 Put 写入的 value 值. 这个新生成的快照会被挂载到一个双向链表上, 用完后可以调用 ReleaseSnapshot 将其释放掉. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:7","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#getsnapshot"},{"categories":null,"content":"ReleaseSnapshot /** * 释放一个之前获取的快照, 释放后, 调用者不能再使用该快照了. * @param snapshot 指向要释放的快照的指针 */ virtual void ReleaseSnapshot(const Snapshot* snapshot) = 0; 从双向链表上删除指定的快照. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:8","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#releasesnapshot"},{"categories":null,"content":"GetProperty /** * DB 实现可以通过该方法导出自身状态相关的信息. 如果提供的属性可以被 DB 实现理解, * 那么第二个参数将会存储该属性对应的当前值同时该方法返回 true, 其它情况该方法返回 false. * * 合法的属性名称包括: * * \"leveldb.num-files-at-level\u003cN\u003e\" - 返回 level \u003cN\u003e 的文件个数, 其中 \u003cN\u003e 是一个数字. * * \"leveldb.stats\" - 返回多行字符串, 描述该 DB 内部操作相关的统计数据. * * \"leveldb.sstables\" - 返回多行字符串, 描述构成该 DB 的全部 sstable 相关信息. * * \"leveldb.approximate-memory-usage\" - 返回被该 DB 使用的内存字节数近似值 * @param property 要查询的属性名称 * @param value 保存属性名称对应的属性值 * @return */ virtual bool GetProperty(const Slice\u0026 property, std::string* value) = 0; leveldb 实现在内部做了一些统计, 可以通过这个接口进行查询. 不过目前可查询状态不多, 具体如下: “leveldb.num-files-at-level” - 返回 level 的文件个数, 其中 是一个 ASCII 格式的数字. “leveldb.stats” - 返回多行字符串, 描述该 DB 内部操作相关的统计数据. “leveldb.sstables” - 返回多行字符串, 描述构成该 DB 的全部 sstable 相关信息. “leveldb.approximate-memory-usage” - 返回被该 DB 使用的内存字节数近似值 ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:9","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#getproperty"},{"categories":null,"content":"GetAppoximateSizes /** * 对于 [0, n-1] 中每个 i, 将位于 [range[i].start .. range[i].limit) * 中全部 keys 所占用文件系统空间近似大小存储到 sizes[i] 中. * * 注意, 如果数据被压缩过了, 那么返回的 sizes 存储的就是压缩后数据所占用文件系统空间大小. * * 返回结果可能不包含最近刚写入的数据所占用空间. * @param range 指定要查询一组 keys 范围 * @param n range 和 sizes 两个数组的大小 * @param sizes 存储查询到的每个 range 对应的文件系统空间近似大小 */ virtual void GetApproximateSizes(const Range* range, int n, uint64_t* sizes) = 0; 计算 range 包含的键区间在磁盘上占用的空间大小, 每个子区间占用会保存到 sizes 对应位置. 计算过程也很简单, 就是遍历 range 列表, 针对每个子区间起止 key, 去数据库中确认其大致字节偏移, 然后\"止\"-“始” 即为子区间占用空间的大致大小. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:10","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#getappoximatesizes"},{"categories":null,"content":"CompactRange /** * 将 key 范围 [*begin,*end] 对应的底层存储压紧, 注意范围是左闭右闭. * * 尤其是, 压实过程会将已经删除或者复写过的数据会被丢弃, 同时会将数据 * 重新安放以减少后续数据访问操作的成本. * 这个操作是为那些理解底层实现的用户准备的. * * 如果 begin==nullptr, 则从第一个键开始; 如果 end==nullptr 则到最后一个键为止. * 所以, 如果像下面这样做则意味着压紧整个数据库: * * db-\u003eCompactRange(nullptr, nullptr); * @param begin 起始键 * @param end 截止键 */ virtual void CompactRange(const Slice* begin, const Slice* end) = 0; 手动触发与目标键区间重叠的文件压实. 具体为: 检查每个 level, 确认其包含的键区间释放与目标键区间有交集. 因为当前在写 memtable 可能与目标键区间有交集, 所以强制触发一次 memtable 压实(即将当前 memtable 文件转为 sorted string table 文件并写入磁盘)并生成新 log 文件和对应的 memtable. 针对与目标键区间有交集的各个 level 触发一次手动压实 具体压实过程后续会写一篇文章进行介绍. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:11","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#compactrange"},{"categories":null,"content":"leveldb 的文件类型下面分别介绍 leveldb 最重要的几个文件类型. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#leveldb-的文件类型"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件格式"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件格式的好处"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件的缺点并不是"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件主要接口"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#写-log"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#读-log"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#与-log-文件配套的-memtable"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#结构"},{"categories":null,"content":"log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#用途"},{"categories":null,"content":"sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L \u003e= 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: \u003cbeginning_of_file\u003e [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) \u003cend_of_file\u003e 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter.\u003cName\u003e 到 filter block 的 BlockHandle 的映射. 其中, \u003cName\u003e 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [fi","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件"},{"categories":null,"content":"sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L = 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [fi","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件格式"},{"categories":null,"content":"sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L = 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [fi","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#filter-meta-block"},{"categories":null,"content":"sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L = 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [fi","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#stats-meta-block"},{"categories":null,"content":"sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L = 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [fi","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件主要接口"},{"categories":null,"content":"sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L = 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [fi","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件读接口"},{"categories":null,"content":"sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L = 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [fi","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件写接口"},{"categories":null,"content":"MANIFEST 文件MANIFEST 文件可以看作 leveldb 存储元数据的地方. 它列出了每一个 level 及其包含的全部 sorted string table 文件, 每个 sorted string table 文件对应的键区间, 以及其它重要的元数据. 每当重新打开数据库的时候, 就会创建一个新的 MANIFEST 文件(文件名中嵌有一个新生成的数字). MANIFEST 文件被格式化成形同 log 文件的格式, 针对它所服务的数据的变更都会被追加到该文件后面. 比如每当某个 level 发生文件新增或者删除操作时, 就会有一条日志被追加到 MANIFEST 中. MANIFEST 文件在实现时又叫 descriptor 文件, 文件格式同 log 文件, 所以写入/读取方法就复用了. 其每条日志就是一个序列化后的 leveldb::VersionEdit. 每次针对 level 架构有文件增删时都要写日志到 manifest 文件. 与 MANIFEST 相关的数据结构之 VersionSet每个 db 都有一个 class leveldb::VersionSet 实例, 它保存了 db 当前的 level 架构视图(具体存储结构为其 Version 成员). MANIFEST 文件可以看作是它所维护的信息的反映. 它的重要方法有: VersionSet::Recover 负责在打开数据库时将 MANIFEST 文件反序列化构造 level 架构视图, 这个过程会依赖 VersionEdit 类. VersionSet::LogAndApply 负责将当前 VersionEdit 和当前 Version 进行合并, 然后序列化为一条日志记录到 MANIFEST 文件. 最后把新的 version 替换当前 version. 与 MANIFEST 相关的 Version 结构class leveldb::Version 是 leveldb 数据库 level 架构的内存表示, 它存储了每一个 level 及其全部的文件信息(文件名, 键范围等等). 每次调用 db 的 Get 方法在 memtable 找不到目标 key 时就会到各个 level 的文件去搜寻, 这个搜寻过程所依赖的就是数据库 VersionSet(下面介绍) 保存的当前 Version 存储的 level 架构信息进行的, 具体实现见 leveldb::Version::Get 方法. 当条件满足时, VersionSet 会将当前 Version 和当前 VesionEdit 合并生成一个新的 Version 替换当前 Version. 与 MANIFEST 相关的 VersionEditMANIFEST 文件的每一条日志就是一个序列化的 class leveldb::VersionEdit. 它可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#manifest-文件"},{"categories":null,"content":"MANIFEST 文件MANIFEST 文件可以看作 leveldb 存储元数据的地方. 它列出了每一个 level 及其包含的全部 sorted string table 文件, 每个 sorted string table 文件对应的键区间, 以及其它重要的元数据. 每当重新打开数据库的时候, 就会创建一个新的 MANIFEST 文件(文件名中嵌有一个新生成的数字). MANIFEST 文件被格式化成形同 log 文件的格式, 针对它所服务的数据的变更都会被追加到该文件后面. 比如每当某个 level 发生文件新增或者删除操作时, 就会有一条日志被追加到 MANIFEST 中. MANIFEST 文件在实现时又叫 descriptor 文件, 文件格式同 log 文件, 所以写入/读取方法就复用了. 其每条日志就是一个序列化后的 leveldb::VersionEdit. 每次针对 level 架构有文件增删时都要写日志到 manifest 文件. 与 MANIFEST 相关的数据结构之 VersionSet每个 db 都有一个 class leveldb::VersionSet 实例, 它保存了 db 当前的 level 架构视图(具体存储结构为其 Version 成员). MANIFEST 文件可以看作是它所维护的信息的反映. 它的重要方法有: VersionSet::Recover 负责在打开数据库时将 MANIFEST 文件反序列化构造 level 架构视图, 这个过程会依赖 VersionEdit 类. VersionSet::LogAndApply 负责将当前 VersionEdit 和当前 Version 进行合并, 然后序列化为一条日志记录到 MANIFEST 文件. 最后把新的 version 替换当前 version. 与 MANIFEST 相关的 Version 结构class leveldb::Version 是 leveldb 数据库 level 架构的内存表示, 它存储了每一个 level 及其全部的文件信息(文件名, 键范围等等). 每次调用 db 的 Get 方法在 memtable 找不到目标 key 时就会到各个 level 的文件去搜寻, 这个搜寻过程所依赖的就是数据库 VersionSet(下面介绍) 保存的当前 Version 存储的 level 架构信息进行的, 具体实现见 leveldb::Version::Get 方法. 当条件满足时, VersionSet 会将当前 Version 和当前 VesionEdit 合并生成一个新的 Version 替换当前 Version. 与 MANIFEST 相关的 VersionEditMANIFEST 文件的每一条日志就是一个序列化的 class leveldb::VersionEdit. 它可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#与-manifest-相关的数据结构之-versionset"},{"categories":null,"content":"MANIFEST 文件MANIFEST 文件可以看作 leveldb 存储元数据的地方. 它列出了每一个 level 及其包含的全部 sorted string table 文件, 每个 sorted string table 文件对应的键区间, 以及其它重要的元数据. 每当重新打开数据库的时候, 就会创建一个新的 MANIFEST 文件(文件名中嵌有一个新生成的数字). MANIFEST 文件被格式化成形同 log 文件的格式, 针对它所服务的数据的变更都会被追加到该文件后面. 比如每当某个 level 发生文件新增或者删除操作时, 就会有一条日志被追加到 MANIFEST 中. MANIFEST 文件在实现时又叫 descriptor 文件, 文件格式同 log 文件, 所以写入/读取方法就复用了. 其每条日志就是一个序列化后的 leveldb::VersionEdit. 每次针对 level 架构有文件增删时都要写日志到 manifest 文件. 与 MANIFEST 相关的数据结构之 VersionSet每个 db 都有一个 class leveldb::VersionSet 实例, 它保存了 db 当前的 level 架构视图(具体存储结构为其 Version 成员). MANIFEST 文件可以看作是它所维护的信息的反映. 它的重要方法有: VersionSet::Recover 负责在打开数据库时将 MANIFEST 文件反序列化构造 level 架构视图, 这个过程会依赖 VersionEdit 类. VersionSet::LogAndApply 负责将当前 VersionEdit 和当前 Version 进行合并, 然后序列化为一条日志记录到 MANIFEST 文件. 最后把新的 version 替换当前 version. 与 MANIFEST 相关的 Version 结构class leveldb::Version 是 leveldb 数据库 level 架构的内存表示, 它存储了每一个 level 及其全部的文件信息(文件名, 键范围等等). 每次调用 db 的 Get 方法在 memtable 找不到目标 key 时就会到各个 level 的文件去搜寻, 这个搜寻过程所依赖的就是数据库 VersionSet(下面介绍) 保存的当前 Version 存储的 level 架构信息进行的, 具体实现见 leveldb::Version::Get 方法. 当条件满足时, VersionSet 会将当前 Version 和当前 VesionEdit 合并生成一个新的 Version 替换当前 Version. 与 MANIFEST 相关的 VersionEditMANIFEST 文件的每一条日志就是一个序列化的 class leveldb::VersionEdit. 它可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#与-manifest-相关的-version-结构"},{"categories":null,"content":"MANIFEST 文件MANIFEST 文件可以看作 leveldb 存储元数据的地方. 它列出了每一个 level 及其包含的全部 sorted string table 文件, 每个 sorted string table 文件对应的键区间, 以及其它重要的元数据. 每当重新打开数据库的时候, 就会创建一个新的 MANIFEST 文件(文件名中嵌有一个新生成的数字). MANIFEST 文件被格式化成形同 log 文件的格式, 针对它所服务的数据的变更都会被追加到该文件后面. 比如每当某个 level 发生文件新增或者删除操作时, 就会有一条日志被追加到 MANIFEST 中. MANIFEST 文件在实现时又叫 descriptor 文件, 文件格式同 log 文件, 所以写入/读取方法就复用了. 其每条日志就是一个序列化后的 leveldb::VersionEdit. 每次针对 level 架构有文件增删时都要写日志到 manifest 文件. 与 MANIFEST 相关的数据结构之 VersionSet每个 db 都有一个 class leveldb::VersionSet 实例, 它保存了 db 当前的 level 架构视图(具体存储结构为其 Version 成员). MANIFEST 文件可以看作是它所维护的信息的反映. 它的重要方法有: VersionSet::Recover 负责在打开数据库时将 MANIFEST 文件反序列化构造 level 架构视图, 这个过程会依赖 VersionEdit 类. VersionSet::LogAndApply 负责将当前 VersionEdit 和当前 Version 进行合并, 然后序列化为一条日志记录到 MANIFEST 文件. 最后把新的 version 替换当前 version. 与 MANIFEST 相关的 Version 结构class leveldb::Version 是 leveldb 数据库 level 架构的内存表示, 它存储了每一个 level 及其全部的文件信息(文件名, 键范围等等). 每次调用 db 的 Get 方法在 memtable 找不到目标 key 时就会到各个 level 的文件去搜寻, 这个搜寻过程所依赖的就是数据库 VersionSet(下面介绍) 保存的当前 Version 存储的 level 架构信息进行的, 具体实现见 leveldb::Version::Get 方法. 当条件满足时, VersionSet 会将当前 Version 和当前 VesionEdit 合并生成一个新的 Version 替换当前 Version. 与 MANIFEST 相关的 VersionEditMANIFEST 文件的每一条日志就是一个序列化的 class leveldb::VersionEdit. 它可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#与-manifest-相关的-versionedit"},{"categories":null,"content":"CURRENT 文件CURRENT 文件是一个简单的文本文件. 由于每次重新打开数据库都会生成一个 MANIFEST 文件, 所以需要一个地方记录最新的 MANIFEST 文件是哪个, CURRENT 就干这个事情, 它相当于一个指针, 其内容即是当前最新的 MANIFEST 文件的名称. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#current-文件"},{"categories":null,"content":"文件位置与命名各类型文件位置与命名如下: dbname/CURRENT dbname/LOCK dbname/LOG dbname/LOG.old dbname/MANIFEST-[0-9]+ dbname/[0-9]+.(log|sst|ldb) 其中 dbname 为用户指定. –End– ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#文件位置与命名"},{"categories":null,"content":"Leveldb 是一个高速 KV 数据库, 它提供了一个持久性的 KV 存储. 其中 keys 和 values 都是随机字节数组, 并且存储时根据用户指定的比较函数对 keys 进行排序. 它由 Google 开发的, 其作者为大名鼎鼎的 Sanjay Ghemawat (sanjay@google.com) 和 Jeff Dean (jeff@google.com). 感谢他们对人类的贡献. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#"},{"categories":null,"content":"基本介绍该部分主要介绍 leveldb 的功能, 局限性以及性能等. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#基本介绍"},{"categories":null,"content":"特性 keys 和 values 都可以是随机的字节数组. 数据被按照 key 的顺序进行存储. 调用者可以提供一个定制的比较函数来覆盖默认的比较器. 基础操作有 Put(key,value), Get(key), Delete(key). 多个更改可以在一个原子批处理中一起生效. 用户可以创建一个瞬时快照来获取数据的一致性视图. 支持针对数据的前向和后向遍历. 数据通过 Snappy 压缩程序库自动压缩. 与外部交互的操作都被抽象成了接口(如文件系统操作等), 所以用户可以根据接口定制自己期望的操作系统交互行为. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#特性"},{"categories":null,"content":"局限性 LevelDB 不是 SQL 数据库. 它没有关系数据模型, 不支持 SQL 查询, 也不支持索引. 同时只能有一个进程(可能是具有多线程的进程)访问一个特定的数据库. 该程序库没有内置基于网络的 CS 架构, 有需求的用户可以自己封装. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#局限性"},{"categories":null,"content":"性能下面是通过运行 db_bench 程序得出的性能测试报告. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#性能"},{"categories":null,"content":"测试配置我们使用的是一个包含一百万数据项的数据库, 其中 key 是 16 字节, value 是 100 字节, value 压缩后大约是原来的一半, 测试配置如下: LevelDB: version 1.1 Date: Sun May 1 12:11:26 2011 CPU: 4 x Intel(R) Core(TM)2 Quad CPU Q6600 @ 2.40GHz CPUCache: 4096 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 Raw Size: 110.6 MB (estimated) File Size: 62.9 MB (estimated) ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#测试配置"},{"categories":null,"content":"写性能“fill” 基准测试创建了一个全新的数据库, 以顺序(下面 “seq” 结尾者)或者随机(下面 “random” 结尾者)方式写入. “fillsync” 基准测试每次写操作都将数据从操作系统刷到磁盘; 其它的操作会将数据保存在系统中一段时间. “overwrite” 基准测试做随机写, 这些操作会更新数据库中已有的键. fillseq : 1.765 micros/op; 62.7 MB/s fillsync : 268.409 micros/op; 0.4 MB/s (10000 ops) fillrandom : 2.460 micros/op; 45.0 MB/s overwrite : 2.380 micros/op; 46.5 MB/s 上述每个 “op” 对应一个 key/value 对的写操作. 也就是说, 一个随机写基准测试每秒大约进行四十万次写操作(1,000,000/2.46). 每个 “fillsync” 操作时间消耗(大约 0.3 毫秒)少于一次磁盘寻道(大约 10 毫秒). 我们怀疑这是因为磁盘本身将更新操作缓存到了内存, 并且在数据真正落盘前返回响应. 该方式是否安全取决于断电后磁盘是否有备用电力将数据落盘. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#写性能"},{"categories":null,"content":"读性能我们分别给出正向顺序读、反向顺序读的性能以及随机查询的性能指标. 注意, 基准测试创建的数据库很小. 因此该性能报告描述的是 leveldb 的全部数据集能放入到内存的场景. 如果数据不在操作系统缓存中, 读取一点数据的性能消耗主要在于一到两次的磁盘寻道. 写性能基本不会受数据集是否能放入内存的影响. readrandom : 16.677 micros/op; (approximately 60,000 reads per second) readseq : 0.476 micros/op; 232.3 MB/s readreverse : 0.724 micros/op; 152.9 MB/s LevelDB 会在后台压实底层的数据来改善读性能. 上面列出的结果是在经过一系列随机写操作后得出的. 如果经过压实(通常是自动触发), 那么上述指标会更好. readrandom : 11.602 micros/op; (approximately 85,000 reads per second) readseq : 0.423 micros/op; 261.8 MB/s readreverse : 0.663 micros/op; 166.9 MB/s 读操作消耗高的地方有一些来自重复解压从磁盘读取的数据块. 如果我们能提供足够的缓存给 leveldb 来将解压后的数据保存在内存中, 读性能会进一步改善: readrandom : 9.775 micros/op; (approximately 100,000 reads per second before compaction) readrandom : 5.215 micros/op; (approximately 190,000 reads per second after compaction) ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#读性能"},{"categories":null,"content":"使用举例下面从构建和头文件介绍开始, 对 leveldb 的基本使用进行介绍. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#使用举例"},{"categories":null,"content":"构建该工程开箱支持 CMake. 所以构建起来超简单: $ mkdir -p build \u0026\u0026 cd build $ cmake -DCMAKE_BUILD_TYPE=Release .. \u0026\u0026 cmake --build . $ sudo make install 更多高级用法请请参照 CMake 文档和本项目的 CMakeLists.txt. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#构建"},{"categories":null,"content":"头文件介绍LevelDB 对外的接口都包含在 include/*.h 中. 除了该目录下的文件, 用户不应该依赖其它目录下任何文件. include/db.h: 主要的接口在这, 使用 leveldb 从这里开始. include/options.h: 使用 leveldb 过程各种操作包括读写有关的控制参数. include/comparator.h: 比较函数的抽象, 如果你想用逐字节比较 key 那么可以直接使用默认的比较器. 如果你想定制排序逻辑(如处理不同的字符编解码等)可以定制自己的比较函数. include/iterator.h: 迭代数据的接口. 你可以从一个 DB 对象获取到一个迭代器. include/write_batch.h: 原子地应用多个更新到一个数据库. include/slice.h: 类似 string, 维护着指向字节数组的一个指针和相应长度. include/status.h: 许多公共接口都会返回 Status, 用于指示成功或其它各种错误. include/env.h: 操作系统环境的抽象. 在 util/env_posix.cc 中有一个该接口的 posix 实现. include/table.h, include/table_builder.h: 底层的模块, 大多数客户端可能不会直接用到. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#头文件介绍"},{"categories":null,"content":"打开(或新建)一个数据库leveldb 数据库都有一个名字, 该名字对应了文件系统上一个目录, 而且该数据库内容全都存在该目录下. 下面的例子显示了如何打开一个数据库以及在必要情况下创建之. #include \u003ccassert\u003e#include \"leveldb/db.h\" leveldb::DB* db; leveldb::Options options; options.create_if_missing = true; leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); assert(status.ok()); ... 如果你想在数据库已存在的时候触发一个异常, 将下面这行加到 leveldb::DB::Open 调用之前: options.error_if_exists = true; ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#打开或新建一个数据库"},{"categories":null,"content":"Status 类型你可能注意到上面的 leveldb::Status 类型了. leveldb 中大部分方法在遇到错误的时候会返回该类型的值. 你可以检查它是否为 ok, 然后打印关联的错误信息即可: leveldb::Status s = ...; if (!s.ok()) cerr \u003c\u003c s.ToString() \u003c\u003c endl; ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#status-类型"},{"categories":null,"content":"关闭数据库当数据库不再使用的时候, 像下面这样直接删除数据库对象就可以了: ... open the db as described above ... ... do something with db ... delete db; 非常简单是不是? 因为 DB 类的实现是基于 RAII 的, 在 delete 时触发析构方法自动进行清理工作. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:5:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#关闭数据库"},{"categories":null,"content":"数据库读写数据库提供了 Put、Delete 以及 Get 方法来修改、查询数据库. 下面的代码展示了将 key1 对应的 value 移动(先拷贝后删除)到 key2 下. std::string value; leveldb::Status s = db-\u003eGet(leveldb::ReadOptions(), key1, \u0026value); if (s.ok()) s = db-\u003ePut(leveldb::WriteOptions(), key2, value); if (s.ok()) s = db-\u003eDelete(leveldb::WriteOptions(), key1); ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:6:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#数据库读写"},{"categories":null,"content":"原子更新注意, 上一小节中如果进程在 Put 了 key2 之后但是删除 key1 之前挂了, 那么同样的 value 就出现在了多个 keys 之下. 该问题可以通过使用 WriteBatch 类原子地应用一组操作来避免. #include \"leveldb/write_batch.h\"... std::string value; leveldb::Status s = db-\u003eGet(leveldb::ReadOptions(), key1, \u0026value); if (s.ok()) { leveldb::WriteBatch batch; batch.Delete(key1); batch.Put(key2, value); s = db-\u003eWrite(leveldb::WriteOptions(), \u0026batch); } WriteBatch 保存着一系列将被应用到数据库的编辑操作, 这些操作会按照添加的顺序依次被执行. 注意, 我们先执行 Delete 后执行 Put, 这样如果 key1 和 key2 一样的情况下我们也不会错误地丢失数据. 除了原子性, WriteBatch 也能加速更新过程, 因为可以把一大批独立的操作添加到同一个 batch 中然后一次性执行. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:7:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#原子更新"},{"categories":null,"content":"同步写操作默认地, leveldb 每个写操作都是异步的: 进程把要写的内容 push 给操作系统后立马返回. 从操作系统内存到底层持久性存储的迁移异步地发生. 当然, 也可以把某个写操作的 sync 标识打开, 以等到数据真正被记录到持久化存储再让写操作返回. (在 Posix 系统上, 这是通过在写操作返回前调用 fsync(...) 或 fdatasync(...) 或 msync(..., MS_SYNC) 来实现的. ) leveldb::WriteOptions write_options; write_options.sync = true; db-\u003ePut(write_options, ...); 异步写通常比同步写快 1000 倍. 异步写的缺点是, 一旦机器崩溃可能会导致最后几个更新操作丢失. 注意, 仅仅是写进程崩溃(而非机器重启)则不会引起任何更新操作丢失, 因为哪怕 sync 标识为 false, 在进程退出之前写操作也已经从进程内存 push 到了操作系统. 异步写总是可以安全使用. 比如你要将大量的数据写入数据库, 如果丢失了最后几个更新操作, 你可以重启整个写过程. 如果数据量非常大, 一个优化点是, 每进行 N 个异步写操作则进行一次同步地写操作, 如果期间发生了崩溃, 重启自从上一个成功的同步写操作以来的更新操作即可. (同步的写操作可以同时更新一个标识, 该标识用于描述崩溃重启后从何处开始重启更新操作. ) WriteBatch 可以作为异步写操作的替代品. 多个更新操作可以放到同一个 WriteBatch 中然后通过一次同步写(即 write_options.sync 置为 true)一起应用. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:8:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#同步写操作"},{"categories":null,"content":"并发一个数据库同时只能被一个进程打开. LevelDB 会从操作系统获取一把锁来防止多进程同时打开同一个数据库. 在单个进程中, 同一个 leveldb::DB 对象可以被多个并发的线程安全地使用, 也就是说, 不同的线程可以写入或者获取迭代器, 或者针对同一个数据库调用 Get, 前述全部操作均不需要借助外部同步设施(leveldb 实现会自动地确保必要的同步). 但是其它对象, 比如 Iterator 或者 WriteBatch 需要外部自己提供同步保证. 如果两个线程共享此类对象, 需要使用锁进行互斥访问. 具体见对应的头文件. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:9:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#并发"},{"categories":null,"content":"迭代数据库下面的用例展示了如何打印数据库中全部的 (key, value) 对. leveldb::Iterator* it = db-\u003eNewIterator(leveldb::ReadOptions()); for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { cout \u003c\u003c it-\u003ekey().ToString() \u003c\u003c \": \" \u003c\u003c it-\u003evalue().ToString() \u003c\u003c endl; } assert(it-\u003estatus().ok()); // Check for any errors found during the scan delete it; 下面的用例展示了如何打印 [start, limit) 范围内数据: for (it-\u003eSeek(start); it-\u003eValid() \u0026\u0026 it-\u003ekey().ToString() \u003c limit; it-\u003eNext()) { ... } 当然你也可以反向遍历(注意, 反向遍历可能比正向遍历要慢一些, 具体见前面的读性能基准测试). for (it-\u003eSeekToLast(); it-\u003eValid(); it-\u003ePrev()) { ... } ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:10:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#迭代数据库"},{"categories":null,"content":"快照快照提供了针对整个 KV 存储的一致性只读视图. ReadOptions::snapshot 不为空表示读操作应该作用在 DB 的某个特定版本上; 若为空, 则读操作将会作用在当前版本的一个隐式的快照上. 快照通过调用 DB::GetSnapshot() 方法创建: leveldb::ReadOptions options; options.snapshot = db-\u003eGetSnapshot(); ... apply some updates to db ... // 获取与前面快照对应的数据库迭代器 leveldb::Iterator* iter = db-\u003eNewIterator(options); ... read using iter to view the state when the snapshot was created ... delete iter; db-\u003eReleaseSnapshot(options.snapshot); 注意, 当一个快照不再使用的时候, 应该通过 DB::ReleaseSnapshot 接口进行释放. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:11:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#快照"},{"categories":null,"content":"Slice 切片it-\u003ekey() 和 it-\u003evalue() 调用返回的值是 leveldb::Slice 类型的实例. 熟悉 Golang 或者 Rust 的同学对 slice 应该不陌生. slice 是一个简单的数据结构, 包含一个长度和一个指向外部字节数组的指针. 返回一个切片比返回 std::string 更加高效, 因为不需要隐式地拷贝大量的 keys 和 values. 另外, leveldb 方法不返回空字符结尾的 C 风格地字符串, 因为 leveldb 的 keys 和 values 允许包含 \\0 字节. C++ 风格的 string 和 C 风格的空字符结尾的字符串很容易转换为一个切片: leveldb::Slice s1 = \"hello\"; std::string str(\"world\"); leveldb::Slice s2 = str; 一个切片也很容易转换回 C++ 风格的字符串: std::string str = s1.ToString(); assert(str == std::string(\"hello\")); 注意, 当使用切片时, 调用者要确保它内部指针指向的外部字节数组保持存活. 比如, 下面的代码就有问题: leveldb::Slice slice; if (...) { std::string str = ...; slice = str; } Use(slice); 当 if 语句结束的时候, str 将会被销毁, 切片的底层存储也随之消失了, 后面再用就出问题了. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:12:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#slice-切片"},{"categories":null,"content":"比较器前面的例子中用的都是默认的比较函数, 即逐字节按照字典序比较. 你可以定制自己的比较函数, 然后在打开数据库的时候传入. 只需继承 leveldb::Comparator 然后定义相关逻辑即可, 下面是一个例子: class TwoPartComparator : public leveldb::Comparator { public: // Three-way comparison function: // if a \u003c b: negative result // if a \u003e b: positive result // else: zero result int Compare(const leveldb::Slice\u0026 a, const leveldb::Slice\u0026 b) const { int a1, a2, b1, b2; ParseKey(a, \u0026a1, \u0026a2); ParseKey(b, \u0026b1, \u0026b2); if (a1 \u003c b1) return -1; if (a1 \u003e b1) return +1; if (a2 \u003c b2) return -1; if (a2 \u003e b2) return +1; return 0; } // Ignore the following methods for now: const char* Name() const { return \"TwoPartComparator\"; } void FindShortestSeparator(std::string*, const leveldb::Slice\u0026) const {} void FindShortSuccessor(std::string*) const {} }; 然后使用上面定义的比较器打开数据库: // 实例化比较器 TwoPartComparator cmp; leveldb::DB* db; leveldb::Options options; options.create_if_missing = true; // 将比较器赋值给 options.comparator options.comparator = \u0026cmp; // 打开数据库 leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:13:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#比较器"},{"categories":null,"content":"后向兼容性比较器 Name 方法返回的结果在创建数据库时会被绑定到数据库上, 后续每次打开都会进行检查. 如果名称改了, 对 leveldb::DB::Open 的调用就会失败. 因此, 当且仅当在新的 key 格式和比较函数与已有的数据库不兼容而且已有数据不再被需要的时候再修改比较器名称. 总而言之, 一个数据库只能对应一个比较器, 而且比较器由名字唯一确定, 一旦修改名称或者比较器逻辑, 数据库的操作逻辑就统统会出错, 毕竟 leveldb 是一个有序的 KV 存储. 如果非要修改比较逻辑怎么办呢? 你可以根据预先规划一点一点的演进你的 key 格式, 注意, 事先的演进规划非常重要. 比如, 你可以存储一个版本号在每个 key 的结尾(大多数场景, 一个字节足够了). 当你想要切换到新的 key 格式的时候(比如新增 third-part 到上面例子 TwoPartComparator 处理的 keys 中), 那么你需要做的是: (a) 保持比较器名称不变 (b) 递增新 keys 的版本号 (c) 修改比较器函数以让其使用版本号来决定如何进行排序. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:13:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#后向兼容性"},{"categories":null,"content":"性能调优通过修改 include/leveldb/options.h 中定义的类型的默认值来对 leveldb 的性能进行调优. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:14:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#性能调优"},{"categories":null,"content":"Block 大小Leveldb 把相邻的 keys 组织在同一个 block 中(具体见后面文章针对 sorted string table 文件格式的描述), 而且 block 是 leveldb 把数据从内存到转移到持久化存储和从持久化存储转移到内存的基本单位. 默认的, 压缩前 block 大约为 4KB. 经常处理大块数据的应用可能希望把这个值调大, 而针对数据做\"点读\" 的应用可能希望这个值小一点, 这样性能可能会更高一些. 但是, 没有证据表明该值小于 1KB 或者大于几个 MB 的时候性能会表现更好. 同时要注意, 针对大的 block size, 压缩效率会更高一些. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:14:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#block-大小"},{"categories":null,"content":"压缩每个 block 在写入持久存储之前都会被单独压缩. 压缩默认是开启的, 因为默认的压缩算法非常快, 而且对于不可压缩的数据会自动关闭压缩功能. 极少有场景会让用户想要完全关闭压缩功能, 除非基准测试显示关闭压缩会显著改善性能. 按照下面方式做就关闭了压缩功能: leveldb::Options options; options.compression = leveldb::kNoCompression; ... leveldb::DB::Open(options, name, ...) .... ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:14:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#压缩"},{"categories":null,"content":"缓存数据库的内容存储在文件系统的一组文件里, 每个文件保存着一系列压缩后的 blocks. 如果 options.block_cache 不为空, 它就会被用于缓存频繁被使用的 block 内容(已解压缩). #include \"leveldb/cache.h\" leveldb::Options options; // 打开数据库之前分配一个 100MB 的 LRU Cache 用于缓存解压的 blocks options.block_cache = leveldb::NewLRUCache(100 * 1048576); // 100MB cache leveldb::DB* db; // 打开数据库 leveldb::DB::Open(options, name, \u0026db); ... use the db ... delete db delete options.block_cache; 注意 cache 保存的是未压缩的数据, 因此应该根据应用程序所需的数据大小来设置它的大小. (已压缩数据的缓存工作交给操作系统的 buffer cache 或者用户提供的定制的 Env 实现去干. ) 当执行一个大块数据读操作时, 应用程序可能想要取消缓存功能, 这样读进来的大块数据就不会导致 cache 中当前大部分数据被置换出去, 我们可以为它提供一个单独的 iterator 来达到该目的: leveldb::ReadOptions options; // 缓存设置为关闭 options.fill_cache = false; // 用该设置去创建一个新的迭代器 leveldb::Iterator* it = db-\u003eNewIterator(options); // 用该迭代器去处理大块数据 for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { ... } ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:14:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#缓存"},{"categories":null,"content":"Key 的布局设计注意, 磁盘传输的单位以及磁盘缓存的单位都是一个 block. 相邻的 keys(已排序)总是在同一个 block 中. 因此应用程序可以通过把需要一起访问的 keys 放在一起, 同时把不经常使用的 keys 放到一个独立的键空间区域来提升性能. 举个例子, 假设我们正基于 leveldb 实现一个简单的文件系统. 我们打算存储到这个文件系统的数据项类型如下: filename -\u003e permission-bits, length, list of file_block_ids file_block_id -\u003e data 我们可以给上面表示 filename 的 key 增加一个字符前缀, 比如 ‘/’, 然后给表示 file_block_id 的 key 增加另一个不同的前缀, 比如 ‘0’, 这样这些不同用途的 key 就具有了各自独立的键空间区域, 扫描元数据的时候我们就不用读取和缓存大块文件内容数据了. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:14:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#key-的布局设计"},{"categories":null,"content":"过滤器鉴于 leveldb 数据在磁盘上的组织形式, 一次 Get() 调用可能涉及多次磁盘读操作. 可配置的 FilterPolicy 机制可以用来大幅减少磁盘读次数. leveldb::Options options; // 设置启用基于布隆过滤器的过滤策略 options.filter_policy = NewBloomFilterPolicy(10); leveldb::DB* db; // 用该设置打开数据库 leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... use the database ... delete db; delete options.filter_policy; 上述代码将一个基于布隆过滤器的过滤策略与数据库进行了关联. 基于布隆过滤器的过滤方式依赖于如下事实, 在内存中保存每个 key 的部分位(在上面例子中是 10 位, 因为我们传给 NewBloomFilterPolicy 的参数是 10). 这个过滤器将会使得 Get() 调用中非必须的磁盘读操作大约减少 100 倍. 每个 key 用于过滤器的位数增加将会进一步减少读磁盘次数, 当然也会占用更多内存空间. 我们推荐数据集无法全部放入内存同时又存在大量随机读的应用设置一个过滤器策略. 如果你在使用定制的比较器, 你应该确保你在用的过滤器策略与你的比较器兼容. 举个例子, 如果一个比较器在比较 key 的时候忽略结尾的空格, 那么NewBloomFilterPolicy 一定不能与此比较器共存. 相反, 应用应该提供一个定制的过滤器策略, 而且它也应该忽略键的尾部空格. 示例如下: class CustomFilterPolicy : public leveldb::FilterPolicy { private: FilterPolicy* builtin_policy_; public: CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) {} ~CustomFilterPolicy() { delete builtin_policy_; } const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; } void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Use builtin bloom filter code after removing trailing spaces // 将尾部空格移除后再使用内置的布隆过滤器 std::vector\u003cSlice\u003e trimmed(n); for (int i = 0; i \u003c n; i++) { trimmed[i] = RemoveTrailingSpaces(keys[i]); } return builtin_policy_-\u003eCreateFilter(\u0026trimmed[i], n, dst); } }; 当然也可以自己提供非基于布隆过滤器的过滤器策略, 具体见 leveldb/filter_policy.h. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:14:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#过滤器"},{"categories":null,"content":"校验和Leveldb 将一个校验和与它存储在文件系统中的全部数据进行关联. 根据激进程度有两种方式控制校验和的核对: ReadOptions::verify_checksums 可以设置为 true 来强制核对从文件系统读取的全部数据的进行校验和检查. 默认为 false. Options::paranoid_checks 在数据库打开之前设置为 true 可以使得数据库一旦检测到数据损毁即报错. 取决于数据库损坏部位, 报错时机可能是打开数据库后的时候, 也可能是在后续执行某个操作的时候. 该配置默认是关闭状态, 这样即使持久性存储部分虽坏数据库也能继续使用. 如果数据库损坏了(当开启 Options::paranoid_checks 的时候可能就打不开了), leveldb::RepairDB 函数可以用于对尽可能多的数据进行修复. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:15:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#校验和"},{"categories":null,"content":"近似空间大小GetApproximateSizes 方法用于获取一个或多个键区间占据的文件系统近似大小(单位, 字节). leveldb::Range ranges[2]; ranges[0] = leveldb::Range(\"a\", \"c\"); ranges[1] = leveldb::Range(\"x\", \"z\"); uint64_t sizes[2]; leveldb::Status s = db-\u003eGetApproximateSizes(ranges, 2, sizes); 上述代码结果是, size[0] 保存 [a..c) 键区间对应的文件系统大致字节数, size[1] 保存 [x..z) 键区间对应的文件系统大致字节数. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:16:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#近似空间大小"},{"categories":null,"content":"环境变量由 leveldb 发起的全部文件操作以及其它的操作系统调用最后都会被路由给一个 leveldb::Env 对象. 用户也可以提供自己的 Env 实现以达到更好的控制. 比如, 如果应用程序想要针对 leveldb 的文件 IO 引入一个人工延迟以限制 leveldb 对同一个系统中其它应用的影响: // 定制自己的 Env class SlowEnv : public leveldb::Env { ... implementation of the Env interface ... }; SlowEnv env; leveldb::Options options; // 用定制的 Env 打开数据库 options.env = \u0026env; Status s = leveldb::DB::Open(options, ...); ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:17:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#环境变量"},{"categories":null,"content":"移植性如果某个特定平台提供 leveldb/port/port.h 导出的类型/方法/函数实现, 那么 leveldb 可以被移植到该平台上, 更多细节见 leveldb/port/port_example.h. 另外, 新平台可能还需要一个新的默认的 leveldb::Env 实现. 具体可参考 leveldb/util/env_posix.h 实现. –End– ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:18:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#移植性"},{"categories":null,"content":"之前的博客采用 OctoPress 搭建, 但是当时同步数据没有把 _post 下面的 md 原始文件同步, 导致这次迁移没法把之前几年积攒的文章搬过来. Hello world, again! ","date":"2020-09-10","objectID":"/hello-world-again/:0:0","series":null,"tags":["trivial"],"title":"Hello World, Again","uri":"/hello-world-again/#"}]