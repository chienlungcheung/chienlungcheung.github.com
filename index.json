[{"categories":null,"content":" AI算法工程师-CV","date":"2023-11-27","objectID":"/ebg-hiring/:1:0","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#ai算法工程师-cv"},{"categories":null,"content":" 职责： 参与人脸识别、轨迹点识别的需求分析、识别引擎算法设计； 负责核心模块的程序设计和代码编写，解决重难点技术问题； 遵循公司相关技术规范，保障研发产品的功能及性能指标。 ","date":"2023-11-27","objectID":"/ebg-hiring/:1:1","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#职责"},{"categories":null,"content":" 要求： 计算机相关专业本科以上学历，2 年及以上工作经验； 熟悉基本的图像预处理理论知识及图像处理基本方法，并能够运用工具进行图像分析； 有OMR、OCR、答题卡识别等相关研究与项目经验者优先； 了解常见的开源库OpenCV、tensorflow、caffe; 熟练掌握C++、Python语言，精通linux/unix下编程，熟悉gdb等调用工具； 良好的逻辑思维能力，优秀的分析和解决问题的能力，对解决具有挑战性问题充满激情； 知识面广，思路开阔，创新能力强，对新技术持有敏感性并愿意致力于新技术的探索和研究。 ","date":"2023-11-27","objectID":"/ebg-hiring/:1:2","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#要求"},{"categories":null,"content":" AI算法工程师-NLP","date":"2023-11-27","objectID":"/ebg-hiring/:2:0","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#ai算法工程师-nlp"},{"categories":null,"content":" 职责： 参与自然语言理解需求分析、划题、查重引擎算法设计； 负责核心模块的程序设计和代码编写，解决重难点技术问题； 遵循公司相关技术规范，保障研发产品的功能及性能指标。 ","date":"2023-11-27","objectID":"/ebg-hiring/:2:1","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#职责-1"},{"categories":null,"content":" 要求： 计算机相关专业本科以上学历，2 年及以上工作经验； 熟悉基本的自然语言理解理论知识及处理基本方法，并能够运用工具进行图像分析； 有GPT、Bert等相关研究与项目经验者优先； 了解常见的开源库OpenCV、tensorflow、caffe; 熟练掌握C++、Python语言，精通linux/unix下编程，熟悉gdb等调用工具； 良好的逻辑思维能力，优秀的分析和解决问题的能力，对解决具有挑战性问题充满激情； 知识面广，思路开阔，创新能力强，对新技术持有敏感性并愿意致力于新技术的探索和研究。 ","date":"2023-11-27","objectID":"/ebg-hiring/:2:2","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#要求-1"},{"categories":null,"content":" 软件开发工程师-Java","date":"2023-11-27","objectID":"/ebg-hiring/:3:0","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#软件开发工程师-java"},{"categories":null,"content":" 职责： 参与软硬一体业务系统的需求分析、架构设计和技术路线演进； 设计具备高扩展性、高性能、安全、稳定、可靠的技术系统； 负责核心模块的设计和代码编写，解决重难点技术问题； 遵循公司相关技术规范，保障研发产品的功能及性能指标。 ","date":"2023-11-27","objectID":"/ebg-hiring/:3:1","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#职责-2"},{"categories":null,"content":" 要求： 计算机相关专业本科以上学历，5年以上软硬一体系统开发经验，2年以上架构设计经验； 良好的逻辑思维能力，具备软硬一体系统整体设计和编码能力，有很强的分析问题和解决问题的能力，对解决具有挑战性问题充满激情； 知识面广，思路开阔，创新能力强，对新技术持有敏感性并愿意致力于新技术的探索和研究。 ","date":"2023-11-27","objectID":"/ebg-hiring/:3:2","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#要求-2"},{"categories":null,"content":" 软件开发工程师-Web","date":"2023-11-27","objectID":"/ebg-hiring/:4:0","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#软件开发工程师-web"},{"categories":null,"content":" 职责： 参与高并发业务系统的需求分析、架构设计和技术路线演进； 设计具备高扩展性、高性能、安全、稳定、可靠的技术系统； 负责核心模块的设计和代码编写，解决重难点技术问题； 遵循公司相关技术规范，保障研发产品的功能及性能指标。 ","date":"2023-11-27","objectID":"/ebg-hiring/:4:1","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#职责-3"},{"categories":null,"content":" 要求： 计算机相关专业本科以上学历，5年以上java开发经验，2年以上架构设计经验； 有高并发分布式系统开发经验，熟悉缓存技术、网站优化、服务器优化、集群技术处理、网站负载均衡、系统性能调优等相关中间件及软件编程高级技术； 对数据库的基本理论和内部实现机制有深刻的理解，能够熟练应用mysql/nosql，有实际大数据量的数据库设计经验； 良好的逻辑思维能力，熟悉业务抽象和数据模型设计，具有很强的分析问题和解决问题的能力，对解决具有挑战性问题充满激情； 知识面广，思路开阔，创新能力强，对新技术持有敏感性并愿意致力于新技术的探索和研究。 ","date":"2023-11-27","objectID":"/ebg-hiring/:4:2","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#要求-3"},{"categories":null,"content":" 联系我感兴趣或者有推荐的可以给我发邮件 cWx6aGFuZ0BpZmx5dGVrLmNvbQ== ","date":"2023-11-27","objectID":"/ebg-hiring/:5:0","series":null,"tags":null,"title":"招人！","uri":"/ebg-hiring/#联系我"},{"categories":null,"content":"[toc] 语言符号化是逻辑计算的前提，最近这一百年，从图灵、丘奇到麦卡锡、闵斯基再到科尔莫格洛夫，这些前辈都在寻找语言的可计算性。最近这几十年，语言的可计算性经历了从 N-gram 到 word embedding 再到 attention 演进，每一次的突破都将 NLP 领域推进了一大步。本文试图来描述这个演进过程。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:0:0","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#"},{"categories":null,"content":" 1 语言模型(LM)技术体系的时代划分针对语言模型技术体系的演进，我们可以依据关键技术的出现和发展来进行分段。 古典时代（Classical Era）： 起始：早期的统计方法，如基于词频的方法。 高潮：N-gram 模型的发展。 结束：N-gram 模型因数据稀疏性问题而受到限制。 特点：主要依赖统计和计数方法，模型简单，容易理解。 嵌入时代（Embedding Era）： 起始：神经网络语言模型的初步尝试，如 Bengio 于 2003 年的论文（《A Neural Probabilistic Language Model》）。 高潮：Word2Vec、GloVe 等词嵌入技术的诞生和普及。 结束：嵌入技术成为标准，但缺乏长距离上下文信息捕获能力。 特点：词向量能够捕获语义和句法信息，但模型通常只考虑局部上下文。 深度学习时代（Deep Learning Era）： 起始：RNN、LSTM 和 GRU 的广泛应用于语言建模。 高潮：Transformer 架构和 GPT、BERT 等预训练模型的成功。 现状：巨大的预训练模型如 OpenAI 的 GPT-3 和 Google 的 BERT 系列继续主导这个领域。 特点：能够处理长距离上下文，模型复杂度增加，需要大量的数据和计算资源。 每个时代都有其独特的特点和挑战。 N-gram 是自然语言处理和语言建模中的经典技术，以下是 N-gram 解决的问题和遗留的问题： 解决的问题： 简单性与效率：N-gram 模型相对简单，它只是基于统计计数来预测下一个单词的出现。因此，构建和使用 N-gram 模型是高效的，特别是对于大规模的文本数据。 本地上下文捕捉：N-gram 模型可以捕捉到 n-1 个单词的局部上下文，这在很多应用中是有用的，比如拼写检查和一些简单的文本生成任务。 无需复杂训练：N-gram 模型不需要像神经网络模型那样的迭代训练过程，只需一次通过数据进行计数。 模型可解释性：与深度学习模型相比，N-gram 模型更容易理解和解释，因为它基于实际文本中的统计计数。 遗留的问题： 稀疏性问题：尽管 N-gram 可以捕获有限的上下文，但随着 n 的增大，数据中出现的唯一的 N-gram 组合会急剧增加，导致数据稀疏问题。这意味着很多 N-gram 组合可能不会在训练数据中出现，但在实际应用中可能会遇到。 考虑一个极端情况：对于一个包含 10,000 个词的语料库，2-gram 的可能组合是 $10,000^2$，而 5-gram 的可能组合是 $10,000^5$。实际语料库中的这些组合只是这些可能组合的一小部分。这意味着许多组合可能在训练数据中只出现一次或根本不出现。这就导致了数据稀疏问题：模型对于许多 N-gram 组合可能没有足够的数据来准确估计它们的概率。 固定窗口大小：N-gram 模型的窗口大小是固定的，这意味着它不能捕获超出固定窗口大小的长范围依赖。 缺乏语义理解：N-gram 模型完全基于统计计数，它没有捕捉到单词间的语义关系。例如，“猫”和“猫科动物”在语义上可能很相似，但 N-gram 模型不能识别这种关系，除非它们在相同的上下文中经常出现。 计算和存储需求：随着 n 的增大，需要存储的 N-gram 组合数量也会增加，这可能导致存储和计算上的挑战。 平滑问题：为了解决不在训练集中出现的 N-gram 组合问题，需要使用各种平滑技术，如 Add-k 平滑。这些技术本身带有一些偏见和限制。 不灵活：N-gram 模型通常对于其训练数据外的新的、不同的上下文是不灵活的。 总的来说，尽管 N-gram 为一些 NLP 问题提供了简单有效的解决方案，但其固有的限制也促使研究者探索新的、更复杂的模型，如基于神经网络的语言模型，以捕捉语言的复杂性和多样性。 从 Bengio 于 2003 年提出的基于神经网络的语言模型开始，Word embedding 技术经历了几个关键的发展阶段： Bengio的神经网络语言模型 (2003): 解决的问题：传统的 N-gram 模型遭受了稀疏性的问题，尤其是当词汇量很大或需要考虑更长的依赖时。Bengio 的模型使用了一个前馈神经网络来学习单词的分布式表示，并用这些表示来预测下一个单词，从而解决了稀疏性的问题。 遗留的问题：尽管该模型成功地引入了神经网络到语言建模中，但它的训练仍然相对缓慢，并且计算成本较高。 Word2Vec (Mikolov 等人, 2013): 解决的问题：Word2Vec 提出了两种有效的模型架构（CBOW 和 Skip-gram），这两种架构都大大提高了单词嵌入的训练速度和效率。Word2Vec的嵌入捕获了单词之间的语义和句法关系。 遗留的问题：每个单词只有一个嵌入，这意味着词义多义性的问题（即一个单词有多个含义）没有被直接处理。 GloVe (Pennington等人, 2014): 解决的问题：GloVe模型是基于全局统计信息的，而不仅仅是局部信息。这使得嵌入更能反映大量语料库中的统计规律。 遗留的问题：与Word2Vec相同，每个单词只有一个嵌入，没有处理多义词问题。 ELMo (Peters 等人, 2018): 解决的问题：ELMo 使用了深度双向 LSTM 来获取上下文敏感的单词嵌入，从而能够为一个单词生成不同的嵌入，取决于它的上下文，有效地处理了词义多义性问题。 遗留的问题：尽管 ELMo 嵌入提供了丰富的语义信息，但计算成本仍然较高，特别是对于需要实时处理的应用。 Transformer \u0026 BERT (Vaswani 等人, 2017; Devlin 等人, 2018): 解决的问题：Transformer 结构引入了自注意力机制，使模型能够并行化处理整个句子，大大提高了效率。BERT 则利用了 Transformer 的结构，并通过双向无监督预训练进一步增强了上下文敏感性。 遗留的问题：虽然 BERT 的性能非常出色，但它的模型尺寸巨大，对计算资源的需求也很高。 自从 Bengio 的初步工作以来，Word embedding 技术已经取得了长足的进步，特别是在捕获单词和上下文之间的复杂关系方面。每一步的进展都在解决前一步的限制，但同时也带来了新的挑战，这推动了研究的进一步发展。 由于嵌入时代与大模型诞生密不可分，下面重点讲一下这个时代及其之后的技术演进。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:1:0","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#1-语言模型lm技术体系的时代划分"},{"categories":null,"content":" 2 CNN：图像预训练的启示 自从深度学习爆火，预训练过程就成为图像领域的常规方法了，而且这种做法很有效，能明显促进应用的效果。 上图展示图像领域的预训练过程。我们设计好网络结构以后，对于图像来说一般是 CNN 的多层叠加网络结构。可以先用某个训练集合（一般是通用型数据，如 /images/natural-language-computability/imagenet）对这个网络进行预训练，在对应任务上学会网络参数，然后存起来以备后用。假设我们面临另一个任务，网络结构采取相同的网络结构。在较低的 CNN 层，网络参数初始化的时候可以加载前述在通用数据上学习好的参数；在较高的 CNN 层，参数仍然随机初始化。参数设置好后用新任务的数据来训练网络。 在新任务上训练数据时，针对参数迭代有两种做法。一种是“frozen”（参数冻结），即之前从通用任务引入的浅层参数在训练新任务过程中不变;另外一种是“Fine-Tuning”（参数微调），即全量参数在新任务训练过程中会被改变，但这种改变是在预训练参数技术上进行的相比从零训练全量参数要简单很多。不管采用哪种做法都有一个很大的好处，即使得过去因为数据量太少而无法训练的任务变得可达。 针对动辄上亿参数的网络结构如 Resnet、Densenet 等，少量的训练数据很难训练好如此复杂的网络。但是如果其中大量参数预先通过大的训练集合（如 /images/natural-language-computability/imageNet）进行训练，然后将训练好的参数直接拿来初始化新任务的大部分网络结构参数，再在新任务比较少的数据量上进行训练，就容易多了。此外，即使新任务可用训练数据量很大，前置预训练过程也能极大加快新任务训练的收敛速度。 预训练为什么可行？ CNN 的设计在某种程度上受到了动物视觉系统的启发，尤其是猫和猴子的视觉皮层。在 20 世纪 60 年代，两位神经生物学家 David Hubel 和 Torsten Wiesel 对猫的视觉皮层进行了实验，发现存在专门对特定方向的边缘或线条响应的神经元，这与 CNN 中的边缘检测器有相似之处。 CNN 的层级结构使其能从浅到深抽象不同层次的特征： 浅层（例如第一、第二层）： 主要捕捉基本的视觉特征，如边缘、颜色和纹理。这些特征通常是局部的和简单的。 中层： 开始捕捉更复杂的特征，如简单的形状和模式，例如圆圈、条纹等。 深层： 能够表示更高级、更抽象的特征，如物体的各种部分、复杂的形状等。 最深层： 在更高的抽象层面，能够识别整个物体或场景，如狗、车或风景等。 越是底层的特征越通用，也就是说不论什么领域的图像都会具备诸如边角线弧线等底层基础特征；层级越高，抽取出的特征越与具体任务相关。因此，预训练好的网络参数，越是底层的网络参数抽取出特征越与具体任务无关。这种通用性正是预训练为什么可行的原因。因此，如果有预训练好的通用模型，在针对新的具体任务做训练之前，可以使用预训练模型底层参数初始化新任务网络参数。由于高层特征跟具体任务关联较大，可以随机初始化或者采用 Fine-tuning 方式通过新数据集“清洗”掉高层无关的参数。 既然预训练方法在图像领域这么好用，那 NLP 领域是否可以做类似的尝试呢？ ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:2:0","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#2-cnn图像预训练的启示"},{"categories":null,"content":" 3 Word2Vec：自然语言嵌入神经网络Word embedding 诞生于 2003 年，我们可以将它看作 NLP 领域的预训练技术。 介绍 word embedding 之前需要先了解语言的分布式表示和语言模型： 语言的分布式表示（Distributed Representation）： 这是一种表示文本信息（如单词、短语、句子等）的方法，通常采用稠密的向量形式。 分布式表示的核心观念是：词义由其在文本中的上下文定义。换句话说，出现在相似上下文中的单词应该有相似的表示。因此，“cat” 和 “kitten” 这两个单词的向量表示会比 “cat” 和 “car” 更加接近。 Word2Vec、GloVe 和 FastText 是常用的方法来得到单词的分布式表示。 语言模型（Language Model，简称 LM）： 语言模型是用来预测一个词序列的概率的模型。例如，给定一个句子或文本片段的前半部分，它可以预测下一个词是什么。具体来说，给定一个词序列 w1,w2,…,wn，语言模型的任务就是评估这个词序列出现的概率 $P(w_1,w_2,…,w_n)$。 常见的语言模型有： 统计语言模型如 N-gram 模型：这是最简单的语言模型之一，它使用词的 n-1 个前面的词作为上下文来预测当前词。 神经网络语言模型如循环神经网络（RNN）模型：RNN 可以捕捉长距离的依赖关系，用于建立更加复杂的语言模型。 基于注意力的语言模型如 Transformer 和基于 Transformer 的模型：这类模型近年来非常流行，特别是 BERT、GPT、T5 等。它们使用 self-attention 机制来捕获文本中的长距离和短距离依赖。 两者联系与区别： 当我们训练深度学习语言模型（如基于 Transformer 的模型）时，模型的内部会为输入的文本自动学习一种分布式表示。这些表示捕捉了文本中的语义和语法信息，并可以用于其他 NLP 任务。例如，BERT 是一个预训练的语言模型，它通过遮蔽（masked）单词来进行训练。当训练完成后，即使我们不使用 BERT 作为语言模型，我们仍然可以使用其内部的分布式表示来完成其他任务，如文本分类、情感分析等。 分布式表示和语言模型是两个相关但不同的概念，前者是文本信息的表示方式，后者是预测词序列概率的模型。但在深度学习的语言模型中，两者往往紧密结合。 无论是基于统计的语言模型还是基于神经网络的语言模型，它们的核心目标都是最大化一个序列对应的联合概率。但是，为了在实际应用中避免计算困难和解决数据稀疏性问题，这些模型通常都会利用链式规则将联合概率分解为条件概率的乘积。例如： n-gram 语言模型：这是一个统计方法，它使用 n 个词的滑动窗口来估计条件概率。n-gram 模型的限制在于它只能捕获 n 个词的局部依赖关系，而且对于稀有的 n-gram 组合，可能没有足够的数据进行准确估计。 神经网络语言模型：这种模型使用神经网络来估计条件概率。由于神经网络可以学习分布式表示，它能够捕获更长距离的依赖关系，并且更好地处理数据稀疏性问题。例如，word2vec、LSTM 和 Transformer（如 GPT 和 BERT）等模型都属于这类。 在训练这些模型时，无论是统计模型还是神经网络模型，它们都会试图最大化整个数据集上的条件概率，从而间接地最大化联合概率。 那我们如何设计一个神经网络来实现一个语言模型呢？我们期望针对训练好的神经网络，输入一句话前面若干单词，它能正确输出后面紧跟的单词。 我们可以设计如上图的网络结构，这其实就是大名鼎鼎的“神经网络语言模型”。2003 年，Bengio 发表在 JMLR 上的论文《A Neural Probabilistic Language Model》创造了该模型。2013 年，当深度学习开始渗透到 NLP 领域的时候，该设计重新焕发光彩。 下面我们来看如何设计并实现它。 神经网络的学习目标为输入某个单词如 model 的前面 n-1 个单词，要求网络正确预测单词 model，即最大化条件概率 $P(W_t=“model”|W_{t-n+1},W_{t-n+2},…,W_{t-1};θ)$，其中 θ 是前述条件概率最大时的模型参数取值。 训练过程为： $W_t$ 前面每个单词 $W_i$ 都用 Onehot 方式编码（比如 0001000）作为原始单词输入。 前述每个 onehot 向量分别乘以矩阵 C 后获得向量 $C(W_i)$，该向量即为 $W_i$ 对应的 word embedding。 然后将每个 $C(W_i)$ 级联在一起，输入后续的隐层。 最后通过 softmax 去预测后面应该接哪个单词。 我们重点谈一下 $C(W_i)$ 的学习过程。 前面提到 $C(W_i)$ 就是单词对应的 Word Embedding 值。矩阵 $C$ 包含 $V$ 行，$V$ 代表词典的大小，每一行内容代表对应单词的 Word embedding 值。当然，矩阵 $C$ 需要通过学习获得的网络参数；训练初试可以用随机值初始化矩阵 $C$；当这个网络训练好后，矩阵 $C$ 的内容就会被正确赋值，每一行代表一个单词对应的 Word embedding 值。因此，通过这个神经网络学习任务，我们不仅自己能够根据上文预测后接单词是什么，同时获得一个副产品矩阵 $C$，即每个单词的 Word Embedding。 2013 年，NLP 领域最振奋人心的工作就是 Word2Vec，它可以更好地计算出 word embedding。Word2Vec 工作原理如下图： Word2Vec 的网络结构与前述 Bengio 设计的网络结构基本类似。不过这里需要指出：尽管网络结构相近，而且也是做语言模型任务，但是其训练方法不太一样。 Word2Vec 有两种训练方法。第一种叫 CBOW，其核心思想是从一个句子里面把一个词抠掉，用这个词的上文和下文去预测被抠掉的这个词（后面 BERT 的掩码语言模型 MLM 与之异曲同工）；第二种叫做 Skip-gram，和 CBOW 正好反过来，输入某个单词，要求网络预测它的上下文单词。这两种方法与前述 Bengio 设计的网络仅用一个单词的上文预测该单词的做法是有显著差异的。 为什么 Word2Vec 这么处理？原因很简单，Bengio 设计的网络主要任务是要学习一个解决语言模型任务的网络结构，语言模型就是要看到上文预测下文，而word embedding 部分只是该设计的一个副产品；而 Word2Vec 目标不一样，它的目标就获取更好地 word embedding。 Word2Vec 发明的 CBOW 的训练方法与后来的 BERT 关系莫大，同时它也是典型的预训练过程，可以在其之后接上具体的语言模型任务，如下图所示。 它的使用方法与前面 Bengio 设计的网络是一样的：句子中每个单词以 Onehot 形式作为输入，每个 onehot 向量乘以学好的 Word Embedding 矩阵 $C$，获得单词对应的 Word Embedding。 不知道大家发现没有，上面这个操作本质是个查表操作。当我们用词典排序为 $i$ 的单词对应的 onehot 向量（$1$ 行 $V$ 列，且仅第 $i$ 个值为 $1$）乘以矩阵 $C$ （$V$ 行 $M$ 列）的时候，得到的就是矩阵 $C$ 的第 $i$ 行；由此可见，前述向量矩阵相乘操作可以避免，直接取出矩阵 $C$ 的第 $i$ 行即可。 我们现在回到预训练话题。 Word Embedding 矩阵 $C$ 其实就是 Onehot 输入层到 embedding 层的网络参数。因此，针对新任务使用训练好的 Word Embedding 等价于将新任务对应网络的 Onehot 层到 embedding 层的网络参数初始化了。这与图像领域使用通用任务的低层参数初始化新任务对应的低层参数是一样的。区别在于 Word Embedding 只能初始化第一层网络参数，再高层的参数就无能为力了。下游 NLP 任务在使用 Word Embedding 的时候也类似图像有两种做法，一种是 Frozen，就是 Word Embedding 那层网络参数固定不动；另外一种是 Fine-Tuning，就是 Word Embedding 这层参数使用新的训练集合训练也需要跟着训练过程更新掉。 Bengio 和后来的 word2vec 开启了 NLP 领域的预训练，但是这时候 Word Embedding 还存在一些缺陷。典型地，基于 word2vec 技术，每个单词只有一个向量，但是日常使用中经常出现一词多义的情况。这个问题影响了 word embedding 在实际案例中的应用，亟待解决。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:3:0","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#3-word2vec自然语言嵌入神经网络"},{"categories":null,"content":" 4 ELMo：双向模型三层嵌入消除歧义针对 word2vec 遗留的一词多义问题，ELMo 提供了一种简洁优雅的解决方案。ELMo 是“Embedding from Language Models”的简称，出自论文《Deep contextualized word representations》。ELMo 与以往方案差异最大的点即论文标题中提到的“deep contextualized”。ELMo 之前的做法都是将预训练模型最后一层（top layer）的输出串接到下游模型上，而 ELMo 是将自身网络各层参数做了线性组合后再输入到下游模型中。 学习高质量的单词表示是一个很大的挑战，ELMo 将问题具体建模为：1、如何学习单词复杂特征如语法和语义；2、如何处理一词多义。 个人认为，ELMo 考虑了语言双向训练的重要性， 先进性堪比后来的 BERT。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:4:0","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#4-elmo双向模型三层嵌入消除歧义"},{"categories":null,"content":" 4.1 ELMo 预训练过程如下图所示，ELMo 采用了一个两层双向 LSTM（长短期记忆网络） 网络架构很好地解决了这个挑战。 两层双向 LSTM 的设计，其每个方面都扮演着重要的角色： 双向LSTM： 双向 LSTM 包含两个方向的 LSTM 网络，一个处理正向的文本序列（从开始到结束），另一个处理反向的文本序列（从结束到开始）。这种双向结构允许模型同时捕获前向（上下文中之前的词汇）和后向（上下文中之后的词汇）的上下文信息。 在传统的单向模型中，每个词只根据它之前的词来进行理解。双向LSTM通过结合来自两个方向的信息，能更全面地理解每个词在其整个上下文中的含义。 两层结构： ELMo使用了两层 LSTM 结构，这意味着它有两个叠加的 LSTM 网络层。每一层都能够从它的输入中捕获和学习不同级别的信息。 第一层 LSTM 捕获和处理相对基础的语言特征（如语法结构），而第二层 LSTM 则能够基于第一层的输出进一步提炼和理解更复杂的语言特征（如语义关系）。这种层叠的方法允许模型更深入地理解文本，从而生成更精准的词嵌入。 总结来说，ELMo 的双向 LSTM 设计让模型能够有效地捕获词汇的双向上下文信息，而两层结构则使得模型可以在不同层次上学习和理解语言，这两者结合使 ELMo 在许多自然语言处理任务中都表现出色。 如前面章节所述，语言模型训练的目的是找到一组模型参数，使得在这组参数下，训练集中的 token 序列（如单词、字符等）出现的联合概率最大化。 ELMo 也不例外，假设一个长度为 N 的序列$（t_1,t_2,…,t_N）$： 在已知前序序列 $（t_1,t_2,…,t_N）$ 前提下 $t_k$ 出现概率为 $p(t_k|t_1,t_2,…,t_{k-1})$，则整个序列的联合概率分布为： 在已知后续序列 $（t_1,t_2,…,t_N）$ 前提下 $t_k$ 出现概率为 $p(t_k|t_{k+1},t_{k+2},…,t_N)$，则整个序列的联合概率分布为： 综合两个方向，我们的预训练目标为最大化下述前向和后向的联合似然函数（概率取对数后由相乘改为相加，计算更方便）： 其中 $Θ_x$、$Θ_{LSTM}$、$Θ_s$ 分别是输入层、LSTM 层以及 Softmax 层待学习的参数。预训练目的即为求解使得上述联合概率分布最大时前述参数的值。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:4:1","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#41-elmo预训练过程"},{"categories":null,"content":" 4.2 ELMo 预训练模型用于下游任务ELMo 在用于下游任务时会冻结参数，也就是相关参数期间不会被修改或微调。不同于 word2vec 只是将最终一层参数输入下游任务，ELMo 在使用时会针对具体下游任务学习一个矩阵，该矩阵会将 ELMo 各层已冻结参数进行线性组合，然后输入到下游任务中。从效果上看，ELMo 通过预训练捕获的知识为下游任务提供了附加特征。 具体使用过程如下。 预训练结束后，针对每个 token $k$，ELMo 模型训练出了大小为 $2L+1$ 的表示（representation）集合，记为 $R_k$，具体为： 其中 $x_k^{LM}$、$\\overrightarrow{h_{k,j}^{LM}}$、$\\overleftarrow{h_{k,j}^{LM}}$ 分别为输入层的词嵌入表示（具体实现通过字符级 CNN 得到，这里不做重点讨论，作为默认即可）、第 $j$ 层前向 LSTM 输出、第 $j$ 层后向 LSTM 输出。 在用于下游任务时，针对 token $k$ 需要基于 $R_k$ 调整生成新的 embedding，记为 $ELMo_k^{task}$，这是 ELMo 相比以往模型最大的创新点，既保留了预训练过程的知识，又针对具体下游任务学到了新的组合方式。 的学习过程如下： 上述公式各参数含义如下： $\\overrightarrow{h_{k,j}^{LM}}$：这是对于第 $k$ 个 token，经过特定任务调整的 ELMo 表示。 $E$：这是一个函数，它结合了不同层次的表示和任务相关的参数来生成最终的任务特定嵌入。 $R_k$：这是第 k 个token的所有层次表示的集合，包括词嵌入和所有LSTM层的隐藏状态。 $\\theta^{task}$：这是特定任务的参数集合，用于调整不同层次表示的权重。 $\\gamma^{task}$ ：这是一个缩放参数，用于调整整个嵌入向量的规模。 $\\sum_{j=0}^{L}s_j^{task}h_{k,j}^{LM}$：这是一个加权和，其中 $s_j^{task}$ 是第 $j$ 层的 softmax 归一化权重，它乘以 $h_{k,j}^{LM}$，即第 $k$ 个 token 在第 $j$ 层的表示。 $h_{k,j}^{LM}$：是 $R_k$ 中的一个元素，它表示第 $k$ 个 token 在第 $j$ 层的隐藏状态，这包括了从字符级 CNN 得到的原始词嵌入（当 $j = 0$）以及所有 LSTM 层的输出。 上述过程图形化表示如下： 针对输入的每个 token，都会为具体下游任务学习生成对应的 embedding。 以上即为 ELMo 相关预训练和使用的重点内容。论文显示，采用 ELMo 后的每个 NLP 任务效果提升在 $6 - 20%$ 之间，可以说相当显著。 ELMo 解决了一词多义问题，那它有什么不足呢？相比后来的 GPT 和 Bert，LSTM 的特征抽取能力还是弱了很多。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:4:2","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#42-elmo预训练模型用于下游任务"},{"categories":null,"content":" 5 GPT：超强特征抽取器上场GPT 是 “Generative Pre-Training” 的简称，从名字看其含义是指的生成式的预训练。GPT 也采用两阶段过程，第一个阶段是利用语言模型进行预训练，第二阶段通过 Fine-tuning 的模式解决下游任务。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:5:0","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#5-gpt超强特征抽取器上场"},{"categories":null,"content":" 5.1 GPT 预训练过程GPT采用的 Transformer 架构，但仅包含 Decoder 部分。 下图展示了 GPT 基于 Transformer 模型的神经网络架构，用于处理不同类型的文本理解任务。图中的上半部分是一个高级的分类器，它包括多个任务特定的结构。下半部分是Transformer模型的标准架构，显示了一个12层的堆叠结构，每一层由三个主要部分组成：自注意力机制、前馈网络和层归一化。 Transformer 核心架构: 文本和位置嵌入（Text \u0026 Position Embed）: 输入文本被转换为嵌入向量，同时加上位置信息，以保留单词的顺序。 Masked Multi Self Attention: 自注意力机制可以使模型在处理每个单词时考虑到序列中的所有单词，“Masked\"意味着在训练过程中防止未来信息的泄露。 Layer Norm: 层归一化有助于稳定训练过程，加快收敛速度，并改善泛化能力。 Feed Forward: 前馈网络在自注意力后对特征进行进一步的处理。 以下是 GPT 相对于ELMo的一些潜在优势： Transformer 架构: GPT使用的是 Transformer 架构，这种架构利用了自注意力机制，允许模型更有效地捕捉长距离依赖关系。这与ELMo使用的基于LSTM的双向模型相比，能够更灵活地处理文本中的各种结构模式。 GPT 在每个 Transformer 层中计算自注意力，这允许它在多个层级捕捉细粒度的特征。相比之下，ELMo通过分层的LSTM来抽取特征，可能不会捕捉到 Transformer 能够捕捉到的某些复杂模式。 端到端的训练: GPT 可以通过端到端的训练进行微调，适应特定的下游任务。而ELMo生成的词嵌入通常在特定任务上需要额外的模型层进行进一步的训练。 总体来说，GPT 的优势在于其 Transformer 架构和生成能力，这使得它在处理和理解语言时更加灵活和强大。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:5:1","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#51-gpt预训练过程"},{"categories":null,"content":" 5.2 GPT 预训练模型用于下游任务不同于 ELMo 在应用于下游任务时会冻结各层参数，GPT在用于下游任务时会连同下游任务一起进行全参数微调。 在针对下游具体任务的微调过程中包含语言建模作为辅助目标（即再次将 GPT 预训练目标纳入微调）有两个优点： 改善了监督模型的泛化能力： 在训练有监督的模型时，主要目标是最小化在特定任务（例如分类、情感分析等）上的损失。当同时包含语言建模作为训练目标时，模型不仅要学习如何在这个特定任务上表现良好，而且还要学习生成或预测文本。这种类型的多任务学习可以促使模型学习到更通用的特征表示，从而在面对未见过的数据时，提高其泛化性能。 加速了收敛： 收敛指的是模型在训练过程中快速达到最低损失的能力。通过在微调中包含语言建模任务，模型可以更快地调整其参数以适应特定的下游任务。这可能是因为语言建模作为辅助任务提供了额外的信号和约束，帮助模型更快地找到损失函数的最优点。 这也意味着在微调过程中，模型的损失函数将包括两部分：一部分针对下游任务（如分类的损失），另一部分是语言模型的损失（如基于预测下一个单词的交叉熵损失）。 在GPT论文中，上述公式表示的是总损失函数 $L_3(C)$，它是由两部分组成的：下游任务的损失函数 $L_2(C)$ 和语言模型的损失函数 $L_1(C)$ 的加权和。这个组合损失函数被用来在微调 GPT 模型时同时优化两个目标： • $L_2(C)$ 是针对特定下游任务的损失函数，比如分类、回答问题或其他类型的任务。 • $L_1(C)$ 通常是语言模型的损失，用于最大化序列的对数似然，通常是通过预测下一个单词的方式来实现的。 其中 $\\lambda$ 是一个超参数，它权衡了语言模型损失在总损失中的比重。这可以帮助模型在保持语言生成能力的同时，对下游任务进行专门的微调。在这种设置中， $\\lambda$ 的值决定了在微调过程中，保留语言模型能力与提高下游任务性能之间的平衡。如果 $\\lambda$ 较大，模型将更强调语言模型能力；如果 $\\lambda$ 较小，模型将更专注于下游任务。 这样做的一个潜在优点是，它可以防止在微调阶段对特定任务过度优化，从而导致模型失去其原有的语言理解能力——一种被称为“灾难性遗忘”的现象。通过继续对语言模型的目标进行优化，模型能够在保持原有语言能力的同时，更好地适应新任务。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:5:2","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#52-gpt预训练模型用于下游任务"},{"categories":null,"content":" 6 BERT：双向语言模型崛起在 BERT 之前，语言模型不管是基于 feature 的模型（如 ELMo）还是基于 fine-tuning 的模型（如 GPT）本质上都是单向的，而 BERT 是一个真正的双向语言模型： • GPT使用的是Transformer模型的解码器（Decoder）部分，而不是编码器。解码器在原始的Transformer模型中用于生成文本，它在生成每个新词时只能依赖于之前的词。 • ELMo 虽然表面看是一个双向语言模型，但其预训练过程不管是从左到右还是从右到左都是分开独立进行的，只是在最后进行了一个浅层的（shallow）的拼接。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:6:0","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#6-bert双向语言模型崛起"},{"categories":null,"content":" 6.1 BERT 预训练过程BERT 预训练与微调架构如下图所示，它将预训练和微调的架构统一了起来。 客观地讲， BERT 在网络架构上没有创新，它与 GPT 类似都是沿用了 Transformer，它的效果拔群源自其在预训练期间采用的两个无监督任务：MLM（masked language model，掩码语言模型）与 NSP（next sentence prediction，下个句子预测）。前者用于挖掘句子内单词间的关系，后者用于挖掘句子间的关系。 6.1.1 MLM 任务标准的条件语言模型只能通过从左到右或者从右到左的方式来训练，否则每个单词会间接看到自己，这让目标训练变得像作弊。 为了解决这个问题，BERT 采用了 MLM 来解决，具体就是训练时随机遮蔽 15% 的单词（用 MASK 标记替换），训练目标为猜测这个被遮蔽的单词到底是什么。但是微调过程不会出现 MASK 标记，所以这种遮蔽导致了预训练和微调的不匹配。为了解决这个问题，BERT 针对每个被随机选中的词采用下述三种方式之一进行处理： • （1）80% 几率被遮蔽（模拟缺失）； • （2）10% 几率被替换为其它无关词（模拟噪音）； • （3）10% 几率保留原词不变。 6.1.2 NSP 任务许多重要的的下游任务，比如 QA（question answering，问答） 和 NLI（natural language inference，自然语言推理），都需要理解两个句子之间的关系，而这无法通过语言模型直接捕获。为了解决这个问题，BERT 在预训练阶段加入了二值化 NSP 任务：针对我们选择的一对句子，句子 A 和 句子 B： • 句子 B 50% 几率是句子 A 的真正后缀，标记为 IsNext。 • 句子 B 50% 几率是随机选择的并不跟随句子 A，标记为 NotNext。 然后 BERT 针对这两个标记分类进行预训练，从而学习到句子 A 和句子 B 之间的关系。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:6:1","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#61-bert预训练过程"},{"categories":null,"content":" 6.1 BERT 预训练过程BERT 预训练与微调架构如下图所示，它将预训练和微调的架构统一了起来。 客观地讲， BERT 在网络架构上没有创新，它与 GPT 类似都是沿用了 Transformer，它的效果拔群源自其在预训练期间采用的两个无监督任务：MLM（masked language model，掩码语言模型）与 NSP（next sentence prediction，下个句子预测）。前者用于挖掘句子内单词间的关系，后者用于挖掘句子间的关系。 6.1.1 MLM 任务标准的条件语言模型只能通过从左到右或者从右到左的方式来训练，否则每个单词会间接看到自己，这让目标训练变得像作弊。 为了解决这个问题，BERT 采用了 MLM 来解决，具体就是训练时随机遮蔽 15% 的单词（用 MASK 标记替换），训练目标为猜测这个被遮蔽的单词到底是什么。但是微调过程不会出现 MASK 标记，所以这种遮蔽导致了预训练和微调的不匹配。为了解决这个问题，BERT 针对每个被随机选中的词采用下述三种方式之一进行处理： • （1）80% 几率被遮蔽（模拟缺失）； • （2）10% 几率被替换为其它无关词（模拟噪音）； • （3）10% 几率保留原词不变。 6.1.2 NSP 任务许多重要的的下游任务，比如 QA（question answering，问答） 和 NLI（natural language inference，自然语言推理），都需要理解两个句子之间的关系，而这无法通过语言模型直接捕获。为了解决这个问题，BERT 在预训练阶段加入了二值化 NSP 任务：针对我们选择的一对句子，句子 A 和 句子 B： • 句子 B 50% 几率是句子 A 的真正后缀，标记为 IsNext。 • 句子 B 50% 几率是随机选择的并不跟随句子 A，标记为 NotNext。 然后 BERT 针对这两个标记分类进行预训练，从而学习到句子 A 和句子 B 之间的关系。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:6:1","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#611-mlm任务"},{"categories":null,"content":" 6.1 BERT 预训练过程BERT 预训练与微调架构如下图所示，它将预训练和微调的架构统一了起来。 客观地讲， BERT 在网络架构上没有创新，它与 GPT 类似都是沿用了 Transformer，它的效果拔群源自其在预训练期间采用的两个无监督任务：MLM（masked language model，掩码语言模型）与 NSP（next sentence prediction，下个句子预测）。前者用于挖掘句子内单词间的关系，后者用于挖掘句子间的关系。 6.1.1 MLM 任务标准的条件语言模型只能通过从左到右或者从右到左的方式来训练，否则每个单词会间接看到自己，这让目标训练变得像作弊。 为了解决这个问题，BERT 采用了 MLM 来解决，具体就是训练时随机遮蔽 15% 的单词（用 MASK 标记替换），训练目标为猜测这个被遮蔽的单词到底是什么。但是微调过程不会出现 MASK 标记，所以这种遮蔽导致了预训练和微调的不匹配。为了解决这个问题，BERT 针对每个被随机选中的词采用下述三种方式之一进行处理： • （1）80% 几率被遮蔽（模拟缺失）； • （2）10% 几率被替换为其它无关词（模拟噪音）； • （3）10% 几率保留原词不变。 6.1.2 NSP 任务许多重要的的下游任务，比如 QA（question answering，问答） 和 NLI（natural language inference，自然语言推理），都需要理解两个句子之间的关系，而这无法通过语言模型直接捕获。为了解决这个问题，BERT 在预训练阶段加入了二值化 NSP 任务：针对我们选择的一对句子，句子 A 和 句子 B： • 句子 B 50% 几率是句子 A 的真正后缀，标记为 IsNext。 • 句子 B 50% 几率是随机选择的并不跟随句子 A，标记为 NotNext。 然后 BERT 针对这两个标记分类进行预训练，从而学习到句子 A 和句子 B 之间的关系。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:6:1","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#612-nsp任务"},{"categories":null,"content":" 6.2 BERT 预训练模型用于下游任务在传统语言模型中，处理文本对通常分为两个阶段：先独立处理每个句子，然后在某种形式上合并它们的信息。而 BERT 通过自注意力机制在一个统一框架下处理合并后的句子对。这意味着模型不仅能够在每个句子内部进行注意力分配，还能跨越两个句子进行注意力分配，即实现句子间的双向交叉注意力。这种机制使得模型能够更深入地理解两个句子之间的关系。这种统一框架使得 BERT 很容易和下游任务结合进行端到端全参数微调。 在输入阶段，预训练中的句子A和句子B类似于：(1) 在释义任务中的句子对，(2) 在蕴含关系判断任务中的假设-前提对，(3) 在问答任务中的问题-段落对，以及(4) 在文本分类或序列标记任务中的退化的文本-空对（即只有一个文本输入而没有第二个输入）。在输出阶段，标记的表示被送入一个输出层来处理标记级别的任务，如序列标记或问答；而[CLS]标记的表示则被送入另一个输出层来处理分类任务，如蕴含关系判断或情感分析。 在 BERT 中，根据任务的不同，模型的输出被用于不同的目的。具体来说，有两种主要的输出类型： Token级别的任务：对于需要在单词或标记（token）级别上进行预测的任务（如序列标记或问答系统），模型的输出是每个标记的表示（representation）。这意味着模型对输入文本中的每个单词或标记进行了编码，并产生了一个与之对应的向量表示。这些表示随后被输入到一个输出层，用于执行特定的任务。例如，在序列标记任务中，如命名实体识别，每个单词的输出表示用于判断它是否是一个实体，并确定实体的类型。 分类任务：对于需要进行整体判断或分类的任务（如文本蕴含判断或情感分析），模型使用特殊的[CLS]标记的表示。在BERT中，[CLS]是每个输入序列前加的一个特殊标记，其表示被模型设计为捕捉整个序列的综合信息。这个[CLS]标记的输出表示随后被输入到另一个输出层，用于执行分类任务。例如，在情感分析中，[CLS]的表示可以用来判断整个句子或文档的情感倾向。 总结来说，BERT模型根据任务的需求，利用不同的输出表示进行处理：对于标记级任务，使用每个标记的表示；对于分类任务，则使用[CLS]标记的表示。这种设计使得BERT能够灵活地应用于多种不同的自然语言处理任务。 ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:6:2","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#62-bert预训练模型用于下游任务"},{"categories":null,"content":" 7 总结本文描述了自然语言表示从 N-gram 到 BERT 的演进过程。其中对神经网络对自然语言的建模进行了深入描述，从 Bengio 到 word2vec 再到 Bert，每一步的演进都对技术的进步提供了巨大的推动作用。 –end– ","date":"2023-11-19","objectID":"/natural-language-computability-evolution/:7:0","series":null,"tags":["NLP","embedding","computability"],"title":"自然语言的可计算性：从 N-gram 到 BERT","uri":"/natural-language-computability-evolution/#7-总结"},{"categories":null,"content":" TL;DR: 本文介绍了复杂性的概念，以及如何应对复杂性的方法。复杂性是软件工程和团队管理的核心问题，应对复杂性的能力也是软件工程师的一项核心竞争力。本文综合了业界在复杂性定义和应对上的（部分）最佳理论研究和优秀实践，旨在说明软件工程师应该客观看待复杂性、接受复杂性、主动降低目标系统的复杂性，同时提升自己应对复杂性的能力。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:0:0","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#"},{"categories":null,"content":" 0 前言在软件工程领域，“四人帮”（Gang of Four）提出了二十三个设计模式，Robert C. Martin 提出了 SOLID 法则，还有其它软件工程师耳熟能详的如 KISS 原则（Keep It Simple, Stupid）、YAGNI 原则（You Ain’t Gonna Need It）、DRY 原则（Don’t Repeat Yourself）、控制反转（Inversion of Control, IoC）等等。这些原则或者方法对于没多少编程经验的工程师来讲是相当枯燥无聊的，但对有经验的工程师来说却甘之如饴。这两类工程师在实践上的差异就在于是否处理过有较高复杂性的项目。 以上提到的原则和方法都是为了应对软件工程中的复杂性而提出的。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:1:0","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#0-前言"},{"categories":null,"content":" 1 难 ≠ 复杂难和复杂是对一件事在两个维度上的表述，前者偏纵向（单点深度）、偏线性（可拆解，可分而治之），后者偏网状（立体交织）、偏非线性（无法拆解，按下葫芦浮起瓢）。 作为一个工程师，前期可能主要集中在从易到难解决问题，随着职业发展逐渐接触从简单到复杂的问题。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:2:0","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#1-难--复杂"},{"categories":null,"content":" 1.1 复杂性的定义复杂系统是由大量组分组成的网络，不存在中央控制，通过简单运作规则产生出复杂的集体行为和复杂的信息处理，并通过学习和进化产生适应性。（梅拉妮·米歇尔，2008） 这个定义比较正式和抽象，换句话讲，就是一个系统包含多个单元，各个单元之间不是通过简单叠加而构成系统，而是通过互相传递信息交织在一起，从而产生复杂的行为。 简单一句话讲，复杂就是整体大于部分之和。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:2:1","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#11-复杂性的定义"},{"categories":null,"content":" 1.2 软件工程领域的复杂性软件工程领域的复杂性贯穿整个生命周期： 需求复杂性：软件工程中需求的不明确、变动、冲突等问题，以及需求与实际系统的对应关系。 设计和架构的复杂性：在设计软件时需要考虑的因素很多，比如性能、可扩展性、可维护性等，而且需要将这些因素融入到软件的架构中，这个过程很复杂。 代码的复杂性：软件工程中代码的复杂性，包括代码的结构、逻辑、依赖关系等，代码的复杂性直接影响到软件的质量和维护成本。 技术的复杂性：在软件工程中涉及到的技术栈、工具、平台等的复杂性，不同的技术之间可能存在冲突，需要选择合适的技术来满足需求。 人员和团队的复杂性：软件工程中涉及的人员多，团队之间需要协作，而且人员的技能、经验、思维方式等都不同，这些都增加了工程的复杂性。 项目管理的复杂性：软件工程中涉及的时间、成本、质量、风险等的管理复杂性，需要合理分配资源，确保项目按时按质完成。 运维和部署的复杂性：在软件部署到生产环境后，需要考虑的因素，如服务器的配置、软件的运维、故障恢复等，这些都增加了软件工程的复杂性。 除了这些，还有因为不同类型客户、各级领导意志与软件系统交织所带来的巨大复杂性。 工程师如何应对这些复杂性呢？可喜的是，在软件工程诞生之前的二十世纪中叶，各路科学家就已经在各个领域遇到了类似的问题并且产生了三大横断学科（系统论、控制论、信息论），相关学科理论的融合为该问题的解决指明了方向。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:2:2","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#12-软件工程领域的复杂性"},{"categories":null,"content":" 2 复杂性的一般性应对","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:3:0","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#2-复杂性的一般性应对"},{"categories":null,"content":" 2.1 必要多样性法则及其理论基础二十世纪中叶，英国精神科医生和系统论的先驱威廉·罗斯·阿什比（W. Ross Ashby，1903年9月6日—1972年11月15日）提出了一个应对复杂性的定性描述，即必要多样性法则（The Law of Requisite Variety）：控制系统的复杂性至少等于目标系统的复杂性，否则无法胜任。换句话说，对于能够完全控制系统 B 的系统 A，必须至少与系统 B 一样复杂。 必要多样性法则又叫阿什比定律。 阿什比用信息熵的性质来说明必要多样性法则的合理性。他在《控制论导论》(An Introduction to Cybernetics) 中将系统的多样性用信息熵来量化。 熵是信息论中的一个重要概念，它用来量化信息的不确定性。对于一个随机变量 $X$，其熵定义为： $$H(X)=−∑p(x)logp(x)$$ 其中，$p(x)$ 是 $X$ 取值为 $x$ 的概率。（学过哈夫曼（Huffman）编码的同学应该对这个公式不陌生，该公式是哈夫曼编码的理论基础。） 阿什比利用熵来量化系统的多样性。系统的多样性越高，其熵也越高，表示系统的状态更加不确定，需要更多的信息来描述。必要多样性法则说的是，为了实现对系统的有效控制，控制器的多样性必须大于或等于系统的多样性。这可以用熵的性质来说明。根据信息论的基本原理，信息的传递可以消除不确定性$^{注1}$。如果控制器的多样性（熵）小于系统的多样性（熵），那么控制器的信息不足以消除系统的不确定性，因此无法实现对系统的有效控制。相反，如果控制器的多样性（熵）大于或等于系统的多样性（熵），那么控制器的信息就足以消除系统的不确定性，从而实现对系统的有效控制。 对应到软件工程领域，目标系统即软件工程，控制系统即工程师团队。 注1： 假设你在玩一个猜数字的游戏，范围是从 1 到 100。每次你猜一个数字，我会告诉你是对的还是错的。在游戏开始时，你对正确答案是完全不确定的，因为它可以是 1 到 100 中的任何一个数字。但随着你的猜测和我给你的反馈，你的不确定性逐渐减少，直到最后你找到正确答案，不确定性消除。 这个例子中，我的反馈就是信息的传递，它帮助你消除了对正确答案的不确定性。这就是\"信息的传递可以消除不确定性\"的意义。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:4:0","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#21-必要多样性法则及其理论基础"},{"categories":null,"content":" 2.2 复杂性的经济支柱模型该模型由混沌工程发明人改编自 Kent Beck（极限编程发明人） 在一篇文章（《Taming Complexity with Reversibility》）中提出的模型$^{注2}$。 该模型有四个支柱： 状态（产品功能、应用程序的数据、工程师团队等） 关系（状态之间的关系） 环境（如软件所属的商业环境） 可逆性（软件的可逆性指的是采用小步快跑方式收集反馈进而演进软件） 该模型指出，一个组织控制其中某个支柱的程度，反映出该组织可以应对竞争性生产过程的复杂性的成功程度。 大多数商业目标都鼓励增加“状态”的数量，因为这样可以增加产品的功能，从而增加产品的价值。而且大部分组织面对任务或功能增加时的第一反应也是招人加大团队规模，这会进一步增加系统的状态数量。但是，增加“状态”的数量会增加“关系”的数量（可以看作有向完全图，极端情况下关系增长率是 $O(n^2)$）从而增加“环境”的复杂性。“环境”一般只能适应无法被普通工程师影响。 这里我们重点说说可逆性，这也是工程师可以左右的一点。不同于硬件，只要不采用“瀑布式”研发流程，软件的可逆性是可以保证的。软件的可逆性可以通过小步快跑的方式来实现，即通过小的改动来收集反馈，进而演进软件。这种方式可以有效降低软件工程的复杂性。 注2： 参见《混沌工程》P33. ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:4:1","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#22-复杂性的经济支柱模型"},{"categories":null,"content":" 3 如何应对软件工程的复杂性在认识复杂性之后，不同领域的实践为我们指出了三个应对方向：一是降低软件系统的复杂性，二是提升工程师团队的复杂性，三是提升软件的可逆性。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:5:0","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#3-如何应对软件工程的复杂性"},{"categories":null,"content":" 3.1 降低目标系统的复杂性Frederic Brooks 在其论文《没有银弹》$^{注3}$中提出了软件工程的两种复杂性：本质复杂性（Essential Complexity）和偶然复杂性（Accidental Complexity）。 本质复杂性：指由于软件系统要满足的需求本身就是复杂的，因此在软件工程中不可避免地存在的复杂性。本质复杂性是与软件的功能和性能需求直接相关的复杂性，是不可消除的。 偶然复杂性：指在软件工程过程中由于技术、工具、人员等外在因素导致的复杂性。它是可以通过改进技术、工具、流程等方式来减少或消除的复杂性。 在本质复杂性降低方面，工程师不能只坐在“后面”编程，需要向前多走 5%$^{注4}$ 接触市场和客户，更好地理解需求从而降低本质复杂性。正如《100x Software Engineering》$^{注5}$一文所述：百倍工程师可以在功能定义上推倒重来；他们能够删减功能中合适的 5%，从而将功能的实现简化 100 倍。 在偶然复杂性降低方面，需要工程师有宽广的技术视野和扎实的工程能力$^{注6}$，能够选择合适的技术和工具，以及设计合理的架构，从而降低偶然复杂性。 除此之外，《原则》（瑞·达利欧，2017）在“打造良好的文化”一节中将“极度求真和极度透明”作为核心原则，《不拘一格——网飞的自由与责任工作法》（里德·哈斯廷斯等，2020）将“实现最高坦诚度”作为企业文化的核心要素。“简单真诚”应该是成就卓越组织的必备特质。如果工程师团队能够贯彻“简单真诚”的价值观，也可以显著降低协作时候的偶然复杂性。“简单真诚”可以提高团队的沟通效率，减少不必要的误解和冲突，使团队成员能够更专注于解决实际问题。“简单真诚”的价值观也有助于简化流程和方法，使得目标更清晰。正如“康威定律”所指出的“设计系统的架构受制于产生这些设计的组织的沟通结构。”，简单真诚的组织结构也会映射到软件系统的设计中，从而降低项目的整体复杂性。 注3：参见《No Silver Bullet–Essence and Accident in Software Enginerring》，IFIP 第十届世界计算会议论文集。 注4: 该数值是业务群前研发总经理于 2017 年对研发团队提出的基本要求之一，该要求既进取又克制，贯彻至今。 注5: 参见《100x Software Engineering》，https://betterstack.com/community/blog/100x-software-engineering/ 注6: BG 公共研发部总经理于 2021 年在技术委员会上提议加强 CBG 工程师的这两项能力，并由我和其他几名同事一起推动落实，具体为每季度一次的技术视野研讨会和每月一次的工程力提升课程。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:5:1","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#31-降低目标系统的复杂性"},{"categories":null,"content":" 3.2 提升控制系统的复杂性控制系统即工程师团队，下面从人才密度和内部开源两个方面阐述如何提升工程师团队的复杂性。 3.2.1 人才密度在《不拘一格——网飞的自由与责任工作法》一书中提到了“人才密度”的概念。该书对人才的定义为：具有超凡的创新能力，能够完成繁重的任务，并能很好地相互协作。 人才密度是指在一个团队中，具有这种能力的人的比例。人才密度越高，团队的创新能力越强，团队的复杂性也就越高，从而更好地应对软件系统的复杂性。 吸引招纳更多的优秀人才，意味着组织主动升维同时提升自我的多样性，这对组织活性提升和进化是极其有益的。《不拘一格》认为“人才密度的增速要快于企业复杂程度的增速”，这与我们在这里讨论的主题高度一致。 大模型的崛起让我们见证了其在知识获取以及编程辅助方面的威力。大模型几乎打穿了从技术培训、架构设计、代码编写与测试到运维部署的各个环节。拥有大模型相当于拥有了一个人才密度极高的团队，大模型值得引进到所有的工程师团队。 3.2.2 内部开源在《大教堂与集市》一书中，Eric Raymond 剖析 Linux 成功因素时，提到了一个有趣的效应：一群专家（或一群无知的家伙）的平均观点要比一个随机选择的人的观点更有预见性。这种现象被称为“德尔菲效应”$^{注7}$。 最近几十年，开源运动风起云涌，整个互联网的基石都是建立在开源软件的基础之上。它对商业和技术的贡献无需赘言。 内部开源就是为了让更多优秀的人参与到软件的开发中来（Google 是内部开源的典范，几乎全部代码对全员可见。$^{注8}$），从而提升软件的质量和可靠性。内部开源可以间接提升项目团队的复杂性，从而更好地应对软件系统的复杂性。（之所以选择“内部”，只是为了保密和避免一些不必要的法律风险。） 所谓“只要眼睛多，bug 容易捉”$^{注9}$。 注7: 该方法是在 20 世纪 40 年代由赫尔默(Helmer)和戈登(Gordon)首创。1946 年，美国兰德公司为避免集体讨论存在的屈从于权威或盲目服从多数的缺陷，首次用这种方法用来进行定性预测，后来该方法被迅速广泛采用。20 世纪中期，当美国政府执意发动朝鲜战争的时候。兰德公司又提交了一份预测报告，预告这场战争必败。政府完全没有采纳，结果一败涂地。从此以后，德尔菲法得到广泛认可。 注8: 《Why Google Stores Billions of Lines of Code in a Single Repository》，https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext 注9: 《大教堂与集市》，P31. ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:5:2","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#32-提升控制系统的复杂性"},{"categories":null,"content":" 3.2 提升控制系统的复杂性控制系统即工程师团队，下面从人才密度和内部开源两个方面阐述如何提升工程师团队的复杂性。 3.2.1 人才密度在《不拘一格——网飞的自由与责任工作法》一书中提到了“人才密度”的概念。该书对人才的定义为：具有超凡的创新能力，能够完成繁重的任务，并能很好地相互协作。 人才密度是指在一个团队中，具有这种能力的人的比例。人才密度越高，团队的创新能力越强，团队的复杂性也就越高，从而更好地应对软件系统的复杂性。 吸引招纳更多的优秀人才，意味着组织主动升维同时提升自我的多样性，这对组织活性提升和进化是极其有益的。《不拘一格》认为“人才密度的增速要快于企业复杂程度的增速”，这与我们在这里讨论的主题高度一致。 大模型的崛起让我们见证了其在知识获取以及编程辅助方面的威力。大模型几乎打穿了从技术培训、架构设计、代码编写与测试到运维部署的各个环节。拥有大模型相当于拥有了一个人才密度极高的团队，大模型值得引进到所有的工程师团队。 3.2.2 内部开源在《大教堂与集市》一书中，Eric Raymond 剖析 Linux 成功因素时，提到了一个有趣的效应：一群专家（或一群无知的家伙）的平均观点要比一个随机选择的人的观点更有预见性。这种现象被称为“德尔菲效应”$^{注7}$。 最近几十年，开源运动风起云涌，整个互联网的基石都是建立在开源软件的基础之上。它对商业和技术的贡献无需赘言。 内部开源就是为了让更多优秀的人参与到软件的开发中来（Google 是内部开源的典范，几乎全部代码对全员可见。$^{注8}$），从而提升软件的质量和可靠性。内部开源可以间接提升项目团队的复杂性，从而更好地应对软件系统的复杂性。（之所以选择“内部”，只是为了保密和避免一些不必要的法律风险。） 所谓“只要眼睛多，bug 容易捉”$^{注9}$。 注7: 该方法是在 20 世纪 40 年代由赫尔默(Helmer)和戈登(Gordon)首创。1946 年，美国兰德公司为避免集体讨论存在的屈从于权威或盲目服从多数的缺陷，首次用这种方法用来进行定性预测，后来该方法被迅速广泛采用。20 世纪中期，当美国政府执意发动朝鲜战争的时候。兰德公司又提交了一份预测报告，预告这场战争必败。政府完全没有采纳，结果一败涂地。从此以后，德尔菲法得到广泛认可。 注8: 《Why Google Stores Billions of Lines of Code in a Single Repository》，https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext 注9: 《大教堂与集市》，P31. ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:5:2","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#321-人才密度"},{"categories":null,"content":" 3.2 提升控制系统的复杂性控制系统即工程师团队，下面从人才密度和内部开源两个方面阐述如何提升工程师团队的复杂性。 3.2.1 人才密度在《不拘一格——网飞的自由与责任工作法》一书中提到了“人才密度”的概念。该书对人才的定义为：具有超凡的创新能力，能够完成繁重的任务，并能很好地相互协作。 人才密度是指在一个团队中，具有这种能力的人的比例。人才密度越高，团队的创新能力越强，团队的复杂性也就越高，从而更好地应对软件系统的复杂性。 吸引招纳更多的优秀人才，意味着组织主动升维同时提升自我的多样性，这对组织活性提升和进化是极其有益的。《不拘一格》认为“人才密度的增速要快于企业复杂程度的增速”，这与我们在这里讨论的主题高度一致。 大模型的崛起让我们见证了其在知识获取以及编程辅助方面的威力。大模型几乎打穿了从技术培训、架构设计、代码编写与测试到运维部署的各个环节。拥有大模型相当于拥有了一个人才密度极高的团队，大模型值得引进到所有的工程师团队。 3.2.2 内部开源在《大教堂与集市》一书中，Eric Raymond 剖析 Linux 成功因素时，提到了一个有趣的效应：一群专家（或一群无知的家伙）的平均观点要比一个随机选择的人的观点更有预见性。这种现象被称为“德尔菲效应”$^{注7}$。 最近几十年，开源运动风起云涌，整个互联网的基石都是建立在开源软件的基础之上。它对商业和技术的贡献无需赘言。 内部开源就是为了让更多优秀的人参与到软件的开发中来（Google 是内部开源的典范，几乎全部代码对全员可见。$^{注8}$），从而提升软件的质量和可靠性。内部开源可以间接提升项目团队的复杂性，从而更好地应对软件系统的复杂性。（之所以选择“内部”，只是为了保密和避免一些不必要的法律风险。） 所谓“只要眼睛多，bug 容易捉”$^{注9}$。 注7: 该方法是在 20 世纪 40 年代由赫尔默(Helmer)和戈登(Gordon)首创。1946 年，美国兰德公司为避免集体讨论存在的屈从于权威或盲目服从多数的缺陷，首次用这种方法用来进行定性预测，后来该方法被迅速广泛采用。20 世纪中期，当美国政府执意发动朝鲜战争的时候。兰德公司又提交了一份预测报告，预告这场战争必败。政府完全没有采纳，结果一败涂地。从此以后，德尔菲法得到广泛认可。 注8: 《Why Google Stores Billions of Lines of Code in a Single Repository》，https://cacm.acm.org/magazines/2016/7/204032-why-google-stores-billions-of-lines-of-code-in-a-single-repository/fulltext 注9: 《大教堂与集市》，P31. ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:5:2","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#322-内部开源"},{"categories":null,"content":" 3.3 提升软件的可逆性对于软件的可逆性的重大改进始于极限编程（Extreme Programming, XP）和敏捷开发（Agile Development）。 瀑布式方法涉及前期规划、设计和冗长的交付过程。如果采用瀑布式的开发方式，长达一年的开发后，产品可能被用户否掉，木已成舟无法逆转。然而 XP 中的短迭代做法可以在数周之内就能将 MVP（最小可行性产品）摆在用户面前。如果用户不喜欢可以将其抛弃，逆转设计决策下一次迭代将更贴近用户的实际需求。 这种“早发布，常发布”的做法可以大幅提升软件的可逆性。通过小的改动来收集反馈，进而演进软件。这种方式可以有效应对软件开发的复杂性。 ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:5:3","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#33-提升软件的可逆性"},{"categories":null,"content":" 4 总结本文尝试厘清软件工程中的“难”与“复杂性”，介绍了二十世纪中叶迄今各位科学家提出的应对系统复杂性的理论，并结合其它领域的实践提出了若干应对软件工程复杂性的方法，如提升人才密度、保持简单真诚的价值观、内部开源、将大模型引入到工程师团队以及小步快跑的迭代模式。 –end– ","date":"2023-10-22","objectID":"/complexity-and-how-to-deal-with-it/:6:0","series":null,"tags":["思考","复杂"],"title":"复杂性及其应对：当我们在谈论软件工程的时候，我们在谈论什么","uri":"/complexity-and-how-to-deal-with-it/#4-总结"},{"categories":null,"content":"[toc] 本人身处广告营销领域，广告系统本质是个推荐系统，本文旨在梳理相关推荐算法。《深度学习推荐系统》是一本很不错的入门书籍，很系统，不过很多地方写得不太清楚，阅读过程查阅了大量资料。这篇文章作为开篇，会写一个系列，目的是将推荐领域相关核心概念和算法全覆盖。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:0:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#"},{"categories":null,"content":" 0 推荐算法四要素如果把推荐系统看作一个函数，可以表达为 $F(i,c,u)$，其中 $i$ 为 item 即待推荐物品，$c$ 为 context 即当前上下文（时间、地理等），$u$ 为 user 即用户，$F$ 记为推荐算法。 整个算法演进就在围绕这四个要素做文章。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:1:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#0-推荐算法四要素"},{"categories":null,"content":" 1 协同过滤（Collaborative Filtering, CF）推荐领域最经典的算法之一是协同过滤，该算法是 Xerox 发明，后在 2003 年被 Amazon 发扬光大。该算法将用户和物品构成一个大矩阵，每行是一个用户对每个物品的态度（喜欢 or 不喜欢），这个矩阵就能刻画每个用户对物品的喜好程度，该矩阵成为共现矩阵。 该算法分为 User-Based CF 和 Item-Based CF。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:2:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#1-协同过滤collaborative-filtering-cf"},{"categories":null,"content":" 1.1 User-based CF站在用户角度，针对用户 A，计算与其喜好相似的 topN 用户；然后遍历每个物品，让这些与自己相似的用户针对这个物品投票，计算出每个物品得分；最后按照每个物品得分排序，排名最高的几个物品就可以推荐给用户了。这里有两个关键点： 用户相似度计算，可以取出每个用户在共现矩阵对应的行向量，与目标用户向量计算余弦相似度，找出 topN。 物品得分计算 $Ru,i=\\frac{\\sum(w_{u,s}R_{s,i})}{\\sum{w_{u,s}}}$，其中 $w_{u,s}$ 是用户 u 和 s 的相似度，$R_{s,i}$ 是用户 s 对物品 i 的喜好程度作为投票的权重，$Ru,i$ 即为用户 u 对物品 i 的喜好程度。 获得 u 对全部物品喜好后，排序即可得推荐列表。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:2:1","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#11-user-based-cf"},{"categories":null,"content":" 1.2 Item-based CF站在物品角度，沿用前面的共现矩阵，这次从列角度思考，计算两两列向量的相似度，得到一个对称方阵（只看上三角或下三角即可）。 当一个用户进站访问时，在共现矩阵里查看其对应的行向量，确定其之前表达过喜好的物品，然后针对每个喜欢过的物品到上面物品相似度矩阵查找最相似的 topK 物品，然后对这 K 个物品计算喜好得分，计算公式为 $R_{u,i}=\\sum{w_{j,i}R_{u,j}}$，其中 i 是要确定用户喜好程度的物品， j 是用户 u 表达喜欢过的物品，$w_{j,i}$ 是物品 j 和 i 相似度，$R_{u,j}$ 是用户 u 对 j 的喜好程度。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:2:2","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#12-item-based-cf"},{"categories":null,"content":" 1.3 User-based CF 和 Item-based CF 比较 User-based CF 适合社交类场景，朋友喜欢的自己大概率也喜欢，很符合直觉。但是一般这类场景用户多余物品而且持续增长的话，由于每次都要计算相似用户所以共现矩阵存储和维护是个问题；另外就是针对获取难的物品，比如价格较贵的酒店、奢侈品等，会导致用户对应的向量很稀疏。 Item-based CF 适合兴趣较为稳定的场景，比如电商和电影，用户一段时间内倾向寻找某几类爱好的物品。业界 item-based CF 用得较多。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:2:3","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#13-user-based-cf-和-item-based-cf-比较"},{"categories":null,"content":" 1.4 优势CF 非常直观，可解释强。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:2:4","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#14-优势"},{"categories":null,"content":" 1.5 劣势 CF 泛化能力差，只用了用户和物品信息，上下文信息未用起来。另外因为它无法将两个物品相似这一信息推广到其它物品相似度计算上，两两相似性无法传递。 热门物品（大家都喜欢的物品）具有很强的头部效应，尾部物品由于向量稀疏，即使相似度很高但是无法识别出来，最后算出来的最相近物品都是热门物品。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:2:5","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#15-劣势"},{"categories":null,"content":" 2 矩阵分解","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:3:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#2-矩阵分解"},{"categories":null,"content":" 2.1 背景为了解决 CF 泛化能力差、无法推广物品相似度的问题，矩阵分解技术被提出，2006 年在 netflix 举办的算法竞赛中大放异彩。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:3:1","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#21-背景"},{"categories":null,"content":" 2.2 算法原理针对共现矩阵 $A_{mxn}$，通过矩阵分解技术（奇异值分解 svd、梯度下降等）得到两个矩阵之积： $$A_{mxn}=U_{m,k}*V_{k,n}$$ 其中 U 为用户矩阵，每一行代表一个用户隐向量； V 为物品矩阵，每一列代表一个物品隐向量；k 是一个超参数，越小隐向量表达能力越弱但则泛化能力越强。分解后得到的两个矩阵基于整个共现矩阵分解得到，不再稀疏。 由于奇异值分解适用于稠密矩阵，但是推荐场景下一般矩阵都非常稀疏所以更常用的是梯度下降，梯度是怎么引入的呢？ 我们知道共现矩阵用户 u 针对物品 i 有个喜好值记为 $R_{u,i}$，矩阵分解后（假设已经分解完成了）的用户矩阵对应用户 u 的隐向量记为 $X_u$，物品矩阵对应物品 i 的隐向量记为 $Y_i$，则我们期望 $$X_uY_{i}=R_{u,i}$$ 这个只是期望，左边能尽可能逼近右边我们在工程上就满足了。针对全部用户 u, i 我们可以得到针对全局的最优期望公式即均方误差为： $$min\\sum{(r_{u,i}-X_uY_i)^2}$$ 基于上述公式针对 X 和 Y 求导，然后沿着梯度下降方向迭代 X 和 Y 即可。工程实现上为了避免过拟合，会在上述均方误差公式后面增加一个正则化项，关于正则化是个大话题先不展开，记得它存在目的是为了减少过拟合即可。 得到用户矩阵和物品矩阵后，针对每个用户，计算它和每个用户向量的内积即可得到喜好程度，排序即可地推荐列表。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:3:2","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#22-算法原理"},{"categories":null,"content":" 2.3 优势 相比 CF 泛化能力更强，一定程度解决了数据稀疏问题。 分解得到的用户隐向量和物品隐向量其实是一种变相的 embedding，这个结果便于和其它特征进行组合可以与深度学习无缝拼接，后面介绍深度学习类推荐算法时候会重提这一点。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:3:3","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#23-优势"},{"categories":null,"content":" 2.4 劣势同 CF 一样，只用到了用户和物品两项信息，上下文没有利用起来。从下个算法开始将会重点解决特征利用不充分问题。 过渡说明 接下来描述的每个算法的输入都是一个样本矩阵，其中每一行为一个样本，每一列为一个特征维度；输出是一个概率值，作为点击率。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:3:4","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#24-劣势"},{"categories":null,"content":" 3 LR（logistic regression）","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:4:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#3-lrlogistic-regression"},{"categories":null,"content":" 3.1 背景Logistic 回归最早是在 20 世纪 50 年代为了解决生物统计问题而被提出的。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:4:1","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#31-背景"},{"categories":null,"content":" 3.2 算法原理公式如下： $$\\widehat{y}=sigmoid(\\sum{w_ix_i})=\\frac{1}{1+e^{-\\sum{w_ix_i}}}$$ 公式说明： $\\widehat{y}$ 为针对某个样本的预测输出，$x_i$ 为前述样本的第 $i$ 特征取值，$w_i$ 为该特征对应的权重。需要说明的有两点： $x_i$ 为标量 $w_i$ 也为标量，它的值为学习对象 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:4:2","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#32-算法原理"},{"categories":null,"content":" 3.3 优势 输出就是一个 $(0, 1)$之间的数，符合直觉，可解释性强。 工程化简单，尤其针对海量数据。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:4:3","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#33-优势"},{"categories":null,"content":" 3.4 劣势 仅能捕获特征之间的线性关系，不能很好地表示特征之间的交互。 为了改善预测效果，需要做大量的手工特征工程。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:4:4","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#34-劣势"},{"categories":null,"content":" 4 Poly2（Polynomial Regression of degree 2）","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:5:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#4-poly2polynomial-regression-of-degree-2"},{"categories":null,"content":" 4.1 背景为了捕获特征之间的交互关系，业界引入了 Poly2 算法。 多项式核函数（包括二次多项式核）在支持向量机（SVM）的发展过程中变得非常重要。SVM 最初由 Vladimir Vapnik 和 Alexey Chervonenkis 在1960年代末到1970年代初提出。多项式核函数并不仅限于二次形式，也可以有更高阶的形式，但二次多项式核（Poly2）在许多应用中是很实用的。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:5:1","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#41-背景"},{"categories":null,"content":" 4.2 算法原理公式如下： $$\\widehat{y_i}=sigmoid(w_0+\\sum{w_ix_i}+\\sum\\sum{w_{ij}x_ix_j})$$ 公式说明： 相比 LR 增加了一个二阶项，这个部分负责进行特征交叉。其中： $w_i$、$x_i$ 同 LR 算法说明。 $w_{ij}$ 是一个新增加的待学习标量参数，它负责学习 $x_i$ 和 $x_j$ 之间的交互关系。 $x_ix_j$ 可以看作两个特征之间的哈达玛乘积。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:5:2","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#42-算法原理"},{"categories":null,"content":" 4.3 优势 能够捕获特征之间的二阶交互。 通过特征交互，可以建模非线性关系。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:5:3","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#43-优势"},{"categories":null,"content":" 4.4 劣势 参数多计算复杂度高 $O(d^2)$，其中 $d$ 是特征个数。 由于参数太多，在数据稀疏场景下容易过拟合。 只能捕获二阶交互，更高阶的无法捕获。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:5:4","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#44-劣势"},{"categories":null,"content":" 5 FM（factorization machines）","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:6:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#5-fmfactorization-machines"},{"categories":null,"content":" 5.1 背景为了解决 poly2 在稀疏数据集容易过拟合以及计算量过大的问题而提出。 这个算法是由 Steffen Rendle 在2010年左右提出的。它融合了矩阵分解（例如SVD）和线性回归模型的特性，以有效地处理高维稀疏数据和捕捉特征之间的交互。 公式如下 $$\\widehat{y_i}=sigmoid(w_0+\\sum{w_ix_i}+\\sum\\sum{\u003cv_i,v_j\u003ex_ix_j})$$ 公式说明 形式同 poly2，差别在二阶项。其中： $w_i$、$x_i$ 同 LR 算法说明。 $v_i$ 和 $v_j$ 是一个新增加的待学习向量参数，它们叫做隐向量，长度为 $k$，$k$ 也是一个超参数，大小远远小于特征个数 $d$。 $x_ix_j$ 可以看作两个特征之间的哈达玛乘积。 poly2 是学习任何两个特征值之间的交互关系，而 fm 在全样本中学习每个特征对应的隐向量，这使得 fm 有更好的泛化性，而且参数量大幅减小（从 $d^2$ 降为 $kd$）。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:6:1","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#51-背景"},{"categories":null,"content":" 5.2 优势 可以捕获任意阶数的特征交互，尽管在实践中通常只计算到二阶。 通过参数共享，避免了 poly2 参数过多的问题，同时让模型泛化性更好。 相比 poly2 在稀疏数据集上表现更好，具体来说就是针对某个特征，其它特征都为它的隐向量产生贡献了力量，而且这种力量在其它特征与自己交叉时共享。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:6:2","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#52-优势"},{"categories":null,"content":" 5.3 劣势 尽管计算复杂度相比 poly2 降低，但仍然较大，特别随着隐向量维度 $k$ 变大的时候。 多了一个超参数也就是 $k$ 的调整需求。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:6:3","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#53-劣势"},{"categories":null,"content":" 6 FFM（filed-awareness factorization machines）","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:7:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#6-ffmfiled-awareness-factorization-machines"},{"categories":null,"content":" 6.1 背景FFM 算法是对 FM 算法的扩展，从名字就能看出来。广告点击率预测是一个利益驱动的事情，在大体量下（比如谷歌一年收入万亿）点击率预估稍微提升几个点带来的收益巨大。 FFM 在 FM 基础上引入了域（filed）的概念，隐向量不在共建共用，而是针对每个交互特征独立构建了一个隐向量，这类似 poly2 不过这里用的向量而非标量权重。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:7:1","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#61-背景"},{"categories":null,"content":" 6.2 算法原理公式如下 $$\\widehat{y_i}=sigmoid(w_0+\\sum{w_ix_i}+\\sum\\sum{\u003cv_{i,\\text{filed}(j)},v_{j,\\text{filed}(i)}\u003ex_ix_j})$$ 公式说明 形式同 poly2 和 fm，差别在二阶项，ffm 的二阶项相当于在 fm 基础上融合了 poly2。下面详细解释： $w_i$、$x_i$ 同 LR 算法说明。 $v_{i,\\text{filed}(j)}$ 和 $v_{j,\\text{filed}(i)}$ 是一个新增加的待学习向量参数，它们类似 fm 中引入的隐向量，不过粒度更细化了。fm 中引入的隐向量，针对每个特征只有一个；而 ffm 相当于为每个特征学习了一组隐向量，而不是一个。这里相当于 poly2 为两两特征学习了独立的交互关系，不过不同于 poly2 只用一个标量权重，ffm 用的是向量。每个隐向量长度为 $k$，$k$ 也是一个超参数，大小远远小于特征个数 $d$。 $x_ix_j$ 可以看作两个特征之间的哈达玛乘积。 ffm 参数量为 $kd^2$，参数量和计算量都非常大。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:7:2","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#62-算法原理"},{"categories":null,"content":" 6.3 优势相比 fm 更加注重两两特征之间的独立关系 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:7:3","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#63-优势"},{"categories":null,"content":" 6.4 劣势 参数量和计算量以及存储开销相比 fm 增加非常多。 稀疏数据集容易过拟合。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:7:4","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#64-劣势"},{"categories":null,"content":" 7 LS-PLM（Large Scale Piece-wise Linear Model）","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:8:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#7-ls-plmlarge-scale-piece-wise-linear-model"},{"categories":null,"content":" 7.1 背景这个算法单纯是对 LR 算法的改进，不再像原始 LR 一样无差别应用，而是考虑到业务场景的差异化做得优化，这个优化思想在东方国家比较罕见。该算法在 2012 年便成为阿里巴巴内部最核心的 CTR 预估算法了。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:8:1","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#71-背景"},{"categories":null,"content":" 7.2 算法原理公式如下 $$p(y=1|x)=\\sum_{i=0}^{m}softmax(u_i x)LR(w_i x)=\\sum_{i=0}^{m}\\frac{e^{u_i x}}{\\sum_{j=0}^{m}{e^{u_i x}}}\\frac{1}{1+e^{-w_i x}}$$ 该算法简单说就是主观上认为样本应该可以分为若干类，但不是把每个样本只划分为到某一类中，而是认为每个样本属于哪个分类都是有一定概率的（这个在结构上有点 attention 的意思），针对样本在每个分类上都用该分类所属的参数算一下 LR，然后将各个分类的 LR 结果做加权平均（这个相当于每个类别投票，不过投票权利不同）。这个算法也被叫做 MLR（Mixed-LR），不过我觉得这个叫法不足以反映投票加权的本质。 公式中 $u_i$ 是分类 $i$ 的权重，$w_i$ 是分类 $i$ 的 LR 参数，$x$ 是样本特征向量。输出即为 $y=1$ 的概率。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:8:2","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#72-算法原理"},{"categories":null,"content":" 7.3 优势 端到端非线性学习。通过分类投票，可以捕获特征之间的非线性交互，不用人工设计特征交叉。 可以分布式并行。 稀疏性好，也就是非零参数少，这对在线推理系统很重要因为计算量小。稀疏性好的原因是在目标函数（本文未展示）上用了 $L_{2,1}$ 正则化。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:8:3","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#73-优势"},{"categories":null,"content":" 7.4 劣势相比后来的深度学习，该算法在更深层次的特征交叉以及序列特征使用上是不足的。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:8:4","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#74-劣势"},{"categories":null,"content":" 8.GBDT+LR","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:9:0","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#8gbdtlr"},{"categories":null,"content":" 8.1 背景为了提升模型效果，要么像前面的 lr、poly2、fm、ffm 一样进行手工或者半自动地进行特征组合和筛选，要么优化目标函数引入特征交叉项提升特征组合能力。 我们看到从 LR 到 FFM，这一路进化，核心目标函数并没有变化，进化只是表现在如何更好地进行特征交叉。 2014 年 Facebook 回到原点，提出了 GBDT+LR 组合算法模型。 GBDT（Gradient-Bootsted Discision Tree） 不会在本篇文章深入介绍，这个算法本质是用一堆决策树去逼近目标，但是要介绍下名字。我感觉能跟这个名字一拼的是 LSTM（Long short-term memory），后者好歹还有个短杠区分下 long short 分别修饰谁（short 修饰 term，long 修饰 short-term 即更长的短期），GBDT 前面的 GB 梯度提升比较误导人，算法本质是基于梯度下降寻找下个决策树（下个决策树拟合前序决策树们遗留的残差），这里的梯度下降用 G 表示，多个决策树一起协同用 B 表示。GBDT 是 Boosting 算法的一种，还有另一个流派 Bagging， 后续文章介绍。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:9:1","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#81-背景"},{"categories":null,"content":" 8.2 算法原理这个算法分为两步： 第一步先将原始样本输入 GBDT，这一步虽然也在做目标拟合，但新增了一个项目，就是将全部树的输出进行向量化然后串联起来作为新的特征（有点 embedding 的意思了）。 前一步每一颗树的叶子将被量化为 one-hot 向量（只有命中的叶子才是 1，其它叶子都是 0），然后依次将各个树对应的 one-hot 向量串联起来构成输入样本对应的 embedding 向量，该向量作为 LR 输入，后续同 LR 算法处理。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:9:2","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#82-算法原理"},{"categories":null,"content":" 8.3 优势 LR 的优势，符合直觉，计算简单。 GBDT 可以捕捉特征之间的交互关系，而且是自动找到这些交互，并且实现了非线性映射，这使得模型泛化能力更强。 ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:9:3","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#83-优势"},{"categories":null,"content":" 8.4 劣势 训练复杂性增加，先训练 gbdt 后训练 LR，这也增加了模型部署和维护复杂性。 推理速度相比单个模型可能较慢。 gbdt 训练复杂导致模型可能过拟合。 –end– ","date":"2023-09-12","objectID":"/rs-algo-pre-deeplearning/:9:4","series":null,"tags":["推荐系统","深度学习","机器学习"],"title":"推荐系统前深度学习时代：从协同过滤到 GBDT+LR","uri":"/rs-algo-pre-deeplearning/#84-劣势"},{"categories":null,"content":"我一直反思自己同家境更好更年轻的同辈人的差距，他们年纪不大为人处事做得就很好，心态乐观不怕失败，年纪轻轻就取得了相比同辈更好的成绩。最近两年我意识到，自己虽然十年前就参加工作了，但是由于工作环境单纯（同仁受教育程度相似人际关系较为单纯）自己并未较多接触“残酷”的社会活动。反而前两年做一些业务开拓的事情才发现社会“险恶”的一面，勉强算是步入社会。今天早上读一本书（美前国防部长的《学会领导》），一个词击中了我，就是社会化。我发现家境好的孩子，普遍社会化比较早，早早接触社会运作的真相并参与其中，这对成长很有帮助。 我查了一下发现“社会化”（英文是 “socialization”）确实是一个在社会学和发展心理学领域经常被讨论的概念： 社会化可以理解为一个过程，个体在其中学习并内化社会的规范、价值观和行为模式，从而能够适应和参与社会生活。 家境良好的孩子可能更早地被介绍到各种社交活动、文化体验和教育机会，这有助于他们更早地了解和适应社会的运作方式。此外，家庭环境、父母的教育方式、以及父母与外界的关系也会影响孩子的社会化过程。 社会化从出生就开始。最初，家庭是社会化的主要场所，随后学校和朋友圈逐渐成为重要的社会化环境。这也侧面反映了原生家庭以及学区的重要性。 早期的社会经验确实可以帮助孩子建立自信、提高社交技能、增强适应能力，并对社会有一个更全面的理解。作为父母应该意识到，当你结束一天工作，从社会回到家，孩子当天的社会化可能因为你的归来刚刚开始。对待孩子不能大吼大叫，对待家人也不能随心所欲，这一切交织对孩子的成长影响重大。你可能已经身心俱疲，但你最起码要做到“是亲三分客”。 尽管早期的社会化有其好处，但这并不意味着晚期社会化的人就无法取得成功或适应社会。如果现在才意识到，那就加快你的社会化进程！ – end– ","date":"2023-09-08","objectID":"/socialization/:0:0","series":null,"tags":["思考"],"title":"社会化：当我们在讨论原生家庭时，我们在讨论什么","uri":"/socialization/#"},{"categories":null,"content":"作为一个服务发现和配置管理基础设施, 一致性协议可以在集群内保证各类元数据的一致性. Nacos 提供了两个等级的一致性, CP 用于配置管理, AP 用于服务发现. 该系列分析基于源码 1.1.3 版本. 从 1.1.4 开始用 JRaft 替换了原生的 private-raft, 但我只对后者感兴趣所以延续使用 1.1.3 版本代码进行分析. 文章里涉及代码不多, 主要解释原理, 如果看注释可以点这里. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:0:0","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#"},{"categories":null,"content":" 1 开始之前在详细描述协议之前, 我们可以试想下, 如果我们自己设计一套一致性存储, 应该怎么来设计? 首先就是存储, 要支持以下几个操作 增-put 删-remove 改(覆盖)-put 查-get 其次是一致性 要在涉及变更时通知其它存储节点, 这个特性是修饰. 而通知根据一致性等级分为两种: 同步通知, 强一致性, 变更操作和通知逻辑耦合在一起; 异步通知, 最终一致性, 变更时发起后台通知, 确保最终一致. 类似地, 如何接收其它存储节点的通知也分为两种: 被动式, 开一个接口等着其它节点推送变更通知过来; 主动式, 定期轮询其它节点的接口查询变更. 最后一个问题, 一致性集群本身节点上下线如何处理 主动式: 基于配置, 每个节点定期扫配置, 发现有节点上下线则更新本地维护列表. 被动式: 啥也不管, 开放一个专门接口, 如果有节点上线自动 ping 一下收到就更新列表; 针对维护的列表, 自己定时 ping 一下对端同样接口, 根据是否响应判断存活. 以上几个问题贯穿接下来的系列文章. 好, 我们来看看 Nacos 是怎么做的. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:1:0","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#1-开始之前"},{"categories":null,"content":" 2 代码布局我们先看看确保一致性的代码位于何处. 如下图所示 Nacos 一致性协议代码整体位于 com.alibaba.nacos.naming.consistency 这个包下面. 其中 ephemeral 包下的 distro 协议是 AP 级别的一致性, 用于服务发现; persistent 包下的 raft 协议是 CP 级别的一致性, 用于配置管理. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:2:0","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#2-代码布局"},{"categories":null,"content":" 3 一致性协议接口设计 /** * 与实现无关的一致性服务接口，将一致性协议实现与业务逻辑解耦， * 用户也可以自己定义一套一致性协议实现, 只要提供这里要求的接口即可. */ public interface ConsistencyService { /** * 将一对 \u003ckey, value\u003e 写入 Nacos 集群 */ void put(String key, Record value) throws NacosException; /** * 根据 key 从 Nacos 集群移除相关数据 */ void remove(String key) throws NacosException; /** * 根据 {@code key} 从 Nacos 集群查询相关数据 */ Datum get(String key) throws NacosException; /** * 为与 key 对应的数据新增一个监听器，以监听 Nacos 集群中相关数据的变化 */ void listen(String key, RecordListener listener) throws NacosException; /** * 从与 key 对应的数据的监听器列表中移除指定的监听器 */ void unlisten(String key, RecordListener listener) throws NacosException; /** * 检查该一致性服务是否可用 */ boolean isAvailable(); } ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:3:0","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#3-一致性协议接口设计"},{"categories":null,"content":" 3.1 AP 级一致性协议 distro该级别协议用于服务发现, 服务发现典型的要求被发现的服务节点要和 nacos 保持连接, 所以都是 ephemeral 的. /** * 专用于临时(ephemeral)数据的一致性协议. * * 这个协议服务的数据不要求存在磁盘或数据库里, 因为临时数据会与 server * 保持一个 session, 只要 session 还活着, 临时数据就不会丢. * * 该协议要求写操作要总是成功, 即使发生了网络分区, 也就是说这个协议 * 是 AP 级别的一致性. 当网络恢复的时候, 数据和每个分区被合并为一个集合, * 所以整个集群达成最终一致. */ public interface EphemeralConsistencyService extends ConsistencyService { } ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:3:1","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#31-ap-级一致性协议-distro"},{"categories":null,"content":" 3.2 CP 级一致性协议该级别协议用于配置管理, 配置需要落盘和重复使用, 而且各个 reader 看到的要一样否则可能导致业务出现重大问题, 所以要确保严格一致. /** * 实现该接口的都能保证 CP 级别一致性, 这意味着: * 一旦写操作的响应为成功, 相关数据就被保证成功写入了集群, 而且, * 协议保证数据在各个 server 之间是一致的. */ public interface PersistentConsistencyService extends ConsistencyService { } CP 级协议分析本篇不着墨. 上面代码中只涉及了通用接口, 确切地说只是跟存储有关的接口, 与一致性相关的通通没有涉及. 这与文章开头我们分析“一致性存储”设计的时候提到的两个属性的优先级一致. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:3:2","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#32-cp-级一致性协议"},{"categories":null,"content":" 4 Distro 协议详解Distro 协议用于服务发现, 接下来我会从这几个维度进行解析: 存储设计 一致性设计 Distro 集群节点上下线设计 服务发现交互设计 那 Distro 到底是啥意思? Distro 协议用于服务发现, 当大量服务存在的时候, 相应的也会有大量的服务实例, 此时就要在 nacos 集群做分工, 每个 nacos 节点负责一组服务, 主要工作一为健康检查, 二为定期同步自己维护部分的数据校验和给其它 nacos 节点同时响应其它 nacos 节点拉取自己负责部分数据的工作. Distro 协议(其实叫 Partition 协议)会将服务分组, 每个 Nacos 节点负责一组服务对应的实例健康检查. 上述分组的逻辑为: public boolean responsible(String serviceName) { // 获取当前 nacos 节点索引 int index = healthyList.indexOf(NetUtils.localServer()); int lastIndex = healthyList.lastIndexOf(NetUtils.localServer()); if (lastIndex \u003c 0 || index \u003c 0) { return true; } // 计算 serviceName 的哈希值以及谁应该负责维护它 int target = distroHash(serviceName) % healthyList.size(); // 如果 target 与 index 一样, 则当前 nacos 节点负责维护该 service. return target \u003e= index \u0026\u0026 target \u003c= lastIndex; } Nacos 的健康检查和心跳检测是一套很完整也略微复杂的子系统, 回头细讲. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:4:0","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#4-distro-协议详解"},{"categories":null,"content":" 4.1 Distro 存储设计存储相关的增删改查与我们设想一致. 增 /** * 写入数据, 以 Distro 协议进行一致性保证. * 用户要确保 key 全局唯一, 否则会被覆盖. */ @Override public void put(String key, Record value) { onPut(key, value); // 数据有更新, 启动一个数据同步任务, 当前 nacos 节点会 // 将本次变更主动同步给其它 nacos 节点. taskDispatcher.addTask(key); } 删 /** * 移除数据, Distro 负责善后. */ @Override public void remove(String key) { onRemove(key); listeners.remove(key); } 改 同增. 查 /** * 查询数据, 因为底层存储就是并发 map, 线程安全也能确保一致性. */ @Override public Datum get(String key) { return dataStore.get(key); } 作为几个操作核心底层的 dataStore 就是一个并发 map, 是的, 如前所述, Distro 协议数据保存在内存中, 不做持久化. 下面来看看一致性通知是怎么实现的. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:4:1","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#41-distro-存储设计"},{"categories":null,"content":" 4.2 Distro 一致性设计这部分要思考三个问题: 一个 Nacos 节点数据发生变更时如何通知其它节点实现一致性? 一个 Nacos 节点如何感知其它节点的变更? Nacos 节点数据变更后如何告知客户端? 前两个问题其实是一个问题, 也就是通知的发与收. 4.2.1 Distro 节点之间直接交互变更数据实现一致性我们直接看看变更方法 put 里发生的事情: @Override public void put(String key, Record value) { onPut(key, value); // 数据有更新, 启动一个数据同步任务, 当前 nacos 节点会 // 将本次变更主动同步给其它 nacos 节点. taskDispatcher.addTask(key); } 这段代码逻辑很简单, 直接回答了上面提到的第一个问题: 主动发通知, 而且是异步网络通知. (具体发送见 TaskDispatcher::addTask().) 回答了第一个问题, 第二个就呼之欲出了. 在节点主动发通知时, 目的 endpoint 为 /distro/datum, 我们在代码中搜索响应这个 endpoint 的代码, 其位于 com.alibaba.nacos.naming.controllers.DistroController::onSyncDatum(). 所以第二个问题的答案就是: Nacos 通过 DistroController 对外发布的 /distro/datum 接收其它节点发来的变更通知. 接收到数据后会调用下述代码更新本地数据以实现与发送端数据的一致性: public void onPut(String key, Record value) { if (KeyBuilder.matchEphemeralInstanceListKey(key)) { Datum\u003cInstances\u003e datum = new Datum\u003c\u003e(); datum.value = (Instances) value; datum.key = key; // 推动时钟前进 datum.timestamp.incrementAndGet(); // 放入当前 nacos 节点存储 dataStore.put(key, datum); } // 如果没有监听器对该 key 感兴趣则不做通知 if (!listeners.containsKey(key)) { return; } // 发通知, key 对应数据有变更 notifier.addTask(key, ApplyAction.CHANGE); } onPut 方法将变更写入本地存储, 注意, 由于数据就是其它 nacos 节点发过来的所以不用再发通知了. 总结: Distro 协议在响应用户写数据请求时调用 put 方法将数据写存储同时主动发送异步通知给其它节点; Distro 协议监听 /distro/datum 接口请求接收其它 nacos 节点的变更通知, 并将变更写入本地存储. 从而实现了数据的一致性. 4.2.2 Distro 节点之间通过校验和实现数据一致Distro 协议的 DataSyncer 会定期同步本地 nacos 节点负责的数据校验和给其它 nacos 节点, 如果对端检测到不一致会来拉取数据; 如果外部调用 Distro 的 put 等操作导致本地数据变更时, 本地 nacos 节点会在 TaskDispatcher 追加一个同步任务将变更数据同步到其它 nacos 节点(注意此时不分数据是否本地 nacos 节点负责). 发送校验和请求 DataSyncer 会在后台周期性调度 TimedSync 任务, 该任务负责计算当前 nacos 节点负责的数据对应的校验和并发送给其它 nacos 节点: public class TimedSync implements Runnable { @Override public void run() { Map\u003cString, String\u003e keyChecksums = new HashMap\u003c\u003e(64); // 为当前 nacos 节点负责的数据计算校验和 for (String key : dataStore.keys()) { if (!distroMapper.responsible(KeyBuilder.getServiceName(key))) { continue; } keyChecksums.put(key, dataStore.get(key).value.getChecksum()); } // 将上面计算出来的校验和同步给其它 nacos 节点的 /distro/checksum, // 其它节点发现校验和冲突的时候会主动来拉取数据. for (Server member : getServers()) { if (NetUtils.localServer().equals(member.getKey())) { continue; } NamingProxy.syncCheckSums(keyChecksums, member.getKey()); } } } 接收校验和请求 接收请求的接口为 /distro/checksum, 校验和请求处理流程(具体代码见 DistroConsistencyServiceImpl::onReceiveChecksums): 通过接口接收校验和请求, 请求包含数据的 key 以及数据的校验和(无具体数据的 value). 与本地数据比对, 如果本地不存在对应 key 或者校验和与请求中不同, 则再去请求校验和来源的数据 value 更新本地数据. 与本地数据比对, 如果本地包含不在请求中的 key, 则删除相关数据. 数据不一致, 从校验和发送方拉取数据 校验和接收端发现数据不一致的时候, 该删除的删除, 针对该更新的, 就要去校验和发送方去拉取. 其中, 拉取数据的接口为 /distro/datum, 不过此处用的是 GET 请求. 4.2.3 Nacos 如何将服务变更告知客户端上面讲了数据变更时 nacos 集群节点间如何互相告知, 那客户端是怎么知道的呢? 我们以前面讲到 put 操作时提到的 onPut 为例: public void onPut(String key, Record value) { if (KeyBuilder.matchEphemeralInstanceListKey(key)) { Datum\u003cInstances\u003e datum = new Datum\u003c\u003e(); datum.value = (Instances) value; datum.key = key; // 推动时钟前进 datum.timestamp.incrementAndGet(); // 放入当前 nacos 节点存储 dataStore.put(key, datum); } // 如果没有监听器对该 key 感兴趣则不做通知 if (!listeners.containsKey(key)) { return; } // 发通知, key 对应数据有变更 notifier.addTask(key, ApplyAction.CHANGE); } 只关注最后一行代码, DistroConsistencyServiceImpl 在初始化时会在后台启动一个 Notifier 任务, 它负责在 nacos 数据变更时触发相关操作告知对相关数据感兴趣的客户端. 最终通过 Service::onChange 方法触发 PushService 的通知方法. 至于 PushService 是如何感知到客户端的, 这在后面服务发现相关章节进行分析. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:4:2","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#42-distro-一致性设计"},{"categories":null,"content":" 4.2 Distro 一致性设计这部分要思考三个问题: 一个 Nacos 节点数据发生变更时如何通知其它节点实现一致性? 一个 Nacos 节点如何感知其它节点的变更? Nacos 节点数据变更后如何告知客户端? 前两个问题其实是一个问题, 也就是通知的发与收. 4.2.1 Distro 节点之间直接交互变更数据实现一致性我们直接看看变更方法 put 里发生的事情: @Override public void put(String key, Record value) { onPut(key, value); // 数据有更新, 启动一个数据同步任务, 当前 nacos 节点会 // 将本次变更主动同步给其它 nacos 节点. taskDispatcher.addTask(key); } 这段代码逻辑很简单, 直接回答了上面提到的第一个问题: 主动发通知, 而且是异步网络通知. (具体发送见 TaskDispatcher::addTask().) 回答了第一个问题, 第二个就呼之欲出了. 在节点主动发通知时, 目的 endpoint 为 /distro/datum, 我们在代码中搜索响应这个 endpoint 的代码, 其位于 com.alibaba.nacos.naming.controllers.DistroController::onSyncDatum(). 所以第二个问题的答案就是: Nacos 通过 DistroController 对外发布的 /distro/datum 接收其它节点发来的变更通知. 接收到数据后会调用下述代码更新本地数据以实现与发送端数据的一致性: public void onPut(String key, Record value) { if (KeyBuilder.matchEphemeralInstanceListKey(key)) { Datum datum = new Datum\u003c\u003e(); datum.value = (Instances) value; datum.key = key; // 推动时钟前进 datum.timestamp.incrementAndGet(); // 放入当前 nacos 节点存储 dataStore.put(key, datum); } // 如果没有监听器对该 key 感兴趣则不做通知 if (!listeners.containsKey(key)) { return; } // 发通知, key 对应数据有变更 notifier.addTask(key, ApplyAction.CHANGE); } onPut 方法将变更写入本地存储, 注意, 由于数据就是其它 nacos 节点发过来的所以不用再发通知了. 总结: Distro 协议在响应用户写数据请求时调用 put 方法将数据写存储同时主动发送异步通知给其它节点; Distro 协议监听 /distro/datum 接口请求接收其它 nacos 节点的变更通知, 并将变更写入本地存储. 从而实现了数据的一致性. 4.2.2 Distro 节点之间通过校验和实现数据一致Distro 协议的 DataSyncer 会定期同步本地 nacos 节点负责的数据校验和给其它 nacos 节点, 如果对端检测到不一致会来拉取数据; 如果外部调用 Distro 的 put 等操作导致本地数据变更时, 本地 nacos 节点会在 TaskDispatcher 追加一个同步任务将变更数据同步到其它 nacos 节点(注意此时不分数据是否本地 nacos 节点负责). 发送校验和请求 DataSyncer 会在后台周期性调度 TimedSync 任务, 该任务负责计算当前 nacos 节点负责的数据对应的校验和并发送给其它 nacos 节点: public class TimedSync implements Runnable { @Override public void run() { Map keyChecksums = new HashMap\u003c\u003e(64); // 为当前 nacos 节点负责的数据计算校验和 for (String key : dataStore.keys()) { if (!distroMapper.responsible(KeyBuilder.getServiceName(key))) { continue; } keyChecksums.put(key, dataStore.get(key).value.getChecksum()); } // 将上面计算出来的校验和同步给其它 nacos 节点的 /distro/checksum, // 其它节点发现校验和冲突的时候会主动来拉取数据. for (Server member : getServers()) { if (NetUtils.localServer().equals(member.getKey())) { continue; } NamingProxy.syncCheckSums(keyChecksums, member.getKey()); } } } 接收校验和请求 接收请求的接口为 /distro/checksum, 校验和请求处理流程(具体代码见 DistroConsistencyServiceImpl::onReceiveChecksums): 通过接口接收校验和请求, 请求包含数据的 key 以及数据的校验和(无具体数据的 value). 与本地数据比对, 如果本地不存在对应 key 或者校验和与请求中不同, 则再去请求校验和来源的数据 value 更新本地数据. 与本地数据比对, 如果本地包含不在请求中的 key, 则删除相关数据. 数据不一致, 从校验和发送方拉取数据 校验和接收端发现数据不一致的时候, 该删除的删除, 针对该更新的, 就要去校验和发送方去拉取. 其中, 拉取数据的接口为 /distro/datum, 不过此处用的是 GET 请求. 4.2.3 Nacos 如何将服务变更告知客户端上面讲了数据变更时 nacos 集群节点间如何互相告知, 那客户端是怎么知道的呢? 我们以前面讲到 put 操作时提到的 onPut 为例: public void onPut(String key, Record value) { if (KeyBuilder.matchEphemeralInstanceListKey(key)) { Datum datum = new Datum\u003c\u003e(); datum.value = (Instances) value; datum.key = key; // 推动时钟前进 datum.timestamp.incrementAndGet(); // 放入当前 nacos 节点存储 dataStore.put(key, datum); } // 如果没有监听器对该 key 感兴趣则不做通知 if (!listeners.containsKey(key)) { return; } // 发通知, key 对应数据有变更 notifier.addTask(key, ApplyAction.CHANGE); } 只关注最后一行代码, DistroConsistencyServiceImpl 在初始化时会在后台启动一个 Notifier 任务, 它负责在 nacos 数据变更时触发相关操作告知对相关数据感兴趣的客户端. 最终通过 Service::onChange 方法触发 PushService 的通知方法. 至于 PushService 是如何感知到客户端的, 这在后面服务发现相关章节进行分析. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:4:2","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#421-distro-节点之间直接交互变更数据实现一致性"},{"categories":null,"content":" 4.2 Distro 一致性设计这部分要思考三个问题: 一个 Nacos 节点数据发生变更时如何通知其它节点实现一致性? 一个 Nacos 节点如何感知其它节点的变更? Nacos 节点数据变更后如何告知客户端? 前两个问题其实是一个问题, 也就是通知的发与收. 4.2.1 Distro 节点之间直接交互变更数据实现一致性我们直接看看变更方法 put 里发生的事情: @Override public void put(String key, Record value) { onPut(key, value); // 数据有更新, 启动一个数据同步任务, 当前 nacos 节点会 // 将本次变更主动同步给其它 nacos 节点. taskDispatcher.addTask(key); } 这段代码逻辑很简单, 直接回答了上面提到的第一个问题: 主动发通知, 而且是异步网络通知. (具体发送见 TaskDispatcher::addTask().) 回答了第一个问题, 第二个就呼之欲出了. 在节点主动发通知时, 目的 endpoint 为 /distro/datum, 我们在代码中搜索响应这个 endpoint 的代码, 其位于 com.alibaba.nacos.naming.controllers.DistroController::onSyncDatum(). 所以第二个问题的答案就是: Nacos 通过 DistroController 对外发布的 /distro/datum 接收其它节点发来的变更通知. 接收到数据后会调用下述代码更新本地数据以实现与发送端数据的一致性: public void onPut(String key, Record value) { if (KeyBuilder.matchEphemeralInstanceListKey(key)) { Datum datum = new Datum\u003c\u003e(); datum.value = (Instances) value; datum.key = key; // 推动时钟前进 datum.timestamp.incrementAndGet(); // 放入当前 nacos 节点存储 dataStore.put(key, datum); } // 如果没有监听器对该 key 感兴趣则不做通知 if (!listeners.containsKey(key)) { return; } // 发通知, key 对应数据有变更 notifier.addTask(key, ApplyAction.CHANGE); } onPut 方法将变更写入本地存储, 注意, 由于数据就是其它 nacos 节点发过来的所以不用再发通知了. 总结: Distro 协议在响应用户写数据请求时调用 put 方法将数据写存储同时主动发送异步通知给其它节点; Distro 协议监听 /distro/datum 接口请求接收其它 nacos 节点的变更通知, 并将变更写入本地存储. 从而实现了数据的一致性. 4.2.2 Distro 节点之间通过校验和实现数据一致Distro 协议的 DataSyncer 会定期同步本地 nacos 节点负责的数据校验和给其它 nacos 节点, 如果对端检测到不一致会来拉取数据; 如果外部调用 Distro 的 put 等操作导致本地数据变更时, 本地 nacos 节点会在 TaskDispatcher 追加一个同步任务将变更数据同步到其它 nacos 节点(注意此时不分数据是否本地 nacos 节点负责). 发送校验和请求 DataSyncer 会在后台周期性调度 TimedSync 任务, 该任务负责计算当前 nacos 节点负责的数据对应的校验和并发送给其它 nacos 节点: public class TimedSync implements Runnable { @Override public void run() { Map keyChecksums = new HashMap\u003c\u003e(64); // 为当前 nacos 节点负责的数据计算校验和 for (String key : dataStore.keys()) { if (!distroMapper.responsible(KeyBuilder.getServiceName(key))) { continue; } keyChecksums.put(key, dataStore.get(key).value.getChecksum()); } // 将上面计算出来的校验和同步给其它 nacos 节点的 /distro/checksum, // 其它节点发现校验和冲突的时候会主动来拉取数据. for (Server member : getServers()) { if (NetUtils.localServer().equals(member.getKey())) { continue; } NamingProxy.syncCheckSums(keyChecksums, member.getKey()); } } } 接收校验和请求 接收请求的接口为 /distro/checksum, 校验和请求处理流程(具体代码见 DistroConsistencyServiceImpl::onReceiveChecksums): 通过接口接收校验和请求, 请求包含数据的 key 以及数据的校验和(无具体数据的 value). 与本地数据比对, 如果本地不存在对应 key 或者校验和与请求中不同, 则再去请求校验和来源的数据 value 更新本地数据. 与本地数据比对, 如果本地包含不在请求中的 key, 则删除相关数据. 数据不一致, 从校验和发送方拉取数据 校验和接收端发现数据不一致的时候, 该删除的删除, 针对该更新的, 就要去校验和发送方去拉取. 其中, 拉取数据的接口为 /distro/datum, 不过此处用的是 GET 请求. 4.2.3 Nacos 如何将服务变更告知客户端上面讲了数据变更时 nacos 集群节点间如何互相告知, 那客户端是怎么知道的呢? 我们以前面讲到 put 操作时提到的 onPut 为例: public void onPut(String key, Record value) { if (KeyBuilder.matchEphemeralInstanceListKey(key)) { Datum datum = new Datum\u003c\u003e(); datum.value = (Instances) value; datum.key = key; // 推动时钟前进 datum.timestamp.incrementAndGet(); // 放入当前 nacos 节点存储 dataStore.put(key, datum); } // 如果没有监听器对该 key 感兴趣则不做通知 if (!listeners.containsKey(key)) { return; } // 发通知, key 对应数据有变更 notifier.addTask(key, ApplyAction.CHANGE); } 只关注最后一行代码, DistroConsistencyServiceImpl 在初始化时会在后台启动一个 Notifier 任务, 它负责在 nacos 数据变更时触发相关操作告知对相关数据感兴趣的客户端. 最终通过 Service::onChange 方法触发 PushService 的通知方法. 至于 PushService 是如何感知到客户端的, 这在后面服务发现相关章节进行分析. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:4:2","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#422-distro-节点之间通过校验和实现数据一致"},{"categories":null,"content":" 4.2 Distro 一致性设计这部分要思考三个问题: 一个 Nacos 节点数据发生变更时如何通知其它节点实现一致性? 一个 Nacos 节点如何感知其它节点的变更? Nacos 节点数据变更后如何告知客户端? 前两个问题其实是一个问题, 也就是通知的发与收. 4.2.1 Distro 节点之间直接交互变更数据实现一致性我们直接看看变更方法 put 里发生的事情: @Override public void put(String key, Record value) { onPut(key, value); // 数据有更新, 启动一个数据同步任务, 当前 nacos 节点会 // 将本次变更主动同步给其它 nacos 节点. taskDispatcher.addTask(key); } 这段代码逻辑很简单, 直接回答了上面提到的第一个问题: 主动发通知, 而且是异步网络通知. (具体发送见 TaskDispatcher::addTask().) 回答了第一个问题, 第二个就呼之欲出了. 在节点主动发通知时, 目的 endpoint 为 /distro/datum, 我们在代码中搜索响应这个 endpoint 的代码, 其位于 com.alibaba.nacos.naming.controllers.DistroController::onSyncDatum(). 所以第二个问题的答案就是: Nacos 通过 DistroController 对外发布的 /distro/datum 接收其它节点发来的变更通知. 接收到数据后会调用下述代码更新本地数据以实现与发送端数据的一致性: public void onPut(String key, Record value) { if (KeyBuilder.matchEphemeralInstanceListKey(key)) { Datum datum = new Datum\u003c\u003e(); datum.value = (Instances) value; datum.key = key; // 推动时钟前进 datum.timestamp.incrementAndGet(); // 放入当前 nacos 节点存储 dataStore.put(key, datum); } // 如果没有监听器对该 key 感兴趣则不做通知 if (!listeners.containsKey(key)) { return; } // 发通知, key 对应数据有变更 notifier.addTask(key, ApplyAction.CHANGE); } onPut 方法将变更写入本地存储, 注意, 由于数据就是其它 nacos 节点发过来的所以不用再发通知了. 总结: Distro 协议在响应用户写数据请求时调用 put 方法将数据写存储同时主动发送异步通知给其它节点; Distro 协议监听 /distro/datum 接口请求接收其它 nacos 节点的变更通知, 并将变更写入本地存储. 从而实现了数据的一致性. 4.2.2 Distro 节点之间通过校验和实现数据一致Distro 协议的 DataSyncer 会定期同步本地 nacos 节点负责的数据校验和给其它 nacos 节点, 如果对端检测到不一致会来拉取数据; 如果外部调用 Distro 的 put 等操作导致本地数据变更时, 本地 nacos 节点会在 TaskDispatcher 追加一个同步任务将变更数据同步到其它 nacos 节点(注意此时不分数据是否本地 nacos 节点负责). 发送校验和请求 DataSyncer 会在后台周期性调度 TimedSync 任务, 该任务负责计算当前 nacos 节点负责的数据对应的校验和并发送给其它 nacos 节点: public class TimedSync implements Runnable { @Override public void run() { Map keyChecksums = new HashMap\u003c\u003e(64); // 为当前 nacos 节点负责的数据计算校验和 for (String key : dataStore.keys()) { if (!distroMapper.responsible(KeyBuilder.getServiceName(key))) { continue; } keyChecksums.put(key, dataStore.get(key).value.getChecksum()); } // 将上面计算出来的校验和同步给其它 nacos 节点的 /distro/checksum, // 其它节点发现校验和冲突的时候会主动来拉取数据. for (Server member : getServers()) { if (NetUtils.localServer().equals(member.getKey())) { continue; } NamingProxy.syncCheckSums(keyChecksums, member.getKey()); } } } 接收校验和请求 接收请求的接口为 /distro/checksum, 校验和请求处理流程(具体代码见 DistroConsistencyServiceImpl::onReceiveChecksums): 通过接口接收校验和请求, 请求包含数据的 key 以及数据的校验和(无具体数据的 value). 与本地数据比对, 如果本地不存在对应 key 或者校验和与请求中不同, 则再去请求校验和来源的数据 value 更新本地数据. 与本地数据比对, 如果本地包含不在请求中的 key, 则删除相关数据. 数据不一致, 从校验和发送方拉取数据 校验和接收端发现数据不一致的时候, 该删除的删除, 针对该更新的, 就要去校验和发送方去拉取. 其中, 拉取数据的接口为 /distro/datum, 不过此处用的是 GET 请求. 4.2.3 Nacos 如何将服务变更告知客户端上面讲了数据变更时 nacos 集群节点间如何互相告知, 那客户端是怎么知道的呢? 我们以前面讲到 put 操作时提到的 onPut 为例: public void onPut(String key, Record value) { if (KeyBuilder.matchEphemeralInstanceListKey(key)) { Datum datum = new Datum\u003c\u003e(); datum.value = (Instances) value; datum.key = key; // 推动时钟前进 datum.timestamp.incrementAndGet(); // 放入当前 nacos 节点存储 dataStore.put(key, datum); } // 如果没有监听器对该 key 感兴趣则不做通知 if (!listeners.containsKey(key)) { return; } // 发通知, key 对应数据有变更 notifier.addTask(key, ApplyAction.CHANGE); } 只关注最后一行代码, DistroConsistencyServiceImpl 在初始化时会在后台启动一个 Notifier 任务, 它负责在 nacos 数据变更时触发相关操作告知对相关数据感兴趣的客户端. 最终通过 Service::onChange 方法触发 PushService 的通知方法. 至于 PushService 是如何感知到客户端的, 这在后面服务发现相关章节进行分析. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:4:2","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#423-nacos-如何将服务变更告知客户端"},{"categories":null,"content":" 4.3 Distro 集群节点上下线设计Nacos 集群节点上下线处理(包括硬的和软的), 含 distro 和 raft 两套集群. 其中主角是 ServerListManager, 它负责两件事情: 一是定期检测集群配置文件或环境变量感知 nacos 集群节点变更, 如果有变化则加载并调用监听器进行通知; 二是维护 distro 集群各节点之间的心跳, 注意这个心跳是 nacos 节点之间的不是客户端和 nacos 之间的, 如有节点出问题或者重新上线都能检测到. ServerListManager 有两个定时任务(第二个任务只为 Distro 协议服务, Raft 不需要.): 一个用于定时检测配置文件或环境变量感知集群节点\"硬\"变化; 一个用于定期发送状态报告给整个集群的 nacos 节点用于检测全量节点到底多少是活跃的, 如果有节点虽然还在配置名单上但突然挂了或者突然活了都会被检测到, 感知集群\"软\"变化. Raft 协议各节点之间有定期心跳, 天然能感知配置文件里的全量节点有多少是活跃的; Distro 由于协议本身并没有设计和实现节点间心跳, 所以它对全量节点不感冒, 只对健康节点感兴趣, 是否健康靠的是 ServerListManager 定期触发状态报告这类心跳来检测的. 两个小配角 DistroMapper 和 RaftPeerSet 都实现了 ServerChangeListener 监听器接口, 注册到 ServerListManager, 一旦集群节点变更就会受到通知. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:4:3","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#43-distro-集群节点上下线设计"},{"categories":null,"content":" 4.4 Distro 用于服务发现, 客户端是如何与 nacos 进行交互的在服务发现中, 客户端可以扮演两类角色: 一是作为服务实例向 nacos 注册或者解除注册自己. 二是作为服务发现客户端订阅自己感兴趣的服务. 在 nacos client 包里有一个 NacosService 类, 该类会被用户集成到自己代码中作为客户端使用. 客户端 NacosService 启动时会初始化一个 BeatReactor, 如果该客户端会作为服务实例向 nacos 注册或解除注册服务发现, 则会启动心跳任务定期向 nacos 的 /instance/beat 接口发送心跳请求. Nacos 在 InstanceController 接收并处理该请求, 刷新该客户端对应的最后一次心跳时间戳, nacos 有后台任务检测该时间戳判断客户端是否挂了. 客户端 NacosService 在注册或者解除注册时, 会触发 nacos 数据变化, 该变化会导致接收这两类请求的 nacos 节点向其它 nacos 节点同步变更后的数据(也就是前面讲过的 put 触发的同步流程), 同时通过 PushService 通知订阅相关服务的订阅者. 客户端 NacosService 如果作为服务发现客户端而不是某个服务的实例存在, 那么其在调用 queryList 方法向 nacos 的 /instance/list 接口获取对应服务的实例列表时, 会通过 HostReactor 启动 UpdateTask, 该任务定期请求前述 API 接口刷新本地的实例列表. queryList 同时会将客户端的 ip 和端口(即 PushReceiver)发送给 nacos, 后者在该客户端订阅服务实例列表发生变更时会通过 PushService 主动推送 udp 数据包过来. 上面说 UpdateTask 时提到了\"定期\"一词, 客户端为何需要定期获取列表呢? 因为 PushService 维护着庞大的客户端列表, 它怎么知道哪个客户端活着呢? 靠的就是这个客户端这个\"定期\"获取列表的副租用实现的. 以上即是客户端分别作为服务实例或服务发现客户端时与 nacos 交互的核心流程. ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:4:4","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#44-distro-用于服务发现-客户端是如何与-nacos-进行交互的"},{"categories":null,"content":" 5 总结Distro 协议作为 nacos 自行实现的一套 AP 级一致性协议, 在 nacos 中负责核心的服务发现功能, 该功能是 nacos 两大核心功能之一. 我们从设计一致性存储的三个问题开始, 一边猜测一边分析 nacos 源码得到了答案. 如果我们要自己实现一个完整的一致性存储服务, 相关设计非常值得参考. –end– ","date":"2022-08-19","objectID":"/consistent-protocol-of-nacos-part-1/:5:0","series":null,"tags":["consistent","raft","distro","nacos"],"title":"Nacos 的一致性存储解析--Distro 篇","uri":"/consistent-protocol-of-nacos-part-1/#5-总结"},{"categories":null,"content":"Version 是 leveldb 在磁盘上的文件 level 结构的抽象, 也是访问磁盘文件的隘口. 也就是说, 要读取磁盘上的文件, 必须要经过 Version. 要针对磁盘上的数据库做一些优化或者统计, 也可以通过 Version 来实现. ","date":"2022-06-18","objectID":"/leveldb-annotations-8-versions/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之八: 版本(Version)","uri":"/leveldb-annotations-8-versions/#"},{"categories":null,"content":" 1. Version 用途下面通过读写来了解下 Version 的作用. ","date":"2022-06-18","objectID":"/leveldb-annotations-8-versions/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之八: 版本(Version)","uri":"/leveldb-annotations-8-versions/#1-version-用途"},{"categories":null,"content":" 1.1 一个读取例子被用户高频使用的 DBImpl::Get 方法就会用到 Version. 当在内存中的 memtables 找不到 key 时, 就需要去查询文件. 此时, 当前 Version 开始起作用. 下面代码删掉了与我们这里要介绍内容关联不大的部分. Status DBImpl::Get(const ReadOptions\u0026 options, const Slice\u0026 key, std::string* value) { bool have_stat_update = false; Version::GetStats stats; // 根据 user_key 和快照对应的序列号构造一个 internal_key LookupKey lkey(key, snapshot); // 先查询内存中与当前 log 文件对应的 memtable if (mem-\u003eGet(lkey, value, \u0026s)) { } else if (imm != nullptr \u0026\u0026 imm-\u003eGet(lkey, value, \u0026s)) { // 查不到来待压实的 memtable 去查询 } else { // 查不到再逐 level 去 sstable 文件查找, 这时候就用到了 Version current s = current-\u003eGet(options, lkey, value, \u0026stats); have_stat_update = true; } // 每次查询完都要检查下是否有文件查询次数已经达到最大需要进行压实了. if (have_stat_update \u0026\u0026 current-\u003eUpdateStats(stats)) { MaybeScheduleCompaction(); } return s; } 上面 current 就是当前 Version. 方法末尾的判断目的是记录本次扫盘对压实施加的影响, 如果最底层文件读取次数超出上限则进行压实, 关于压实请看之前文章介绍. ","date":"2022-06-18","objectID":"/leveldb-annotations-8-versions/:1:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之八: 版本(Version)","uri":"/leveldb-annotations-8-versions/#11-一个读取例子"},{"categories":null,"content":" 1.2 一个写例子没有. leveldb 文件一旦落盘便不可更改. ","date":"2022-06-18","objectID":"/leveldb-annotations-8-versions/:1:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之八: 版本(Version)","uri":"/leveldb-annotations-8-versions/#12-一个写例子"},{"categories":null,"content":" 2. Version 构造Version 不单独存在, 由 VersionSet 负责维护. leveldb 在启动时会构造一个 VerionSet, 后者会读取磁盘上的 MANIFEST 文件构造 Version. leveldb 在打开数据库时会调用 Recover() 读取磁盘上的文件, 其中就有构造 Version 所需要的 MANIFEST 文件: Status DB::Open(const Options\u0026 options, const std::string\u0026 dbname, DB** dbptr) { ... // 读取 current 文件, manifest 文件, sstable 文件和 log 文件恢复数据库. // 如果要打开的数据库不存在, Recover 负责进行创建. Status s = impl-\u003eRecover(\u0026edit, \u0026save_manifest); ... } impl-\u003eRecover 做相关工作调用的是 VersionSet 的对应方法: Status DBImpl::Recover(VersionEdit* edit, bool *save_manifest) { ... // 该方法负责从最后一个 MANIFEST 文件解析内容出来与当前 Version // 保存的 level 架构合并保存到一个 // 新建的 Version 中, 然后将这个新的 version 作为当前的 version. // 参数是输出型的, 负责保存一个指示当前 MANIFEST 文件是否可以续用. s = versions_-\u003eRecover(save_manifest); ... } VersionSet::Recover() 解析 MANIFEST 文件每一行, 反序列化为 VersionEdit, 然后将其组装为其当前 Version 即 current_: // 该方法负责从最后一个 MANIFEST 文件解析内容出来与当前 Version // 保存的 level 架构合并保存到一个 // 新建的 Version 中, 然后将这个新的 version 作为当前的 version. Status VersionSet::Recover(bool *save_manifest) { // 读取 CURRENT 文件获取 MANIFEST 文件名称 Status s = ReadFileToString(env_, CurrentFileName(dbname_), \u0026current); // 构造 MANIFEST 文件路径 std::string dscname = dbname_ + \"/\" + current; SequentialFile* file; s = env_-\u003eNewSequentialFile(dscname, \u0026file); // 解析 MANIFEST 文件内容反序列化为 VersionEdit Builder builder(this, current_); { log::Reader reader(file, \u0026reporter, true/*checksum*/, 0/*initial_offset*/); // 循环读取 MANIFEST 文件日志, 每一行日志就是一个 VersionEdit while (reader.ReadRecord(\u0026record, \u0026scratch) \u0026\u0026 s.ok()) { VersionEdit edit; // 将 record 反序列化为 version_edit s = edit.DecodeFrom(record); // 将 VersionEdit 保存到 VersionSet 的 builder 中, // 后者可以一次性将这些文件变更与当前 Version 合并构成新 version. if (s.ok()) { builder.Apply(\u0026edit); } } } // 至此解析 MANIFEST 文件结束, 根据其保存的全部文件变更创建新的 Version if (s.ok()) { Version* v = new Version(this); // 将当前 version 和 builder 的 level 架构合并放到新的 v 中 builder.SaveTo(v); // 将 v 作为当前 version AppendVersion(v); } return s; } 至此, leveldb 的 Version 构造完成. ","date":"2022-06-18","objectID":"/leveldb-annotations-8-versions/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之八: 版本(Version)","uri":"/leveldb-annotations-8-versions/#2-version-构造"},{"categories":null,"content":" 3. Version 增量更新每当有文件变更时(这一般由普通写操作和压实操作触发), 相关变更(如文件新增或删除)会被记录到 VersionEdit. VersionEdit 可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. VersionSet 会将新的 VersionEdit 与当前 Version 合并构造一个新的 Version 服务于 leveldb 用户. VersionEdit 可以看作是 Version 的增量更新, 全量+增量=新全量. 具体代码如下: // 1, 将参数 *edit 内容(可看作当前针对 level 架构的增量更新) // 和当前 version 内容合并构成一个新的 version; // 2, 然后将这个新 version 内容序列化为一条日志写到新建的 manifest 文件; // 3, 同时将该 manifest 文件名写入 current 文件; // 4, 最后把新的 version 替换当前 version. Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu) { // 新建一个 Version 用于合并当前 Version 和 VersionEdit 保存的增量更新 Version* v = new Version(this); { // 将当前 VersionSet 及其 current version 作为输入构建一个新的 Builder Builder builder(this, current_); // 将新的 VersionEdit 与 current version 内容合并 builder.Apply(edit); // 将 Builder 内容输出到 Version v 中 builder.SaveTo(v); } // Initialize new descriptor log file if necessary by creating // a temporary file that contains a snapshot of the current version. // 如有必要通过创建一个临时文件来初始化一个新的文件描述符, 这个临时文件包含了当前 Version 的一个快照 std::string new_manifest_file; Status s; // 如果 MANIFEST 文件指针为空则新建一个 if (descriptor_log_ == nullptr) { // manifest 文件又叫 descriptor 文件 s = env_-\u003eNewWritableFile(new_manifest_file, \u0026descriptor_file_); if (s.ok()) { descriptor_log_ = new log::Writer(descriptor_file_); // 将当前 Version 保存的 level 架构信息保存到一个新 VersionEdit // 中后将其序列化到 MANIFEST 文件. s = WriteSnapshot(descriptor_log_); } } // Unlock during expensive MANIFEST log write { mu-\u003eUnlock(); // 将通过参数传入的 VersionEdit 记录到 manifest 文件. // 这个地方相对于上面的 snapshot 相当于是一个增量. if (s.ok()) { std::string record; edit-\u003eEncodeTo(\u0026record); s = descriptor_log_-\u003eAddRecord(record); } mu-\u003eLock(); } // 将基于当前 Version 和增量 VersionEdit 构建的新 version // 作为当前 version. 它是最新的 level 架构. AppendVersion(v); return s; } —end— ","date":"2022-06-18","objectID":"/leveldb-annotations-8-versions/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之八: 版本(Version)","uri":"/leveldb-annotations-8-versions/#3-version-增量更新"},{"categories":null,"content":"梳理 leveldb 压实逻辑的时候, 看代码逆向逻辑太难了, 非常零碎, 很难搞清楚作者为何那么实现, 想起来 LSM-Tree 论文还没读过, 于是找了读了一下, 可是 原始论文 太晦涩了, 读不懂, 可以参考这篇博客来读论文. ","date":"2022-05-16","objectID":"/lsmtree-paper/:0:0","series":null,"tags":["LSM-Tree","db","paper"],"title":"LSM-Tree 论文阅读笔记","uri":"/lsmtree-paper/#"},{"categories":null,"content":" 1 LSM-Tree 是啥 一种数据库索引方法, 不同于 B-Tree. 数据更改时, 非 update-in-place 形式修改数据, 而是顺序写日志文件(此为 log structure, 以优化写); 每个文件有序(以优化读); 文件多了合并(此为merge)为大的文件减少文件数(以优化读). ","date":"2022-05-16","objectID":"/lsmtree-paper/:1:0","series":null,"tags":["LSM-Tree","db","paper"],"title":"LSM-Tree 论文阅读笔记","uri":"/lsmtree-paper/#1-lsm-tree-是啥"},{"categories":null,"content":" 2 LSM-Tree 解决了什么问题 B-Tree 类索引会导致 update-in-place, 依赖 random access, 而且至少两次 IO–读过来-修改-写回去. random access 很慢, 很容易成为写操作的性能瓶颈. LSM-Tree 创新性解决了这个问题, 可以低成本维护实时索引. ","date":"2022-05-16","objectID":"/lsmtree-paper/:2:0","series":null,"tags":["LSM-Tree","db","paper"],"title":"LSM-Tree 论文阅读笔记","uri":"/lsmtree-paper/#2-lsm-tree-解决了什么问题"},{"categories":null,"content":" 3 LSM-Tree 怎么解决的 不管是机械磁盘还是 SSD 甚至是内存, random access 性能都比 sequential access 慢三个数量级. 这个问题也指出了解决方向. LSM-Tree 采用了 append 模式, 依赖 sequential access, 不管增删改都是 append 形式追加日志到文件(只有一次 IO 操作, 写就完了), 而且通过内存+磁盘两级模式实现了批量 append. ","date":"2022-05-16","objectID":"/lsmtree-paper/:3:0","series":null,"tags":["LSM-Tree","db","paper"],"title":"LSM-Tree 论文阅读笔记","uri":"/lsmtree-paper/#3-lsm-tree-怎么解决的"},{"categories":null,"content":" 4 LSM-Tree 创新点有哪些 内存+磁盘两级存储: 增删改均为 append, 改随机写为顺序写, 先写内存, 满了(设置一个阈值)落地到磁盘. 文件有序, 数目多了会进行合并为一个更大的有序文件. ","date":"2022-05-16","objectID":"/lsmtree-paper/:4:0","series":null,"tags":["LSM-Tree","db","paper"],"title":"LSM-Tree 论文阅读笔记","uri":"/lsmtree-paper/#4-lsm-tree-创新点有哪些"},{"categories":null,"content":" 5 LSM-Tree 有什么缺点 顺序写, 随机读. 解决了写的问题, 但是读又成问题了. 读性能不好. 由于数据是追加到文件中, 整个结构是平的, 而且无序, 无法快速定位到要查找的数据, 导致读取性能不好. 为了缓解(不是解决)这个问题, LSM-Tree 实现采取了多种办法, 具体为: 文件内容要变无序为有序. 最直接的就是先确保内存那份是有序的(一般用 RBTree 或者 SkipList ), 满了序列化到磁盘仍然确保有序性; 同时每份文件尾部加一份当前文件内容的索引块, 查询时根据索引能快速定位到该数据项目标位置. 通过合并让文件数变少. 内存不可能无穷大, 所以前一步提到的内存那份也不能太大, 这就意味着磁盘上的有序文件也不能太大, 但这就会导致一个问题, 文件数会非常多. 想法减少文件数. 怎么减少呢? 合并, 把多个小文件合并为有序大文件. 缓存. 有一个全局的布隆过滤器(这里只是推断为全局, leveldb 实现是每个文件一个布隆过滤器), 所有查询先走一遍布隆过滤器确定是否存在, 不存在就不用查了; 如果疑似存在去查文件内容(所以要查的文件要有序而且文件数尽量越少越好). ","date":"2022-05-16","objectID":"/lsmtree-paper/:5:0","series":null,"tags":["LSM-Tree","db","paper"],"title":"LSM-Tree 论文阅读笔记","uri":"/lsmtree-paper/#5-lsm-tree-有什么缺点"},{"categories":null,"content":" 6 LSM-Tree 适合什么场景 适合写多读少的场景, 不适合大量读的场景. NoSQL 数据库像 Cassandra, HBase, BigTable, MongoDB 等存储引擎都是 LSM-Tree. —End— ","date":"2022-05-16","objectID":"/lsmtree-paper/:6:0","series":null,"tags":["LSM-Tree","db","paper"],"title":"LSM-Tree 论文阅读笔记","uri":"/lsmtree-paper/#6-lsm-tree-适合什么场景"},{"categories":null,"content":"Leveldb 是一个 LSM-Tree 类型的数据库, LSM 最后一个字母就是 merge, 压实就是 merge 具体实现. 该算法在 LSM-Tree 论文阅读笔记 里有介绍, 如果不了解建议先读下这篇小文. ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#"},{"categories":null,"content":" 1. 压实介绍当 level-L 大小超过了上限, 具体来说就是 level-0 文件数超过 4 个, level-L(L\u003e=1) 文件总大小超过 $10^L$ MB, 就会触后台线程的压实操作. 压实过程会从 level-L(L\u003e=1) 挑一个文件, 然后将 level-(L+1) 中与该文件键区间重叠的文件都找出来. 注意, 即使一个 level-L 文件仅仅与 level-(L+1) 某个文件重叠了一部分, level-(L+1) 的这个文件也会整个作为压实过程的输入, 即压实的输入最小单位是文件. 另外, 因为 level-0 比较特殊(该层的文件之间可能相互重叠, 而其它层不会), 我们会把 level-0 到 level-1 的压实过程做特殊处理: 我们每次会从 level-0 选取相互重叠的全部文件, 而不是像其它 level 一样只选取一个文件, 然后将其合并为一个文件然后再和 level-1 与其重叠的文件进行合并. 一次压实会合并多个文件的内容从而生成一系列新的 level-(L+1) 文件, 生成一个新文件的条件有两个: 当前文件达到了 2MB 大小或者当前文件的键区间与超过 10 个 level-(L+2) 文件发生了重叠. 第二个条件的目的在于避免后续对 level-(L+1) 文件进行压实时需要从 level-(L+2) 读取过多的数据. 针对某一个 level 的压实会循环该 level 覆盖的整个键空间. 具体来讲, 针对 level-L, 我们会记住 level-L 上次压实的最后一个 key. 针对 level-L 的下次压实将会挑选从这个 key 之后开始的第一个文件进行. (如果不存在这样的文件, 那么就会遍历回键空间起始 key). 压实会丢弃某个 key 对应的被覆盖过的 values(只保留时间线上最新的那个 value), 也会在没有更高的 level 包含该 key 的时候丢弃针对这个 key 的删除标记(level 越高数据越老, 所以如果某个 key 被在下层标记为删除, 在合并全部上层针对该 key 的操作之前该标记不能移除否则会被查询过程感知到老数据). ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#1-压实介绍"},{"categories":null,"content":" 2. 压实目的 前面提到了 leveldb 的数据库就是一堆在逻辑上分层的文件, 这个分层架构是从底向上开始构建的, 而压实促进了数据从 level-0 向 level-6 (leveldb 默认有 7 个 level) 的流动. 压实主要有两个目的: 消除无效数据节省存储空间. Leveldb 不是 in-place 式修改的数据库, 随着修改变多, 同样的 key 可能会对应不同的数据项, 后追加的数据项会在效果上覆盖先追加的. 这就意味着先追加的数据项浪费了存储空间. 合并重叠的键空间, 减少文件数, 提升查询性能, 最好的结果就是 levels 之间没有任何重叠. 如果 level 之间存在键空间重叠, 查询某个键可能就要搜索多个层. 最简单的例子, 比如 level-1 和 level-2 的键空间都是 [0~100), 但前者的键都是奇数, 后者的键都是偶数, 如果要查询的键为 10, 则会先扫描 level-1(因为键空间重叠但又肯定找不到)然后再去 level-2. 极端情况下可能要从 level-0 一路搜到 level-6 才能查到目标键, 这太消耗性能了. 最直观的解法就是尽量消除不同 level 的键空间重叠情况. 压实行为大体分为三类: memtable 转换为 sstable, 从内存到磁盘, 落地到 level-0. 从这个意义上讲, memtable 是 leveldb 数据源头. sstable 文件从 level-i 直接搬到 level-(i+1), 因为没有重叠所以不做任何合并. level-i 与 level-(i+1) 之间重叠的文件合并, 生成新的 level-(i+1) 文件. 这种情况是我们重点关注的. ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#2-压实目的"},{"categories":null,"content":" 3. leveldb 的数据重复leveldb 存在多个层面的数据重复: 同一个 level 内, 后追加的比先追加的数据更新(newer). 不同 level 之间, 低 level 的比高 level 的更新(newer). 下面我们以同一个 level 内的数据重复举个例子. 如下图所示: ┌───────────┬───────────┬───────────┐ │key1:value3│key1:value2│key1:value1│ └───────────┴───────────┴───────────┘ 这个例子中还有价值的只有 key1:value3, 其它空间可以通过压实释放掉. 不熟悉 leveldb 键构成可能会对上面数据项的存储先后位置有疑问. 在 leveldb 中, 同样的 key, 由于追加有先后所以序列号不同, 后加的序列号更大. 同样 key, 序列号越大存储位置越靠前, 这样一次查找即可定位到, 不用再在同样的 key 段进行遍历了. 上面这个例子还只是同一个文件, 不同文件也可能存在 key 重复(如 level-0 的文件之间存在重叠), 不同 level 之间也可能存在 key 重复. 这些情况都可以通过 leveldb 的压实来处理. ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#3-leveldb-的数据重复"},{"categories":null,"content":" 4. 压实实现下面具体说明一下压实具体实现. ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#4-压实实现"},{"categories":null,"content":" 4.1. 压实触发时机不得不说, 压实可能是 leveldb 里面最零散的一个功能, 不管是读操作还是写操作, 还是啥也不干, 都可能触发压实. 具体触发压实的时机如下: 数据库刚打开的时候, 如果打开成功, 则会触发一次压实. (见 DBImpl::Open()) 如果调用 Get 查询感知到疑似需要进行压实, 则此处进一步检查确定是否触发压实, 触发条件是 allowed_seeks 降到了 0, 也就是因为键空间重叠导致叫下层的文件被访问次数超出阈值. (见 DBImpl::Get()) 使用 DBIter 迭代数据库的时候, 每隔一段就会检查下当前在迭代的 user key 是否包含在多个文件里, 如果是那就会触发压实. (见 DBImpl::RecordReadSample()) 在执行写操作的时候, 可能会把 log 文件写满, 这时就需要生成新的 log 文件并将前一个 log 文件对应的 memtable 做压实, 不过纯针对 memtable 的压实其实比较简单, 不是本文着重介绍的. (见 DBImpl::Write()) 另外就是压实任务本身可能会再调度一次压实, 原因是前一次压实可能在某一层产生过多文件. (见 DBImpl::BackgroundCall()) 以上都是自动触发的, 无须用户做任何操作. 除此之外, leveldb 也提供了手工触发压实的功能. leveldb 提供了接口DBImpl::CompactRange(const Slice* begin, const Slice* end), 该接口提供给应用层使用, 如有需要, 用户可以直接发起压实. leveldb 有个后台线程池, 在上述任何一个条件满足的条件下 DBImpl::MaybeScheduleCompaction() 会被调用, 它会生成一个压实任务并投递到这个线程. DBImpl::MaybeScheduleCompaction() 负责投递一个异步压实任务到后台线程, 它是通过调用 env_-\u003eSchedule(\u0026DBImpl::BGWork, this) 来实现的. 其中 BGWork 即为在后台要执行的函数, 它会间接调用 BackgroundCompaction, 这个函数实现了压实工作逻辑, 藏得比较深, 请大家记住它. 下面简单介绍下 leveldb 执行后台任务的线程池的实现. ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#41-压实触发时机"},{"categories":null,"content":" 4.2. 后台压实线程池实现leveldb::PosixEnv 类实现了后台线程池. 毫无疑问, 后台线程池具体工作逻辑依据 生产者-消费者 模式, 不得不说的三件套分别为: 互斥锁 - background_work_mutex_ 条件变量 - background_work_cv_ 任务队列 - background_work_queue_ 提交任务同时也是启动后台线程的方法为 PosixEnv::Schedule(background_work_function, background_work_arg), 第一个参数为要在后台执行的函数, 第二个参数为函数在后台执行时需要的参数. 要注意的是, leveldb 后台线程池只有一个线程, 有需要执行的任务调用下面详解的方法投递到任务队列即可. void PosixEnv::Schedule( void (*background_work_function)(void* background_work_arg), void* background_work_arg) { // 后面涉及几个状态需要锁保护 background_work_mutex_.Lock(); // 如果之前从未启动过后台线程, 则生成一个并让其独立运行 if (!started_background_thread_) { started_background_thread_ = true; std::thread background_thread(PosixEnv::BackgroundThreadEntryPoint, this); // 调用 detach 后, 线程执行部分与线程对象 background_thread 分离, 独立去运行, // 线程对象不再拥有对执行部分的所有权. // 线程执行完毕会自动释放全部分配的资源. // 所以一会 background_thread 出作用域被销毁也不影响 // 线程在后台运行. background_thread.detach(); } // 如果队列为空, 后台线程之前可能在等待, 激活之 if (background_work_queue_.empty()) { background_work_cv_.Signal(); } // 将本次调度的任务加到任务队列里(通过 emplace 直接构造避免了中间临时变量生成和拷贝). background_work_queue_.emplace(background_work_function, background_work_arg); background_work_mutex_.Unlock(); } ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#42-后台压实线程池实现"},{"categories":null,"content":" 4.3. 压实过程整个压实实现过程, 大体如下: 先压实已满的 memtable 这个发生时机一般是用户 Put 数据的时候, 因为 Put 就是往 memtable 写数据. memtable 满了就不变了, 所以叫 immutable memtable, 简写为 imm. 再压实 sstables, 先构造 Compaction 确定压实范围再进行实质压实. 2.1. 针对 sstables, 要分手动触发和自动触发来构造 Compaction 2.1.1. 如果是手动触发, 传入待压实的 level, 最小 key, 最大 key, 通过 versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend) 来构造 Compaction. 2.1.2. 否则就是自动触发, 调用 versions_-\u003ePickCompaction() 来根据统计信息确定待压实 level 和文件列表, 并构造 Compaction. 2.2. 基于构造的 Compaction 执行 sstables 压实 下面代码是对上面流程的程序化(删减非强相关的异常处理等逻辑以突出重点): // 该方法仅在 DBImpl::BackgroundCall 调用 void DBImpl::BackgroundCompaction() { // 压实过程需要全程持有锁, 这也暗示压实不能耗费太多时间. mutex_.AssertHeld(); // 1. 先压实已满的 memtable, 满了就不变了, 所以 // 叫 immutable memtable, 简写为 imm if (imm_ != nullptr) { CompactMemTable(); return; } // 2.1. 针对 sstables, 下面要分手动触发和自动触发来构造 Compaction Compaction* c; // leveldb 提供了 `DBImpl::CompactRange()` 接口供应用层手工触发压实. bool is_manual = (manual_compaction_ != nullptr); // 2.1.1. 如果手动触发了一个压实 if (is_manual) { ManualCompaction* m = manual_compaction_; // 确定压实范围, 即 level 层待压实文件列表, level+1 与之重叠文件列表. c = versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend); m-\u003edone = (c == nullptr); } else { // 2.1.2. 否则根据统计信息确定待压实 level c = versions_-\u003ePickCompaction(); } // 2.2. 执行 sstables 压实 Status status; if (c == nullptr) { // 2.2.1. 无需压实 } else if (!is_manual \u0026\u0026 c-\u003eIsTrivialMove()) { // 2.2.2. 通过移动文件实现的压实, 直接把文件从 level 移动到 level+1 FileMetaData* f = c-\u003einput(0, 0); // 将该文件从 level 层删除 c-\u003eedit()-\u003eDeleteFile(c-\u003elevel(), f-\u003enumber); // 将该文件增加到 level+1 c-\u003eedit()-\u003eAddFile(c-\u003elevel() + 1, f-\u003enumber, f-\u003efile_size, f-\u003esmallest, f-\u003elargest); // 应用本次移动操作更新 level 文件架构 status = versions_-\u003eLogAndApply(c-\u003eedit(), \u0026mutex_); } else { // 2.2.3. 实打实的压实 CompactionState* compact = new CompactionState(c); // 做压实 status = DoCompactionWork(compact); // 清理压实现场 CleanupCompaction(compact); } delete c; } 下面逐个讲解下上面的核心流程. 4.3.1. memtable 压实memtable 压实本质就是将内存中的 memtable 转换为 sstable 文件并写入到磁盘中. 当且仅当该方法执行成功后, leveldb 会切换到一组新的 log-file/memtable 组合. 在这个过程中, 会计算触发压实的条件, 具体在 versions_-\u003eLogAndApply() 中. void DBImpl::CompactMemTable() { // 调用该方法之前必须获取相应的锁. mutex_.AssertHeld(); // 将内存中的 memtable 内容保存为 sstable 文件. // 每次落盘新文件会更改 level 架构, 需要更新到 VersionEdit 中. VersionEdit edit; // 获取当前 dbimpl 对应的最新 version Version* base = versions_-\u003ecurrent(); // 将 imm_ 引用的 memtable 以 table 文件形式保存到 // 磁盘并将其对应的元信息(level、filemeta 等)保存到 edit 中 // (edit 维护着 level 架构每一层文件信息, 新文件落盘要记录下来) Status s = WriteLevel0Table(imm_, \u0026edit, base); // 用生成的 Table 替换不可变的 memtable if (s.ok()) { edit.SetPrevLogNumber(0); // memtable 已经转换为 Table 写入磁盘了, 之前的 logs 都不需要了. edit.SetLogNumber(logfile_number_); // 更新 level 架构信息 s = versions_-\u003eLogAndApply(\u0026edit, \u0026mutex_); } } 上面代码只保留了核心逻辑, 最核心的就两条: 将 memetable 转换为 sstable 文件落盘并为其选则一个合适的 level, 这是 WriteLevel0Table 负责的, 这种转换前面文章讲过了, 后面会重点说下应该把新生成的 sstable 文件放到哪个 level 的决策逻辑. 将本次落盘导致的 level 架构信息改变更新一下, 这是 versions_-\u003eLogAndApply 负责的, 这里与压实相关的是它会调用确定下个待压实 level 的方法, 下面会进行描述. 4.3.1.1. 基于 memtable 生成的 sstable 应该放到哪个 level这个逻辑是由 Version::PickLevelForMemTableOutput() 负责的, 该方法会在 WriteLevel0Table 中调用. 下面是 WriteLevel0Table 的核心逻辑, 主要是两块(为了突出重点删掉了其它不相干代码): // 将 mem 对应的 memtable 以 table 文件形式保存到磁盘, // 并将本次变更对应的元信息(level、filemeta 等)保存到 edit 中 Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { // 必须持有锁 mutex_.AssertHeld(); // 1. 先把 imm 转换为 sstable 并进行落盘. // 将 memtable 序列化为一个 sstable 文件并写入磁盘; // 文件大小会被保存到 meta 中, 同时将 sstable 对应的 Table 实例放入 // table_cache_ 中. BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026meta); // 2. sstable 落盘完成, 这是物理部分; 下面要更新逻辑部分, 即 level 架构信息. // 为上面新生成的 sstable 文件找一个落脚的 level. // 注意, leveldb 文件存储和 level 架构信息存储是分开的, // 文件落盘就是直接写, 相关架构信息如具体属于哪个 level, 包含的键区间, // 另外记录到其它地方. int level = 0; // meta 相关成员信息在 BuildTable 时填充过了 const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); level = base-\u003ePickLevelForMemTableOutput(min_user_key, max_user_key); // 3. 压实完成, 将相关元信息记录到 edit, 方便后面更新 level 架构. // 将 [min_user_key, max_user_key] 对应的 Table 文件 // 元信息及其 level 记录到 edit 中 ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#43-压实过程"},{"categories":null,"content":" 4.3. 压实过程整个压实实现过程, 大体如下: 先压实已满的 memtable 这个发生时机一般是用户 Put 数据的时候, 因为 Put 就是往 memtable 写数据. memtable 满了就不变了, 所以叫 immutable memtable, 简写为 imm. 再压实 sstables, 先构造 Compaction 确定压实范围再进行实质压实. 2.1. 针对 sstables, 要分手动触发和自动触发来构造 Compaction 2.1.1. 如果是手动触发, 传入待压实的 level, 最小 key, 最大 key, 通过 versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend) 来构造 Compaction. 2.1.2. 否则就是自动触发, 调用 versions_-\u003ePickCompaction() 来根据统计信息确定待压实 level 和文件列表, 并构造 Compaction. 2.2. 基于构造的 Compaction 执行 sstables 压实 下面代码是对上面流程的程序化(删减非强相关的异常处理等逻辑以突出重点): // 该方法仅在 DBImpl::BackgroundCall 调用 void DBImpl::BackgroundCompaction() { // 压实过程需要全程持有锁, 这也暗示压实不能耗费太多时间. mutex_.AssertHeld(); // 1. 先压实已满的 memtable, 满了就不变了, 所以 // 叫 immutable memtable, 简写为 imm if (imm_ != nullptr) { CompactMemTable(); return; } // 2.1. 针对 sstables, 下面要分手动触发和自动触发来构造 Compaction Compaction* c; // leveldb 提供了 `DBImpl::CompactRange()` 接口供应用层手工触发压实. bool is_manual = (manual_compaction_ != nullptr); // 2.1.1. 如果手动触发了一个压实 if (is_manual) { ManualCompaction* m = manual_compaction_; // 确定压实范围, 即 level 层待压实文件列表, level+1 与之重叠文件列表. c = versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend); m-\u003edone = (c == nullptr); } else { // 2.1.2. 否则根据统计信息确定待压实 level c = versions_-\u003ePickCompaction(); } // 2.2. 执行 sstables 压实 Status status; if (c == nullptr) { // 2.2.1. 无需压实 } else if (!is_manual \u0026\u0026 c-\u003eIsTrivialMove()) { // 2.2.2. 通过移动文件实现的压实, 直接把文件从 level 移动到 level+1 FileMetaData* f = c-\u003einput(0, 0); // 将该文件从 level 层删除 c-\u003eedit()-\u003eDeleteFile(c-\u003elevel(), f-\u003enumber); // 将该文件增加到 level+1 c-\u003eedit()-\u003eAddFile(c-\u003elevel() + 1, f-\u003enumber, f-\u003efile_size, f-\u003esmallest, f-\u003elargest); // 应用本次移动操作更新 level 文件架构 status = versions_-\u003eLogAndApply(c-\u003eedit(), \u0026mutex_); } else { // 2.2.3. 实打实的压实 CompactionState* compact = new CompactionState(c); // 做压实 status = DoCompactionWork(compact); // 清理压实现场 CleanupCompaction(compact); } delete c; } 下面逐个讲解下上面的核心流程. 4.3.1. memtable 压实memtable 压实本质就是将内存中的 memtable 转换为 sstable 文件并写入到磁盘中. 当且仅当该方法执行成功后, leveldb 会切换到一组新的 log-file/memtable 组合. 在这个过程中, 会计算触发压实的条件, 具体在 versions_-\u003eLogAndApply() 中. void DBImpl::CompactMemTable() { // 调用该方法之前必须获取相应的锁. mutex_.AssertHeld(); // 将内存中的 memtable 内容保存为 sstable 文件. // 每次落盘新文件会更改 level 架构, 需要更新到 VersionEdit 中. VersionEdit edit; // 获取当前 dbimpl 对应的最新 version Version* base = versions_-\u003ecurrent(); // 将 imm_ 引用的 memtable 以 table 文件形式保存到 // 磁盘并将其对应的元信息(level、filemeta 等)保存到 edit 中 // (edit 维护着 level 架构每一层文件信息, 新文件落盘要记录下来) Status s = WriteLevel0Table(imm_, \u0026edit, base); // 用生成的 Table 替换不可变的 memtable if (s.ok()) { edit.SetPrevLogNumber(0); // memtable 已经转换为 Table 写入磁盘了, 之前的 logs 都不需要了. edit.SetLogNumber(logfile_number_); // 更新 level 架构信息 s = versions_-\u003eLogAndApply(\u0026edit, \u0026mutex_); } } 上面代码只保留了核心逻辑, 最核心的就两条: 将 memetable 转换为 sstable 文件落盘并为其选则一个合适的 level, 这是 WriteLevel0Table 负责的, 这种转换前面文章讲过了, 后面会重点说下应该把新生成的 sstable 文件放到哪个 level 的决策逻辑. 将本次落盘导致的 level 架构信息改变更新一下, 这是 versions_-\u003eLogAndApply 负责的, 这里与压实相关的是它会调用确定下个待压实 level 的方法, 下面会进行描述. 4.3.1.1. 基于 memtable 生成的 sstable 应该放到哪个 level这个逻辑是由 Version::PickLevelForMemTableOutput() 负责的, 该方法会在 WriteLevel0Table 中调用. 下面是 WriteLevel0Table 的核心逻辑, 主要是两块(为了突出重点删掉了其它不相干代码): // 将 mem 对应的 memtable 以 table 文件形式保存到磁盘, // 并将本次变更对应的元信息(level、filemeta 等)保存到 edit 中 Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { // 必须持有锁 mutex_.AssertHeld(); // 1. 先把 imm 转换为 sstable 并进行落盘. // 将 memtable 序列化为一个 sstable 文件并写入磁盘; // 文件大小会被保存到 meta 中, 同时将 sstable 对应的 Table 实例放入 // table_cache_ 中. BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026meta); // 2. sstable 落盘完成, 这是物理部分; 下面要更新逻辑部分, 即 level 架构信息. // 为上面新生成的 sstable 文件找一个落脚的 level. // 注意, leveldb 文件存储和 level 架构信息存储是分开的, // 文件落盘就是直接写, 相关架构信息如具体属于哪个 level, 包含的键区间, // 另外记录到其它地方. int level = 0; // meta 相关成员信息在 BuildTable 时填充过了 const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); level = base-\u003ePickLevelForMemTableOutput(min_user_key, max_user_key); // 3. 压实完成, 将相关元信息记录到 edit, 方便后面更新 level 架构. // 将 [min_user_key, max_user_key] 对应的 Table 文件 // 元信息及其 level 记录到 edit 中 ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#431-memtable-压实"},{"categories":null,"content":" 4.3. 压实过程整个压实实现过程, 大体如下: 先压实已满的 memtable 这个发生时机一般是用户 Put 数据的时候, 因为 Put 就是往 memtable 写数据. memtable 满了就不变了, 所以叫 immutable memtable, 简写为 imm. 再压实 sstables, 先构造 Compaction 确定压实范围再进行实质压实. 2.1. 针对 sstables, 要分手动触发和自动触发来构造 Compaction 2.1.1. 如果是手动触发, 传入待压实的 level, 最小 key, 最大 key, 通过 versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend) 来构造 Compaction. 2.1.2. 否则就是自动触发, 调用 versions_-\u003ePickCompaction() 来根据统计信息确定待压实 level 和文件列表, 并构造 Compaction. 2.2. 基于构造的 Compaction 执行 sstables 压实 下面代码是对上面流程的程序化(删减非强相关的异常处理等逻辑以突出重点): // 该方法仅在 DBImpl::BackgroundCall 调用 void DBImpl::BackgroundCompaction() { // 压实过程需要全程持有锁, 这也暗示压实不能耗费太多时间. mutex_.AssertHeld(); // 1. 先压实已满的 memtable, 满了就不变了, 所以 // 叫 immutable memtable, 简写为 imm if (imm_ != nullptr) { CompactMemTable(); return; } // 2.1. 针对 sstables, 下面要分手动触发和自动触发来构造 Compaction Compaction* c; // leveldb 提供了 `DBImpl::CompactRange()` 接口供应用层手工触发压实. bool is_manual = (manual_compaction_ != nullptr); // 2.1.1. 如果手动触发了一个压实 if (is_manual) { ManualCompaction* m = manual_compaction_; // 确定压实范围, 即 level 层待压实文件列表, level+1 与之重叠文件列表. c = versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend); m-\u003edone = (c == nullptr); } else { // 2.1.2. 否则根据统计信息确定待压实 level c = versions_-\u003ePickCompaction(); } // 2.2. 执行 sstables 压实 Status status; if (c == nullptr) { // 2.2.1. 无需压实 } else if (!is_manual \u0026\u0026 c-\u003eIsTrivialMove()) { // 2.2.2. 通过移动文件实现的压实, 直接把文件从 level 移动到 level+1 FileMetaData* f = c-\u003einput(0, 0); // 将该文件从 level 层删除 c-\u003eedit()-\u003eDeleteFile(c-\u003elevel(), f-\u003enumber); // 将该文件增加到 level+1 c-\u003eedit()-\u003eAddFile(c-\u003elevel() + 1, f-\u003enumber, f-\u003efile_size, f-\u003esmallest, f-\u003elargest); // 应用本次移动操作更新 level 文件架构 status = versions_-\u003eLogAndApply(c-\u003eedit(), \u0026mutex_); } else { // 2.2.3. 实打实的压实 CompactionState* compact = new CompactionState(c); // 做压实 status = DoCompactionWork(compact); // 清理压实现场 CleanupCompaction(compact); } delete c; } 下面逐个讲解下上面的核心流程. 4.3.1. memtable 压实memtable 压实本质就是将内存中的 memtable 转换为 sstable 文件并写入到磁盘中. 当且仅当该方法执行成功后, leveldb 会切换到一组新的 log-file/memtable 组合. 在这个过程中, 会计算触发压实的条件, 具体在 versions_-\u003eLogAndApply() 中. void DBImpl::CompactMemTable() { // 调用该方法之前必须获取相应的锁. mutex_.AssertHeld(); // 将内存中的 memtable 内容保存为 sstable 文件. // 每次落盘新文件会更改 level 架构, 需要更新到 VersionEdit 中. VersionEdit edit; // 获取当前 dbimpl 对应的最新 version Version* base = versions_-\u003ecurrent(); // 将 imm_ 引用的 memtable 以 table 文件形式保存到 // 磁盘并将其对应的元信息(level、filemeta 等)保存到 edit 中 // (edit 维护着 level 架构每一层文件信息, 新文件落盘要记录下来) Status s = WriteLevel0Table(imm_, \u0026edit, base); // 用生成的 Table 替换不可变的 memtable if (s.ok()) { edit.SetPrevLogNumber(0); // memtable 已经转换为 Table 写入磁盘了, 之前的 logs 都不需要了. edit.SetLogNumber(logfile_number_); // 更新 level 架构信息 s = versions_-\u003eLogAndApply(\u0026edit, \u0026mutex_); } } 上面代码只保留了核心逻辑, 最核心的就两条: 将 memetable 转换为 sstable 文件落盘并为其选则一个合适的 level, 这是 WriteLevel0Table 负责的, 这种转换前面文章讲过了, 后面会重点说下应该把新生成的 sstable 文件放到哪个 level 的决策逻辑. 将本次落盘导致的 level 架构信息改变更新一下, 这是 versions_-\u003eLogAndApply 负责的, 这里与压实相关的是它会调用确定下个待压实 level 的方法, 下面会进行描述. 4.3.1.1. 基于 memtable 生成的 sstable 应该放到哪个 level这个逻辑是由 Version::PickLevelForMemTableOutput() 负责的, 该方法会在 WriteLevel0Table 中调用. 下面是 WriteLevel0Table 的核心逻辑, 主要是两块(为了突出重点删掉了其它不相干代码): // 将 mem 对应的 memtable 以 table 文件形式保存到磁盘, // 并将本次变更对应的元信息(level、filemeta 等)保存到 edit 中 Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { // 必须持有锁 mutex_.AssertHeld(); // 1. 先把 imm 转换为 sstable 并进行落盘. // 将 memtable 序列化为一个 sstable 文件并写入磁盘; // 文件大小会被保存到 meta 中, 同时将 sstable 对应的 Table 实例放入 // table_cache_ 中. BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026meta); // 2. sstable 落盘完成, 这是物理部分; 下面要更新逻辑部分, 即 level 架构信息. // 为上面新生成的 sstable 文件找一个落脚的 level. // 注意, leveldb 文件存储和 level 架构信息存储是分开的, // 文件落盘就是直接写, 相关架构信息如具体属于哪个 level, 包含的键区间, // 另外记录到其它地方. int level = 0; // meta 相关成员信息在 BuildTable 时填充过了 const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); level = base-\u003ePickLevelForMemTableOutput(min_user_key, max_user_key); // 3. 压实完成, 将相关元信息记录到 edit, 方便后面更新 level 架构. // 将 [min_user_key, max_user_key] 对应的 Table 文件 // 元信息及其 level 记录到 edit 中 ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#4311-基于-memtable-生成的-sstable-应该放到哪个-level"},{"categories":null,"content":" 4.3. 压实过程整个压实实现过程, 大体如下: 先压实已满的 memtable 这个发生时机一般是用户 Put 数据的时候, 因为 Put 就是往 memtable 写数据. memtable 满了就不变了, 所以叫 immutable memtable, 简写为 imm. 再压实 sstables, 先构造 Compaction 确定压实范围再进行实质压实. 2.1. 针对 sstables, 要分手动触发和自动触发来构造 Compaction 2.1.1. 如果是手动触发, 传入待压实的 level, 最小 key, 最大 key, 通过 versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend) 来构造 Compaction. 2.1.2. 否则就是自动触发, 调用 versions_-\u003ePickCompaction() 来根据统计信息确定待压实 level 和文件列表, 并构造 Compaction. 2.2. 基于构造的 Compaction 执行 sstables 压实 下面代码是对上面流程的程序化(删减非强相关的异常处理等逻辑以突出重点): // 该方法仅在 DBImpl::BackgroundCall 调用 void DBImpl::BackgroundCompaction() { // 压实过程需要全程持有锁, 这也暗示压实不能耗费太多时间. mutex_.AssertHeld(); // 1. 先压实已满的 memtable, 满了就不变了, 所以 // 叫 immutable memtable, 简写为 imm if (imm_ != nullptr) { CompactMemTable(); return; } // 2.1. 针对 sstables, 下面要分手动触发和自动触发来构造 Compaction Compaction* c; // leveldb 提供了 `DBImpl::CompactRange()` 接口供应用层手工触发压实. bool is_manual = (manual_compaction_ != nullptr); // 2.1.1. 如果手动触发了一个压实 if (is_manual) { ManualCompaction* m = manual_compaction_; // 确定压实范围, 即 level 层待压实文件列表, level+1 与之重叠文件列表. c = versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend); m-\u003edone = (c == nullptr); } else { // 2.1.2. 否则根据统计信息确定待压实 level c = versions_-\u003ePickCompaction(); } // 2.2. 执行 sstables 压实 Status status; if (c == nullptr) { // 2.2.1. 无需压实 } else if (!is_manual \u0026\u0026 c-\u003eIsTrivialMove()) { // 2.2.2. 通过移动文件实现的压实, 直接把文件从 level 移动到 level+1 FileMetaData* f = c-\u003einput(0, 0); // 将该文件从 level 层删除 c-\u003eedit()-\u003eDeleteFile(c-\u003elevel(), f-\u003enumber); // 将该文件增加到 level+1 c-\u003eedit()-\u003eAddFile(c-\u003elevel() + 1, f-\u003enumber, f-\u003efile_size, f-\u003esmallest, f-\u003elargest); // 应用本次移动操作更新 level 文件架构 status = versions_-\u003eLogAndApply(c-\u003eedit(), \u0026mutex_); } else { // 2.2.3. 实打实的压实 CompactionState* compact = new CompactionState(c); // 做压实 status = DoCompactionWork(compact); // 清理压实现场 CleanupCompaction(compact); } delete c; } 下面逐个讲解下上面的核心流程. 4.3.1. memtable 压实memtable 压实本质就是将内存中的 memtable 转换为 sstable 文件并写入到磁盘中. 当且仅当该方法执行成功后, leveldb 会切换到一组新的 log-file/memtable 组合. 在这个过程中, 会计算触发压实的条件, 具体在 versions_-\u003eLogAndApply() 中. void DBImpl::CompactMemTable() { // 调用该方法之前必须获取相应的锁. mutex_.AssertHeld(); // 将内存中的 memtable 内容保存为 sstable 文件. // 每次落盘新文件会更改 level 架构, 需要更新到 VersionEdit 中. VersionEdit edit; // 获取当前 dbimpl 对应的最新 version Version* base = versions_-\u003ecurrent(); // 将 imm_ 引用的 memtable 以 table 文件形式保存到 // 磁盘并将其对应的元信息(level、filemeta 等)保存到 edit 中 // (edit 维护着 level 架构每一层文件信息, 新文件落盘要记录下来) Status s = WriteLevel0Table(imm_, \u0026edit, base); // 用生成的 Table 替换不可变的 memtable if (s.ok()) { edit.SetPrevLogNumber(0); // memtable 已经转换为 Table 写入磁盘了, 之前的 logs 都不需要了. edit.SetLogNumber(logfile_number_); // 更新 level 架构信息 s = versions_-\u003eLogAndApply(\u0026edit, \u0026mutex_); } } 上面代码只保留了核心逻辑, 最核心的就两条: 将 memetable 转换为 sstable 文件落盘并为其选则一个合适的 level, 这是 WriteLevel0Table 负责的, 这种转换前面文章讲过了, 后面会重点说下应该把新生成的 sstable 文件放到哪个 level 的决策逻辑. 将本次落盘导致的 level 架构信息改变更新一下, 这是 versions_-\u003eLogAndApply 负责的, 这里与压实相关的是它会调用确定下个待压实 level 的方法, 下面会进行描述. 4.3.1.1. 基于 memtable 生成的 sstable 应该放到哪个 level这个逻辑是由 Version::PickLevelForMemTableOutput() 负责的, 该方法会在 WriteLevel0Table 中调用. 下面是 WriteLevel0Table 的核心逻辑, 主要是两块(为了突出重点删掉了其它不相干代码): // 将 mem 对应的 memtable 以 table 文件形式保存到磁盘, // 并将本次变更对应的元信息(level、filemeta 等)保存到 edit 中 Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { // 必须持有锁 mutex_.AssertHeld(); // 1. 先把 imm 转换为 sstable 并进行落盘. // 将 memtable 序列化为一个 sstable 文件并写入磁盘; // 文件大小会被保存到 meta 中, 同时将 sstable 对应的 Table 实例放入 // table_cache_ 中. BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026meta); // 2. sstable 落盘完成, 这是物理部分; 下面要更新逻辑部分, 即 level 架构信息. // 为上面新生成的 sstable 文件找一个落脚的 level. // 注意, leveldb 文件存储和 level 架构信息存储是分开的, // 文件落盘就是直接写, 相关架构信息如具体属于哪个 level, 包含的键区间, // 另外记录到其它地方. int level = 0; // meta 相关成员信息在 BuildTable 时填充过了 const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); level = base-\u003ePickLevelForMemTableOutput(min_user_key, max_user_key); // 3. 压实完成, 将相关元信息记录到 edit, 方便后面更新 level 架构. // 将 [min_user_key, max_user_key] 对应的 Table 文件 // 元信息及其 level 记录到 edit 中 ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#432-sstables-压实"},{"categories":null,"content":" 4.3. 压实过程整个压实实现过程, 大体如下: 先压实已满的 memtable 这个发生时机一般是用户 Put 数据的时候, 因为 Put 就是往 memtable 写数据. memtable 满了就不变了, 所以叫 immutable memtable, 简写为 imm. 再压实 sstables, 先构造 Compaction 确定压实范围再进行实质压实. 2.1. 针对 sstables, 要分手动触发和自动触发来构造 Compaction 2.1.1. 如果是手动触发, 传入待压实的 level, 最小 key, 最大 key, 通过 versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend) 来构造 Compaction. 2.1.2. 否则就是自动触发, 调用 versions_-\u003ePickCompaction() 来根据统计信息确定待压实 level 和文件列表, 并构造 Compaction. 2.2. 基于构造的 Compaction 执行 sstables 压实 下面代码是对上面流程的程序化(删减非强相关的异常处理等逻辑以突出重点): // 该方法仅在 DBImpl::BackgroundCall 调用 void DBImpl::BackgroundCompaction() { // 压实过程需要全程持有锁, 这也暗示压实不能耗费太多时间. mutex_.AssertHeld(); // 1. 先压实已满的 memtable, 满了就不变了, 所以 // 叫 immutable memtable, 简写为 imm if (imm_ != nullptr) { CompactMemTable(); return; } // 2.1. 针对 sstables, 下面要分手动触发和自动触发来构造 Compaction Compaction* c; // leveldb 提供了 `DBImpl::CompactRange()` 接口供应用层手工触发压实. bool is_manual = (manual_compaction_ != nullptr); // 2.1.1. 如果手动触发了一个压实 if (is_manual) { ManualCompaction* m = manual_compaction_; // 确定压实范围, 即 level 层待压实文件列表, level+1 与之重叠文件列表. c = versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend); m-\u003edone = (c == nullptr); } else { // 2.1.2. 否则根据统计信息确定待压实 level c = versions_-\u003ePickCompaction(); } // 2.2. 执行 sstables 压实 Status status; if (c == nullptr) { // 2.2.1. 无需压实 } else if (!is_manual \u0026\u0026 c-\u003eIsTrivialMove()) { // 2.2.2. 通过移动文件实现的压实, 直接把文件从 level 移动到 level+1 FileMetaData* f = c-\u003einput(0, 0); // 将该文件从 level 层删除 c-\u003eedit()-\u003eDeleteFile(c-\u003elevel(), f-\u003enumber); // 将该文件增加到 level+1 c-\u003eedit()-\u003eAddFile(c-\u003elevel() + 1, f-\u003enumber, f-\u003efile_size, f-\u003esmallest, f-\u003elargest); // 应用本次移动操作更新 level 文件架构 status = versions_-\u003eLogAndApply(c-\u003eedit(), \u0026mutex_); } else { // 2.2.3. 实打实的压实 CompactionState* compact = new CompactionState(c); // 做压实 status = DoCompactionWork(compact); // 清理压实现场 CleanupCompaction(compact); } delete c; } 下面逐个讲解下上面的核心流程. 4.3.1. memtable 压实memtable 压实本质就是将内存中的 memtable 转换为 sstable 文件并写入到磁盘中. 当且仅当该方法执行成功后, leveldb 会切换到一组新的 log-file/memtable 组合. 在这个过程中, 会计算触发压实的条件, 具体在 versions_-\u003eLogAndApply() 中. void DBImpl::CompactMemTable() { // 调用该方法之前必须获取相应的锁. mutex_.AssertHeld(); // 将内存中的 memtable 内容保存为 sstable 文件. // 每次落盘新文件会更改 level 架构, 需要更新到 VersionEdit 中. VersionEdit edit; // 获取当前 dbimpl 对应的最新 version Version* base = versions_-\u003ecurrent(); // 将 imm_ 引用的 memtable 以 table 文件形式保存到 // 磁盘并将其对应的元信息(level、filemeta 等)保存到 edit 中 // (edit 维护着 level 架构每一层文件信息, 新文件落盘要记录下来) Status s = WriteLevel0Table(imm_, \u0026edit, base); // 用生成的 Table 替换不可变的 memtable if (s.ok()) { edit.SetPrevLogNumber(0); // memtable 已经转换为 Table 写入磁盘了, 之前的 logs 都不需要了. edit.SetLogNumber(logfile_number_); // 更新 level 架构信息 s = versions_-\u003eLogAndApply(\u0026edit, \u0026mutex_); } } 上面代码只保留了核心逻辑, 最核心的就两条: 将 memetable 转换为 sstable 文件落盘并为其选则一个合适的 level, 这是 WriteLevel0Table 负责的, 这种转换前面文章讲过了, 后面会重点说下应该把新生成的 sstable 文件放到哪个 level 的决策逻辑. 将本次落盘导致的 level 架构信息改变更新一下, 这是 versions_-\u003eLogAndApply 负责的, 这里与压实相关的是它会调用确定下个待压实 level 的方法, 下面会进行描述. 4.3.1.1. 基于 memtable 生成的 sstable 应该放到哪个 level这个逻辑是由 Version::PickLevelForMemTableOutput() 负责的, 该方法会在 WriteLevel0Table 中调用. 下面是 WriteLevel0Table 的核心逻辑, 主要是两块(为了突出重点删掉了其它不相干代码): // 将 mem 对应的 memtable 以 table 文件形式保存到磁盘, // 并将本次变更对应的元信息(level、filemeta 等)保存到 edit 中 Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { // 必须持有锁 mutex_.AssertHeld(); // 1. 先把 imm 转换为 sstable 并进行落盘. // 将 memtable 序列化为一个 sstable 文件并写入磁盘; // 文件大小会被保存到 meta 中, 同时将 sstable 对应的 Table 实例放入 // table_cache_ 中. BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026meta); // 2. sstable 落盘完成, 这是物理部分; 下面要更新逻辑部分, 即 level 架构信息. // 为上面新生成的 sstable 文件找一个落脚的 level. // 注意, leveldb 文件存储和 level 架构信息存储是分开的, // 文件落盘就是直接写, 相关架构信息如具体属于哪个 level, 包含的键区间, // 另外记录到其它地方. int level = 0; // meta 相关成员信息在 BuildTable 时填充过了 const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); level = base-\u003ePickLevelForMemTableOutput(min_user_key, max_user_key); // 3. 压实完成, 将相关元信息记录到 edit, 方便后面更新 level 架构. // 将 [min_user_key, max_user_key] 对应的 Table 文件 // 元信息及其 level 记录到 edit 中 ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#4321-compaction-构造"},{"categories":null,"content":" 4.3. 压实过程整个压实实现过程, 大体如下: 先压实已满的 memtable 这个发生时机一般是用户 Put 数据的时候, 因为 Put 就是往 memtable 写数据. memtable 满了就不变了, 所以叫 immutable memtable, 简写为 imm. 再压实 sstables, 先构造 Compaction 确定压实范围再进行实质压实. 2.1. 针对 sstables, 要分手动触发和自动触发来构造 Compaction 2.1.1. 如果是手动触发, 传入待压实的 level, 最小 key, 最大 key, 通过 versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend) 来构造 Compaction. 2.1.2. 否则就是自动触发, 调用 versions_-\u003ePickCompaction() 来根据统计信息确定待压实 level 和文件列表, 并构造 Compaction. 2.2. 基于构造的 Compaction 执行 sstables 压实 下面代码是对上面流程的程序化(删减非强相关的异常处理等逻辑以突出重点): // 该方法仅在 DBImpl::BackgroundCall 调用 void DBImpl::BackgroundCompaction() { // 压实过程需要全程持有锁, 这也暗示压实不能耗费太多时间. mutex_.AssertHeld(); // 1. 先压实已满的 memtable, 满了就不变了, 所以 // 叫 immutable memtable, 简写为 imm if (imm_ != nullptr) { CompactMemTable(); return; } // 2.1. 针对 sstables, 下面要分手动触发和自动触发来构造 Compaction Compaction* c; // leveldb 提供了 `DBImpl::CompactRange()` 接口供应用层手工触发压实. bool is_manual = (manual_compaction_ != nullptr); // 2.1.1. 如果手动触发了一个压实 if (is_manual) { ManualCompaction* m = manual_compaction_; // 确定压实范围, 即 level 层待压实文件列表, level+1 与之重叠文件列表. c = versions_-\u003eCompactRange(m-\u003elevel, m-\u003ebegin, m-\u003eend); m-\u003edone = (c == nullptr); } else { // 2.1.2. 否则根据统计信息确定待压实 level c = versions_-\u003ePickCompaction(); } // 2.2. 执行 sstables 压实 Status status; if (c == nullptr) { // 2.2.1. 无需压实 } else if (!is_manual \u0026\u0026 c-\u003eIsTrivialMove()) { // 2.2.2. 通过移动文件实现的压实, 直接把文件从 level 移动到 level+1 FileMetaData* f = c-\u003einput(0, 0); // 将该文件从 level 层删除 c-\u003eedit()-\u003eDeleteFile(c-\u003elevel(), f-\u003enumber); // 将该文件增加到 level+1 c-\u003eedit()-\u003eAddFile(c-\u003elevel() + 1, f-\u003enumber, f-\u003efile_size, f-\u003esmallest, f-\u003elargest); // 应用本次移动操作更新 level 文件架构 status = versions_-\u003eLogAndApply(c-\u003eedit(), \u0026mutex_); } else { // 2.2.3. 实打实的压实 CompactionState* compact = new CompactionState(c); // 做压实 status = DoCompactionWork(compact); // 清理压实现场 CleanupCompaction(compact); } delete c; } 下面逐个讲解下上面的核心流程. 4.3.1. memtable 压实memtable 压实本质就是将内存中的 memtable 转换为 sstable 文件并写入到磁盘中. 当且仅当该方法执行成功后, leveldb 会切换到一组新的 log-file/memtable 组合. 在这个过程中, 会计算触发压实的条件, 具体在 versions_-\u003eLogAndApply() 中. void DBImpl::CompactMemTable() { // 调用该方法之前必须获取相应的锁. mutex_.AssertHeld(); // 将内存中的 memtable 内容保存为 sstable 文件. // 每次落盘新文件会更改 level 架构, 需要更新到 VersionEdit 中. VersionEdit edit; // 获取当前 dbimpl 对应的最新 version Version* base = versions_-\u003ecurrent(); // 将 imm_ 引用的 memtable 以 table 文件形式保存到 // 磁盘并将其对应的元信息(level、filemeta 等)保存到 edit 中 // (edit 维护着 level 架构每一层文件信息, 新文件落盘要记录下来) Status s = WriteLevel0Table(imm_, \u0026edit, base); // 用生成的 Table 替换不可变的 memtable if (s.ok()) { edit.SetPrevLogNumber(0); // memtable 已经转换为 Table 写入磁盘了, 之前的 logs 都不需要了. edit.SetLogNumber(logfile_number_); // 更新 level 架构信息 s = versions_-\u003eLogAndApply(\u0026edit, \u0026mutex_); } } 上面代码只保留了核心逻辑, 最核心的就两条: 将 memetable 转换为 sstable 文件落盘并为其选则一个合适的 level, 这是 WriteLevel0Table 负责的, 这种转换前面文章讲过了, 后面会重点说下应该把新生成的 sstable 文件放到哪个 level 的决策逻辑. 将本次落盘导致的 level 架构信息改变更新一下, 这是 versions_-\u003eLogAndApply 负责的, 这里与压实相关的是它会调用确定下个待压实 level 的方法, 下面会进行描述. 4.3.1.1. 基于 memtable 生成的 sstable 应该放到哪个 level这个逻辑是由 Version::PickLevelForMemTableOutput() 负责的, 该方法会在 WriteLevel0Table 中调用. 下面是 WriteLevel0Table 的核心逻辑, 主要是两块(为了突出重点删掉了其它不相干代码): // 将 mem 对应的 memtable 以 table 文件形式保存到磁盘, // 并将本次变更对应的元信息(level、filemeta 等)保存到 edit 中 Status DBImpl::WriteLevel0Table(MemTable* mem, VersionEdit* edit, Version* base) { // 必须持有锁 mutex_.AssertHeld(); // 1. 先把 imm 转换为 sstable 并进行落盘. // 将 memtable 序列化为一个 sstable 文件并写入磁盘; // 文件大小会被保存到 meta 中, 同时将 sstable 对应的 Table 实例放入 // table_cache_ 中. BuildTable(dbname_, env_, options_, table_cache_, iter, \u0026meta); // 2. sstable 落盘完成, 这是物理部分; 下面要更新逻辑部分, 即 level 架构信息. // 为上面新生成的 sstable 文件找一个落脚的 level. // 注意, leveldb 文件存储和 level 架构信息存储是分开的, // 文件落盘就是直接写, 相关架构信息如具体属于哪个 level, 包含的键区间, // 另外记录到其它地方. int level = 0; // meta 相关成员信息在 BuildTable 时填充过了 const Slice min_user_key = meta.smallest.user_key(); const Slice max_user_key = meta.largest.user_key(); level = base-\u003ePickLevelForMemTableOutput(min_user_key, max_user_key); // 3. 压实完成, 将相关元信息记录到 edit, 方便后面更新 level 架构. // 将 [min_user_key, max_user_key] 对应的 Table 文件 // 元信息及其 level 记录到 edit 中 ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:4:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#4322-基于-compaction-的压实"},{"categories":null,"content":" 5. 总结leveldb 作为一个典型的 LSM-Tree 实现, 压实是不可或缺的. 本文围绕压实目的、时机、实现等几个维度比较详细地介绍了 leveldb 的压实实现. 如果只能用一句话描述 leveldb 的压实, 我们可以描述为“leveldb 基于查询统计或存储统计决定压实的文件及其所属 level, 然后确认 level+1 与其重叠的文件, 将这两层文件进行合并放到 level+1; 合并过程要避免文件过大, 避免是靠检测合并的文件大小与 level+2 的重叠字节数来实现的.” –End– ","date":"2021-10-27","objectID":"/leveldb-annotations-7-compaction/:5:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之七: 压实(Compaction)","uri":"/leveldb-annotations-7-compaction/#5-总结"},{"categories":null,"content":"上一篇讲了 leveldb 中 Table 的设计和实现, 它是磁盘 sstable 文件的内存形式, 但是 Table 在实际中不会被用户直接用到, 而是借助 TableCache. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#"},{"categories":null,"content":" 1 TableCache: leveldb 的磁盘文件缓存结构(代码位于 db/table_cache.h) 磁盘上每个 sstable 文件都对应一个 Table 实例, TableCache 是一个用于缓存这些 Table 实例的缓存. 或者这么说, sstable 文件被加载到内存后, 被缓存到 TableCache 中. 每次用户进行查询操作的时候(即调用DBImpl::Get())可能需要去查询磁盘上的文件(即未在 memtable 中查到), 这就要求有个缓存功能来加速. TableCache 会缓存 sstable 文件对应的 Table 实例, 用于加速用户的查询, 否则每次读文件解析就很慢了. 目前在用的缓存策略是 LRU 以防内存占用过大. 每个 db 实例都会持有一个 TableCache 实例, 对该缓存的的填充是通过副作用实现的, 即当外部调用 DBImpl::Get()-\u003eVersion::Get()-\u003eVersionSet::table_cache_::Get() 进行查询的时候, 如果发现 sstable 对应 Table 实例不在缓存就会将其填充进来. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#1-tablecache-leveldb-的磁盘文件缓存结构"},{"categories":null,"content":" 1.1 概览下面是 TableCache 类构成: class TableCache { public: // 为 file_number 标识的 sstable 文件构造对应的迭代器. Iterator* NewIterator(const ReadOptions\u0026 options, uint64_t file_number, uint64_t file_size, Table** tableptr = nullptr); // 从缓存中查找 internal_key 为 k 的数据项. Status Get(const ReadOptions\u0026 options, uint64_t file_number, uint64_t file_size, const Slice\u0026 k, void* arg, void (*handle_result)(void*, const Slice\u0026, const Slice\u0026)); // 驱逐 file_number 对应的 table 对象 void Evict(uint64_t file_number); private: // 一个基于特定淘汰算法(如 LRU)的 Cache Cache* cache_; }; ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#11-概览"},{"categories":null,"content":" 1.2 数据成员TableCache 的核心数据成员就是 Cache(实际使用的是 ShardedLRUCache, 这里不展开, 后文详述). ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#12-数据成员"},{"categories":null,"content":" 1.3 方法成员TableCache 的核心方法有三个. 1.3.1 查询Get() 方法负责实现从缓存中查找指定 key 的数据项. 若 key 对应的 sstable 文件不在缓存则会根据 file_number 读取文件生成 Table 实例放到缓存中然后再查询(注意这是个副作用), 查到后调用 handle_result 进行处理. 针对该方法的调用链为: DBImpl::Get()-\u003eVersion::Get()-\u003eVersionSet::table_cache_::Get(). 1.3.2 遍历遍历就要有迭代器, NewIterator() 方法负责生成指定 sstable 文件内容对应的迭代器. 该方法主要用于在 Version::AddIterators() 遍历 level 架构中每一个 sstable 文件时构造对应的迭代器, 这些迭代器加上 memtable 的迭代器, 就能遍历整个数据库的内容了. Version::AddIterators() 会从其 table_cache_ 成员根据 file_number 查找其对应的 Table 对象, 若查到则返回其对应迭代器; 否则加载文件(这是一个副作用)并生成对应的 Table 对象放到 table_cache_ 然后返回新构造的 Table 的 iterator. 注意, 如果 NewIterator() 方法的 tableptr 参数非空, 则设置 *tableptr 指向返回的 iterator 底下的 Table 对象. 返回的 *tableptr 对象由 TableCache 所拥有, 所以用户不要删除它; 只要 iterator 还活着, 该对象就有效. 1.3.3 删除Evict() 方法负责驱逐某个 sstable 文件对应的 Table 缓存, 它会在 DBImpl::DeleteObsoleteFiles() 删除过期文件时候执行. 这三个方法的具体实现比较简单, 不再详列. 下面重点说一下其最重要的数据成员 – Cache. 1.3.4 修改嘿, 没有这个接口. 由于 leveldb 是一个 append 类型数据库, 它不会做 inplace 修改, 这同时也避免了解决复杂的数据一致性问题. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#13-方法成员"},{"categories":null,"content":" 1.3 方法成员TableCache 的核心方法有三个. 1.3.1 查询Get() 方法负责实现从缓存中查找指定 key 的数据项. 若 key 对应的 sstable 文件不在缓存则会根据 file_number 读取文件生成 Table 实例放到缓存中然后再查询(注意这是个副作用), 查到后调用 handle_result 进行处理. 针对该方法的调用链为: DBImpl::Get()-\u003eVersion::Get()-\u003eVersionSet::table_cache_::Get(). 1.3.2 遍历遍历就要有迭代器, NewIterator() 方法负责生成指定 sstable 文件内容对应的迭代器. 该方法主要用于在 Version::AddIterators() 遍历 level 架构中每一个 sstable 文件时构造对应的迭代器, 这些迭代器加上 memtable 的迭代器, 就能遍历整个数据库的内容了. Version::AddIterators() 会从其 table_cache_ 成员根据 file_number 查找其对应的 Table 对象, 若查到则返回其对应迭代器; 否则加载文件(这是一个副作用)并生成对应的 Table 对象放到 table_cache_ 然后返回新构造的 Table 的 iterator. 注意, 如果 NewIterator() 方法的 tableptr 参数非空, 则设置 *tableptr 指向返回的 iterator 底下的 Table 对象. 返回的 *tableptr 对象由 TableCache 所拥有, 所以用户不要删除它; 只要 iterator 还活着, 该对象就有效. 1.3.3 删除Evict() 方法负责驱逐某个 sstable 文件对应的 Table 缓存, 它会在 DBImpl::DeleteObsoleteFiles() 删除过期文件时候执行. 这三个方法的具体实现比较简单, 不再详列. 下面重点说一下其最重要的数据成员 – Cache. 1.3.4 修改嘿, 没有这个接口. 由于 leveldb 是一个 append 类型数据库, 它不会做 inplace 修改, 这同时也避免了解决复杂的数据一致性问题. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#131-查询"},{"categories":null,"content":" 1.3 方法成员TableCache 的核心方法有三个. 1.3.1 查询Get() 方法负责实现从缓存中查找指定 key 的数据项. 若 key 对应的 sstable 文件不在缓存则会根据 file_number 读取文件生成 Table 实例放到缓存中然后再查询(注意这是个副作用), 查到后调用 handle_result 进行处理. 针对该方法的调用链为: DBImpl::Get()-\u003eVersion::Get()-\u003eVersionSet::table_cache_::Get(). 1.3.2 遍历遍历就要有迭代器, NewIterator() 方法负责生成指定 sstable 文件内容对应的迭代器. 该方法主要用于在 Version::AddIterators() 遍历 level 架构中每一个 sstable 文件时构造对应的迭代器, 这些迭代器加上 memtable 的迭代器, 就能遍历整个数据库的内容了. Version::AddIterators() 会从其 table_cache_ 成员根据 file_number 查找其对应的 Table 对象, 若查到则返回其对应迭代器; 否则加载文件(这是一个副作用)并生成对应的 Table 对象放到 table_cache_ 然后返回新构造的 Table 的 iterator. 注意, 如果 NewIterator() 方法的 tableptr 参数非空, 则设置 *tableptr 指向返回的 iterator 底下的 Table 对象. 返回的 *tableptr 对象由 TableCache 所拥有, 所以用户不要删除它; 只要 iterator 还活着, 该对象就有效. 1.3.3 删除Evict() 方法负责驱逐某个 sstable 文件对应的 Table 缓存, 它会在 DBImpl::DeleteObsoleteFiles() 删除过期文件时候执行. 这三个方法的具体实现比较简单, 不再详列. 下面重点说一下其最重要的数据成员 – Cache. 1.3.4 修改嘿, 没有这个接口. 由于 leveldb 是一个 append 类型数据库, 它不会做 inplace 修改, 这同时也避免了解决复杂的数据一致性问题. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#132-遍历"},{"categories":null,"content":" 1.3 方法成员TableCache 的核心方法有三个. 1.3.1 查询Get() 方法负责实现从缓存中查找指定 key 的数据项. 若 key 对应的 sstable 文件不在缓存则会根据 file_number 读取文件生成 Table 实例放到缓存中然后再查询(注意这是个副作用), 查到后调用 handle_result 进行处理. 针对该方法的调用链为: DBImpl::Get()-\u003eVersion::Get()-\u003eVersionSet::table_cache_::Get(). 1.3.2 遍历遍历就要有迭代器, NewIterator() 方法负责生成指定 sstable 文件内容对应的迭代器. 该方法主要用于在 Version::AddIterators() 遍历 level 架构中每一个 sstable 文件时构造对应的迭代器, 这些迭代器加上 memtable 的迭代器, 就能遍历整个数据库的内容了. Version::AddIterators() 会从其 table_cache_ 成员根据 file_number 查找其对应的 Table 对象, 若查到则返回其对应迭代器; 否则加载文件(这是一个副作用)并生成对应的 Table 对象放到 table_cache_ 然后返回新构造的 Table 的 iterator. 注意, 如果 NewIterator() 方法的 tableptr 参数非空, 则设置 *tableptr 指向返回的 iterator 底下的 Table 对象. 返回的 *tableptr 对象由 TableCache 所拥有, 所以用户不要删除它; 只要 iterator 还活着, 该对象就有效. 1.3.3 删除Evict() 方法负责驱逐某个 sstable 文件对应的 Table 缓存, 它会在 DBImpl::DeleteObsoleteFiles() 删除过期文件时候执行. 这三个方法的具体实现比较简单, 不再详列. 下面重点说一下其最重要的数据成员 – Cache. 1.3.4 修改嘿, 没有这个接口. 由于 leveldb 是一个 append 类型数据库, 它不会做 inplace 修改, 这同时也避免了解决复杂的数据一致性问题. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#133-删除"},{"categories":null,"content":" 1.3 方法成员TableCache 的核心方法有三个. 1.3.1 查询Get() 方法负责实现从缓存中查找指定 key 的数据项. 若 key 对应的 sstable 文件不在缓存则会根据 file_number 读取文件生成 Table 实例放到缓存中然后再查询(注意这是个副作用), 查到后调用 handle_result 进行处理. 针对该方法的调用链为: DBImpl::Get()-\u003eVersion::Get()-\u003eVersionSet::table_cache_::Get(). 1.3.2 遍历遍历就要有迭代器, NewIterator() 方法负责生成指定 sstable 文件内容对应的迭代器. 该方法主要用于在 Version::AddIterators() 遍历 level 架构中每一个 sstable 文件时构造对应的迭代器, 这些迭代器加上 memtable 的迭代器, 就能遍历整个数据库的内容了. Version::AddIterators() 会从其 table_cache_ 成员根据 file_number 查找其对应的 Table 对象, 若查到则返回其对应迭代器; 否则加载文件(这是一个副作用)并生成对应的 Table 对象放到 table_cache_ 然后返回新构造的 Table 的 iterator. 注意, 如果 NewIterator() 方法的 tableptr 参数非空, 则设置 *tableptr 指向返回的 iterator 底下的 Table 对象. 返回的 *tableptr 对象由 TableCache 所拥有, 所以用户不要删除它; 只要 iterator 还活着, 该对象就有效. 1.3.3 删除Evict() 方法负责驱逐某个 sstable 文件对应的 Table 缓存, 它会在 DBImpl::DeleteObsoleteFiles() 删除过期文件时候执行. 这三个方法的具体实现比较简单, 不再详列. 下面重点说一下其最重要的数据成员 – Cache. 1.3.4 修改嘿, 没有这个接口. 由于 leveldb 是一个 append 类型数据库, 它不会做 inplace 修改, 这同时也避免了解决复杂的数据一致性问题. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#134-修改"},{"categories":null,"content":" 2 Cache 接口(代码位于 include/leveldb/cache.h) 作为一个缓存定义, 最起码要提供的功能就是增删查, 注意没有改, 缓存只是一个视图, 如果要支持修改可能要引入一系列一致性问题. 该接口的设计还有很重要的一点, 也是 leveldb 设计中很重要的一点, 就是引用计数. Cache 中保存的数据项类型为 Cache::Handle, 具体实现中存在一个数据成员表示计数, 这样可以根据计数进行内存复用或回收. 这一点贯穿了各个重要方法的实现. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#2-cache-接口"},{"categories":null,"content":" 2.1 增 // 插入一对 \u003ckey, value\u003e 到 cache 中. virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) = 0; 这个方法的参数列表看着有点吓人. 但最重要的就是前两个参数, 后面的参数简单介绍下: charge 主要用来计算缓存的成本, 也就是内存占用的, 每次插入时候调用方可以将插入的数据字节作为 charge 传进来. deleter 是一个用户针对自己插入的数据定制的清理器, 举个简单的例子(具体代码位于 table_cache.cc): // 一个 deleter, 用于从 Cache 中删除数据项时使用 static void DeleteEntry(const Slice\u0026 key, void* value) { TableAndFile* tf = reinterpret_cast\u003cTableAndFile*\u003e(value); delete tf-\u003etable; delete tf-\u003efile; delete tf; } 比较简单轻量, 就是做内存释放. 那 deleter 啥时候调用呢? Cache 析构的时候. 注意, Insert 的时候, 会给新的数据项设置引用计数. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#21-增"},{"categories":null,"content":" 2.2 删 // 如果 cache 包含了 key 对应的映射, 删除之. virtual void Erase(const Slice\u0026 key) = 0; 注意, 因为引用计数的存在, Erase 可能不会发生物理删除, 除非数据项对应引用计数变为 0. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#22-删"},{"categories":null,"content":" 2.3 查 // 如果 cache 中没有针对 key 的映射, 返回 nullptr. // 其它情况返回对应该映射的 handle. virtual Handle* Lookup(const Slice\u0026 key) = 0; 查询本身没有什么可讲的, 要重点说的还是引用计数相关, 针对查询到的数据项, 要递增其引用计数防止被其它客户端删除. 而这也引申出另一个需求, 就是当调用 Lookup 调用方不再使用数据项的时候, 需要主动调用 Release(handle) 来将引用计数减一. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#23-查"},{"categories":null,"content":" 2.4 其它除了上面三个核心方法. Cache 还涉及下面几点: Cache 对象不支持任何拷贝(用技术语言讲就是它的拷贝构造和赋值构造都被禁掉了), 一个实例只能有一个副本存在, 这样做就省掉了维护各个 Cache 副本一致性的问题. 除了显式地删除, Cache 还有一个 Prune() 方法, 用于移除缓存中全部不再活跃的数据项, 具体取决于 Cache 具体实现, 以 LRUCache 为例就是将不在 in_use_ 链表中的数据项删除. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:2:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#24-其它"},{"categories":null,"content":" 3 ShardedLRUCache 实现(代码位于 util/cache.cc) 前文提到的 Cache 是个抽象类, leveldb 通过 ShardedLRUCache 继承并实现了相关功能. 该类位于一个匿名 namespace, 通过下面的方法暴露相关功能: Cache* NewLRUCache(size_t capacity) { return new ShardedLRUCache(capacity); } (熟悉 golang 的同学可能看了会会心一笑, 真说不准后来的 golang 相关写法沿用了 Google 在 C++ 代码里的习惯.) 要理解这个类, 先从名字开始, 从右到左关键词分别是 LRU, Sharded. 这两个关键词说清楚了实现的关键点: 缓存基于 LRU 算法做淘汰 缓存支持 sharding 即分片 下面从顶向下来描述相关设计与实现. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#3-shardedlrucache-实现"},{"categories":null,"content":" 3.1 ShardingSharding 算法一般有两种, 基于 hash 的或者基于 range 的, 这里是基于 hash 的. 不知道大家可发现这里有个问题. 就是 ShardedLRUCache 为何要进行 sharding, 明明这个类的一个实例只存在于一个节点的内存里(这么说有点绕但为了尽可能严谨先这么表达了. 换个不严格的问法就是 leveld 非分布式, ShardedLRUCache 实例也只在一个机器上, 为啥还要搞成分片的?)? 文档和代码里没有说明, 但我觉得这个问题值得思考一下. Sharding 最明显的目的就是分摊压力到各个 shard, 要么是分摊存储压力(多节点, 每个节点不承担一部分存储), 要么是分摊计算压力(多节点, 每个节点承担一部分计算), 总之这个在分布式环境下比较容易理解. 这里如此设计目的我认为是后者, 即计算压力, 更明确地讲是避免针对 ShardedLRUCache 锁粒度过大导致访问变为串行. 每个 shard 持有各自的锁, 这样可以尽可能地实现并行处理. 这里的 sharding 实质上是实现了 Java 里的 Lock Striping. 3.1.1 sharding 存储ShardedLRUCache 有一个成员, 它包含一个 shard 数组, 每个 shard 就是一个 LRUCache: LRUCache shard_[kNumShards]; 每个 cache 默认 16 个 shards. 3.1.2 增插入数据的时候, 先计算 hash, 然后基于 hash 寻找对应 shard(即 LRUCache) 做真正插入操作: virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) { // 计算 hash const uint32_t hash = HashSlice(key); // 基于 hash 做 sharding return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } 3.1.3 删和插入数据类似, 先计算 hash, 定位到所在 shard, 然后再做具体删除操作: virtual void Erase(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } 3.1.4 查与插入数据类似, 先计算 hash, 定位到可能所在的 shard, 然后再做具体查询操作: virtual Handle* Lookup(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#31-sharding"},{"categories":null,"content":" 3.1 ShardingSharding 算法一般有两种, 基于 hash 的或者基于 range 的, 这里是基于 hash 的. 不知道大家可发现这里有个问题. 就是 ShardedLRUCache 为何要进行 sharding, 明明这个类的一个实例只存在于一个节点的内存里(这么说有点绕但为了尽可能严谨先这么表达了. 换个不严格的问法就是 leveld 非分布式, ShardedLRUCache 实例也只在一个机器上, 为啥还要搞成分片的?)? 文档和代码里没有说明, 但我觉得这个问题值得思考一下. Sharding 最明显的目的就是分摊压力到各个 shard, 要么是分摊存储压力(多节点, 每个节点不承担一部分存储), 要么是分摊计算压力(多节点, 每个节点承担一部分计算), 总之这个在分布式环境下比较容易理解. 这里如此设计目的我认为是后者, 即计算压力, 更明确地讲是避免针对 ShardedLRUCache 锁粒度过大导致访问变为串行. 每个 shard 持有各自的锁, 这样可以尽可能地实现并行处理. 这里的 sharding 实质上是实现了 Java 里的 Lock Striping. 3.1.1 sharding 存储ShardedLRUCache 有一个成员, 它包含一个 shard 数组, 每个 shard 就是一个 LRUCache: LRUCache shard_[kNumShards]; 每个 cache 默认 16 个 shards. 3.1.2 增插入数据的时候, 先计算 hash, 然后基于 hash 寻找对应 shard(即 LRUCache) 做真正插入操作: virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) { // 计算 hash const uint32_t hash = HashSlice(key); // 基于 hash 做 sharding return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } 3.1.3 删和插入数据类似, 先计算 hash, 定位到所在 shard, 然后再做具体删除操作: virtual void Erase(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } 3.1.4 查与插入数据类似, 先计算 hash, 定位到可能所在的 shard, 然后再做具体查询操作: virtual Handle* Lookup(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#311-sharding-存储"},{"categories":null,"content":" 3.1 ShardingSharding 算法一般有两种, 基于 hash 的或者基于 range 的, 这里是基于 hash 的. 不知道大家可发现这里有个问题. 就是 ShardedLRUCache 为何要进行 sharding, 明明这个类的一个实例只存在于一个节点的内存里(这么说有点绕但为了尽可能严谨先这么表达了. 换个不严格的问法就是 leveld 非分布式, ShardedLRUCache 实例也只在一个机器上, 为啥还要搞成分片的?)? 文档和代码里没有说明, 但我觉得这个问题值得思考一下. Sharding 最明显的目的就是分摊压力到各个 shard, 要么是分摊存储压力(多节点, 每个节点不承担一部分存储), 要么是分摊计算压力(多节点, 每个节点承担一部分计算), 总之这个在分布式环境下比较容易理解. 这里如此设计目的我认为是后者, 即计算压力, 更明确地讲是避免针对 ShardedLRUCache 锁粒度过大导致访问变为串行. 每个 shard 持有各自的锁, 这样可以尽可能地实现并行处理. 这里的 sharding 实质上是实现了 Java 里的 Lock Striping. 3.1.1 sharding 存储ShardedLRUCache 有一个成员, 它包含一个 shard 数组, 每个 shard 就是一个 LRUCache: LRUCache shard_[kNumShards]; 每个 cache 默认 16 个 shards. 3.1.2 增插入数据的时候, 先计算 hash, 然后基于 hash 寻找对应 shard(即 LRUCache) 做真正插入操作: virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) { // 计算 hash const uint32_t hash = HashSlice(key); // 基于 hash 做 sharding return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } 3.1.3 删和插入数据类似, 先计算 hash, 定位到所在 shard, 然后再做具体删除操作: virtual void Erase(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } 3.1.4 查与插入数据类似, 先计算 hash, 定位到可能所在的 shard, 然后再做具体查询操作: virtual Handle* Lookup(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#312-增"},{"categories":null,"content":" 3.1 ShardingSharding 算法一般有两种, 基于 hash 的或者基于 range 的, 这里是基于 hash 的. 不知道大家可发现这里有个问题. 就是 ShardedLRUCache 为何要进行 sharding, 明明这个类的一个实例只存在于一个节点的内存里(这么说有点绕但为了尽可能严谨先这么表达了. 换个不严格的问法就是 leveld 非分布式, ShardedLRUCache 实例也只在一个机器上, 为啥还要搞成分片的?)? 文档和代码里没有说明, 但我觉得这个问题值得思考一下. Sharding 最明显的目的就是分摊压力到各个 shard, 要么是分摊存储压力(多节点, 每个节点不承担一部分存储), 要么是分摊计算压力(多节点, 每个节点承担一部分计算), 总之这个在分布式环境下比较容易理解. 这里如此设计目的我认为是后者, 即计算压力, 更明确地讲是避免针对 ShardedLRUCache 锁粒度过大导致访问变为串行. 每个 shard 持有各自的锁, 这样可以尽可能地实现并行处理. 这里的 sharding 实质上是实现了 Java 里的 Lock Striping. 3.1.1 sharding 存储ShardedLRUCache 有一个成员, 它包含一个 shard 数组, 每个 shard 就是一个 LRUCache: LRUCache shard_[kNumShards]; 每个 cache 默认 16 个 shards. 3.1.2 增插入数据的时候, 先计算 hash, 然后基于 hash 寻找对应 shard(即 LRUCache) 做真正插入操作: virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) { // 计算 hash const uint32_t hash = HashSlice(key); // 基于 hash 做 sharding return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } 3.1.3 删和插入数据类似, 先计算 hash, 定位到所在 shard, 然后再做具体删除操作: virtual void Erase(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } 3.1.4 查与插入数据类似, 先计算 hash, 定位到可能所在的 shard, 然后再做具体查询操作: virtual Handle* Lookup(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#313-删"},{"categories":null,"content":" 3.1 ShardingSharding 算法一般有两种, 基于 hash 的或者基于 range 的, 这里是基于 hash 的. 不知道大家可发现这里有个问题. 就是 ShardedLRUCache 为何要进行 sharding, 明明这个类的一个实例只存在于一个节点的内存里(这么说有点绕但为了尽可能严谨先这么表达了. 换个不严格的问法就是 leveld 非分布式, ShardedLRUCache 实例也只在一个机器上, 为啥还要搞成分片的?)? 文档和代码里没有说明, 但我觉得这个问题值得思考一下. Sharding 最明显的目的就是分摊压力到各个 shard, 要么是分摊存储压力(多节点, 每个节点不承担一部分存储), 要么是分摊计算压力(多节点, 每个节点承担一部分计算), 总之这个在分布式环境下比较容易理解. 这里如此设计目的我认为是后者, 即计算压力, 更明确地讲是避免针对 ShardedLRUCache 锁粒度过大导致访问变为串行. 每个 shard 持有各自的锁, 这样可以尽可能地实现并行处理. 这里的 sharding 实质上是实现了 Java 里的 Lock Striping. 3.1.1 sharding 存储ShardedLRUCache 有一个成员, 它包含一个 shard 数组, 每个 shard 就是一个 LRUCache: LRUCache shard_[kNumShards]; 每个 cache 默认 16 个 shards. 3.1.2 增插入数据的时候, 先计算 hash, 然后基于 hash 寻找对应 shard(即 LRUCache) 做真正插入操作: virtual Handle* Insert(const Slice\u0026 key, void* value, size_t charge, void (*deleter)(const Slice\u0026 key, void* value)) { // 计算 hash const uint32_t hash = HashSlice(key); // 基于 hash 做 sharding return shard_[Shard(hash)].Insert(key, hash, value, charge, deleter); } 3.1.3 删和插入数据类似, 先计算 hash, 定位到所在 shard, 然后再做具体删除操作: virtual void Erase(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); shard_[Shard(hash)].Erase(key, hash); } 3.1.4 查与插入数据类似, 先计算 hash, 定位到可能所在的 shard, 然后再做具体查询操作: virtual Handle* Lookup(const Slice\u0026 key) { const uint32_t hash = HashSlice(key); return shard_[Shard(hash)].Lookup(key, hash); } ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#314-查"},{"categories":null,"content":" 3.2 LRUCache前面介绍 sharding 的时候提到了 LRUCache, 它是每个 shard 的实际形态. 虽然它叫 XXCache, 但该类并不是 Cache 接口的实现(虽然干的活其实差不多)但也无所谓. 由于它是 ShardedLRUCache 的实际存储, 所以相关增删查方法都有, 这些根据字面就基本知道干啥了, 不再细说, 我们要特别说说跟 LRU 相关的部分. 3.2.1 存储成员该类包括两个循环链表, 一个是 in_use_ 链表, 一个是 lru_ 链表, 同时还有一个用于快速查询数据项是否存在的哈希表 table_. 其中: in_use_ 存的是在用的数据项 lru_ 存的是可以淘汰(以节省空间, 调用 prune() 方法会将该链表清空)也可以重新提升到 in_use_ 中的数据项 table_ 是 leveldb 自己实现的一个哈希表(当时测试随机读比 g++ 4.4.3 版本的内置 hashtable 快了大约 5%), 存储了出现在 in_use_ 和 lru_ 中的全部数据项(用于快速判断某个数据项是否在 LRUCache 中). 注意它保存了前面提到的两个链表的数据, 如果响应用户查询时发现数据项在 lru_ 中则会自动将其提升到 in_use_ 链表中. 不管是链表还是哈希表, 存储的数据项都是 LRUHandle, 它是个变长数据结构, 有必要展示下其核心部分: struct LRUHandle { void* value; void (*deleter)(const Slice\u0026, void* value); // 下面这个成员专用于在哈希表中指向与自己同一个桶中的后续元素 LRUHandle* next_hash; // 下面两个成员用于 in_use_ 或 lru_ 链表 LRUHandle* next; LRUHandle* prev; // TODO(可选): 当前 charge 大小只允许 uint32_t size_t charge; size_t key_length; // 指示该数据项是否还在 cache 中. bool in_cache; // 引用计数, 包含客户端引用数以及 cache 对该数据项引用数(1). uint32_t refs; // 基于 key 的 hash, 用于 sharding 和比较. uint32_t hash; // key 的起始字符, 注意这个地方有个 trick, // 因为 key 本来是变长的, 所以这里需要 // 将 key_data 作为本数据结构最后一个元素, // 方便构造时根据 key 实际大小延伸. char key_data[1]; Slice key() const { // 仅当当前 LRUHandle 作为一个空列表的 dummy head 时, 下面的 // 断言才不成立. dummy head 不保存任何数据. assert(next != this); return Slice(key_data, key_length); } }; 几个关键点: 为啥有 next_hash? 由于 LRUCache 若仍在缓存则必定被哈希表和其中一个链表(in_use_ 或 lru_)引用, 且哈希表是基于桶的, 所以一个 next 成员就不够用了, 所以专门定义了一个 next_hash 用于哈希表桶里面的链接. 为啥单独保存 key? 变长部分就是保存 key 的 key_data. 这里有个疑问, 为啥 value 用的指针, 而这里要搞个变长结构单独保存一遍 key? 通过查看 Cache::Lookup(key) 的调用可以发现, 外部传入的 key 都是栈上维护的临时变量, 插入 cache 的时候就需要保存, 于是就有了这里的节省内存的变长结构. 引用计数 refs, 它的重要意义前面讲过了, 引用计数设计贯穿了整个 leveldb 的实现, 不再说了. 3.2.2 LRU 算法如何生效的LRUCache 的 prune() 方法会直接清除 lru_ 链表内容, 无差别地清除, 所以这里未体现 LRU 思想, 那在哪儿体现的呢? 在 insert() 方法里. 在 insert() 方法最后有这么一段代码(我是真不喜欢这类有副作用的设计): ... // 如果本 shard 的内存使用量大于容量并且 lru_ 链表不为空, // 则从 lru_ 链表里面淘汰数据项(lru_ 链表数据当前肯定未被使用), // 直至使用量小于容量或者 lru_ 清空. while (usage_ \u003e capacity_ \u0026\u0026 lru_.next != \u0026lru_) { // 这很重要, lru_.next 是 least recently used 的元素 LRUHandle* old = lru_.next; // lru 链表里面的数据项除了被该 shard 引用不会被任何客户端引用 assert(old-\u003erefs == 1); // 从 shard 将 old 彻底删除 bool erased = FinishErase(table_.Remove(old-\u003ekey(), old-\u003ehash)); if (!erased) { // to avoid unused variable when compiled NDEBUG assert(erased); } } ... 这个循环最最重要的是这一行: LRUHandle* old = lru_.next; 因为新加入的元素都被插在 lru_.pre 位置, 所以从 lru_.next 开始遍历就是从最老那个元素遍历. 就是这么简单. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#32-lrucache"},{"categories":null,"content":" 3.2 LRUCache前面介绍 sharding 的时候提到了 LRUCache, 它是每个 shard 的实际形态. 虽然它叫 XXCache, 但该类并不是 Cache 接口的实现(虽然干的活其实差不多)但也无所谓. 由于它是 ShardedLRUCache 的实际存储, 所以相关增删查方法都有, 这些根据字面就基本知道干啥了, 不再细说, 我们要特别说说跟 LRU 相关的部分. 3.2.1 存储成员该类包括两个循环链表, 一个是 in_use_ 链表, 一个是 lru_ 链表, 同时还有一个用于快速查询数据项是否存在的哈希表 table_. 其中: in_use_ 存的是在用的数据项 lru_ 存的是可以淘汰(以节省空间, 调用 prune() 方法会将该链表清空)也可以重新提升到 in_use_ 中的数据项 table_ 是 leveldb 自己实现的一个哈希表(当时测试随机读比 g++ 4.4.3 版本的内置 hashtable 快了大约 5%), 存储了出现在 in_use_ 和 lru_ 中的全部数据项(用于快速判断某个数据项是否在 LRUCache 中). 注意它保存了前面提到的两个链表的数据, 如果响应用户查询时发现数据项在 lru_ 中则会自动将其提升到 in_use_ 链表中. 不管是链表还是哈希表, 存储的数据项都是 LRUHandle, 它是个变长数据结构, 有必要展示下其核心部分: struct LRUHandle { void* value; void (*deleter)(const Slice\u0026, void* value); // 下面这个成员专用于在哈希表中指向与自己同一个桶中的后续元素 LRUHandle* next_hash; // 下面两个成员用于 in_use_ 或 lru_ 链表 LRUHandle* next; LRUHandle* prev; // TODO(可选): 当前 charge 大小只允许 uint32_t size_t charge; size_t key_length; // 指示该数据项是否还在 cache 中. bool in_cache; // 引用计数, 包含客户端引用数以及 cache 对该数据项引用数(1). uint32_t refs; // 基于 key 的 hash, 用于 sharding 和比较. uint32_t hash; // key 的起始字符, 注意这个地方有个 trick, // 因为 key 本来是变长的, 所以这里需要 // 将 key_data 作为本数据结构最后一个元素, // 方便构造时根据 key 实际大小延伸. char key_data[1]; Slice key() const { // 仅当当前 LRUHandle 作为一个空列表的 dummy head 时, 下面的 // 断言才不成立. dummy head 不保存任何数据. assert(next != this); return Slice(key_data, key_length); } }; 几个关键点: 为啥有 next_hash? 由于 LRUCache 若仍在缓存则必定被哈希表和其中一个链表(in_use_ 或 lru_)引用, 且哈希表是基于桶的, 所以一个 next 成员就不够用了, 所以专门定义了一个 next_hash 用于哈希表桶里面的链接. 为啥单独保存 key? 变长部分就是保存 key 的 key_data. 这里有个疑问, 为啥 value 用的指针, 而这里要搞个变长结构单独保存一遍 key? 通过查看 Cache::Lookup(key) 的调用可以发现, 外部传入的 key 都是栈上维护的临时变量, 插入 cache 的时候就需要保存, 于是就有了这里的节省内存的变长结构. 引用计数 refs, 它的重要意义前面讲过了, 引用计数设计贯穿了整个 leveldb 的实现, 不再说了. 3.2.2 LRU 算法如何生效的LRUCache 的 prune() 方法会直接清除 lru_ 链表内容, 无差别地清除, 所以这里未体现 LRU 思想, 那在哪儿体现的呢? 在 insert() 方法里. 在 insert() 方法最后有这么一段代码(我是真不喜欢这类有副作用的设计): ... // 如果本 shard 的内存使用量大于容量并且 lru_ 链表不为空, // 则从 lru_ 链表里面淘汰数据项(lru_ 链表数据当前肯定未被使用), // 直至使用量小于容量或者 lru_ 清空. while (usage_ \u003e capacity_ \u0026\u0026 lru_.next != \u0026lru_) { // 这很重要, lru_.next 是 least recently used 的元素 LRUHandle* old = lru_.next; // lru 链表里面的数据项除了被该 shard 引用不会被任何客户端引用 assert(old-\u003erefs == 1); // 从 shard 将 old 彻底删除 bool erased = FinishErase(table_.Remove(old-\u003ekey(), old-\u003ehash)); if (!erased) { // to avoid unused variable when compiled NDEBUG assert(erased); } } ... 这个循环最最重要的是这一行: LRUHandle* old = lru_.next; 因为新加入的元素都被插在 lru_.pre 位置, 所以从 lru_.next 开始遍历就是从最老那个元素遍历. 就是这么简单. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#321-存储成员"},{"categories":null,"content":" 3.2 LRUCache前面介绍 sharding 的时候提到了 LRUCache, 它是每个 shard 的实际形态. 虽然它叫 XXCache, 但该类并不是 Cache 接口的实现(虽然干的活其实差不多)但也无所谓. 由于它是 ShardedLRUCache 的实际存储, 所以相关增删查方法都有, 这些根据字面就基本知道干啥了, 不再细说, 我们要特别说说跟 LRU 相关的部分. 3.2.1 存储成员该类包括两个循环链表, 一个是 in_use_ 链表, 一个是 lru_ 链表, 同时还有一个用于快速查询数据项是否存在的哈希表 table_. 其中: in_use_ 存的是在用的数据项 lru_ 存的是可以淘汰(以节省空间, 调用 prune() 方法会将该链表清空)也可以重新提升到 in_use_ 中的数据项 table_ 是 leveldb 自己实现的一个哈希表(当时测试随机读比 g++ 4.4.3 版本的内置 hashtable 快了大约 5%), 存储了出现在 in_use_ 和 lru_ 中的全部数据项(用于快速判断某个数据项是否在 LRUCache 中). 注意它保存了前面提到的两个链表的数据, 如果响应用户查询时发现数据项在 lru_ 中则会自动将其提升到 in_use_ 链表中. 不管是链表还是哈希表, 存储的数据项都是 LRUHandle, 它是个变长数据结构, 有必要展示下其核心部分: struct LRUHandle { void* value; void (*deleter)(const Slice\u0026, void* value); // 下面这个成员专用于在哈希表中指向与自己同一个桶中的后续元素 LRUHandle* next_hash; // 下面两个成员用于 in_use_ 或 lru_ 链表 LRUHandle* next; LRUHandle* prev; // TODO(可选): 当前 charge 大小只允许 uint32_t size_t charge; size_t key_length; // 指示该数据项是否还在 cache 中. bool in_cache; // 引用计数, 包含客户端引用数以及 cache 对该数据项引用数(1). uint32_t refs; // 基于 key 的 hash, 用于 sharding 和比较. uint32_t hash; // key 的起始字符, 注意这个地方有个 trick, // 因为 key 本来是变长的, 所以这里需要 // 将 key_data 作为本数据结构最后一个元素, // 方便构造时根据 key 实际大小延伸. char key_data[1]; Slice key() const { // 仅当当前 LRUHandle 作为一个空列表的 dummy head 时, 下面的 // 断言才不成立. dummy head 不保存任何数据. assert(next != this); return Slice(key_data, key_length); } }; 几个关键点: 为啥有 next_hash? 由于 LRUCache 若仍在缓存则必定被哈希表和其中一个链表(in_use_ 或 lru_)引用, 且哈希表是基于桶的, 所以一个 next 成员就不够用了, 所以专门定义了一个 next_hash 用于哈希表桶里面的链接. 为啥单独保存 key? 变长部分就是保存 key 的 key_data. 这里有个疑问, 为啥 value 用的指针, 而这里要搞个变长结构单独保存一遍 key? 通过查看 Cache::Lookup(key) 的调用可以发现, 外部传入的 key 都是栈上维护的临时变量, 插入 cache 的时候就需要保存, 于是就有了这里的节省内存的变长结构. 引用计数 refs, 它的重要意义前面讲过了, 引用计数设计贯穿了整个 leveldb 的实现, 不再说了. 3.2.2 LRU 算法如何生效的LRUCache 的 prune() 方法会直接清除 lru_ 链表内容, 无差别地清除, 所以这里未体现 LRU 思想, 那在哪儿体现的呢? 在 insert() 方法里. 在 insert() 方法最后有这么一段代码(我是真不喜欢这类有副作用的设计): ... // 如果本 shard 的内存使用量大于容量并且 lru_ 链表不为空, // 则从 lru_ 链表里面淘汰数据项(lru_ 链表数据当前肯定未被使用), // 直至使用量小于容量或者 lru_ 清空. while (usage_ \u003e capacity_ \u0026\u0026 lru_.next != \u0026lru_) { // 这很重要, lru_.next 是 least recently used 的元素 LRUHandle* old = lru_.next; // lru 链表里面的数据项除了被该 shard 引用不会被任何客户端引用 assert(old-\u003erefs == 1); // 从 shard 将 old 彻底删除 bool erased = FinishErase(table_.Remove(old-\u003ekey(), old-\u003ehash)); if (!erased) { // to avoid unused variable when compiled NDEBUG assert(erased); } } ... 这个循环最最重要的是这一行: LRUHandle* old = lru_.next; 因为新加入的元素都被插在 lru_.pre 位置, 所以从 lru_.next 开始遍历就是从最老那个元素遍历. 就是这么简单. ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#322-lru-算法如何生效的"},{"categories":null,"content":" 4 总结本文介绍了 leveldb 的缓存相关设计和实现, 缓存用于保存 sstable 文件的内存形式, 可加速用户查询过程. Leveldb 缓存基于 hash 做 sharding 同时支持 LRU 淘汰机制, 前者减小了锁粒度提升了并发性, 后者在内存占用和缓存命中之间实现了折中. –End— ","date":"2021-06-29","objectID":"/leveldb-annotations-6-table-cache/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之六: 文件缓存设计与实现","uri":"/leveldb-annotations-6-table-cache/#4-总结"},{"categories":null,"content":"leveldb, leveldb, 每个 level 保存的内容就是一组 sorted string table (简称 sstable) 文件. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#"},{"categories":null,"content":" 1 sstable 文件布局SSTable 即 sorted string table, 是一个有序文件格式. 该文件主要包含五个部分: 一系列 data blocks, 这里保存的我们需要的数据. 一系列 meta blocks, 这里目前保存的只有布隆过滤器. 通过它, 在不解析 data blocks 的前提下就能知道某个 key 是否存在, 如果可能存在也能快速缩小到可能在哪个 data block. 一个 metaindex block, 包含指向 meta blocks 的索引. 一个 index block, 包含指向 data blocks 的索引. 一个 footer, sstable 文件入口, 保存着指向 metaindex block 和 index block 的索引, 相当于一个二级指针. 不像 kafka 文件存储结构的数据文件和索引文件是各自独立的(在查询时根据具体 key 先在索引文件确定是哪个数据文件), sstable 把索引和数据保存到了同一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 目标 block 起始位置在文件中的偏移量 offset: varint64 // 目标 block 的大小 size: varint64 形象地说, sstable 文件具体布局如下: \u003cbeginning_of_file\u003e [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) \u003cend_of_file\u003e 下面具体讲一下每个段的具体布局. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#1-sstable-文件布局"},{"categories":null,"content":" 1.1 data block 布局每个 block 包含的数据笼统地讲, 包含 \u003c一系列数据项 + restart array + restart number\u003e “. 为了节省存储空间, block 中的数据项的 key 使用了前缀压缩. 具体来说, 存储某个 key 的时候先计算它和前一个数据项 key 的公共前缀长度, 公共前缀不再重复存储而是仅记录一个长度(shared), 由于 block 保存的数据是按 key 有序的, 排在一起的前缀都是比较相近的, 而且相似前缀可能还比较长所以该策略可以大幅节省存储空间. block 中有一个至关重要的概念, 叫 restart point. 这个概念和前面提到的前缀压缩密切相关, 每个 block 的前缀压缩不是从第一个数据项开始就一直下去, 而是每隔一段(间隔可配置)设置一个新的前缀压缩起点(作为新起点的数据项的 key 保存原值而非做前缀压缩), restart point 指的就是新起点, 从这个地方开始继续做前缀压缩. block 中每个数据项的格式如下: shared_bytes: varint32(与前一个 key 公共前缀的长度). 注意, 如果该数据项位于 restart 处, 则 shared_bytes 等于 0. unshared_bytes: varint32(当前 key 除去公共前缀后的长度) value_length: varint32(当前 key 对应的 value 的长度) key_delta: char[unshared_bytes](当前 key 除去共享前缀后的字节内容) value: char[value_length](当前 key 对应的 value 的数据内容) block 结尾处有个 trailer, 格式如下: restarts: uint32[num_restarts](保存 restart points 在 block 内偏移量的数组) num_restarts: uint32(restart points 偏移量数组大小) restarts[i] 保存的是第 i 个 restart point 在 block 内的偏移量. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#11-data-block-布局"},{"categories":null,"content":" 1.2 meta block 布局它由 \u003c一系列 filters + filter-offset 数组 + filters 部分的结束偏移量(4 字节) + base log 值(1 字节)\u003e 构成. 注意该 block 最后 5 字节内容是固定的, 这也是该部分的解析入口. 该部分在写入文件时不进行压缩. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#12-meta-block-布局"},{"categories":null,"content":" 1.3 meta-index block 布局只有一个数据项, key 为 \"filter.\"+过滤器名, value 为 meta block 的 handle. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#13-meta-index-block-布局"},{"categories":null,"content":" 1.4 index block 布局同 data block, 每个数据项的 key 是某个 data block 的最后一个 key, 每个数据项的 value 是这个 data block 的 handle. 注意, 由于该 block 数据项数和 data blocks 个数一样, 相对来说非常少, 所以就没做前缀压缩(具体实现就是将 restart point interval 设置为 1). ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#14-index-block-布局"},{"categories":null,"content":" 1.5 footer 布局Footer 虽然位于 sstable 文件尾部, 但它是名副其实的文件入口, 它的长度固定, 很容易从文件尾定位到, 它包含: 一个指向 metaindex block 的 BlockHandle 一个指向 index block 的 BlockHandle 一个 magic number. Footer 具体格式如下: // 指向 metaindex block 的 BlockHandle metaindex_handle: char[p]; // 指向 index block 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. 关于 sstable 的其它细节请见 Leveldb 源码详解系列之一: 接口与文件. 了解了布局, 下面让我们来看看针对 sstable 的读写实现. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:1:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#15-footer-布局"},{"categories":null,"content":" 2 sstable 文件的序列化与反序列化sstable 文件集合保存着 leveldb 实例的数据, 定义在 db/version_set.h 中的 class leveldb::Version 跟踪每个 level 及其文件, 可以将这个类看做是对 leveldb 全部层级文件架构的抽象. 下面说明一下针对 sstable 文件的序列化和反序列化. ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#2-sstable-文件的序列化与反序列化"},{"categories":null,"content":" 2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. 2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 \u003ckey,value\u003e 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 \u003ckey,value\u003e 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 dat","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#21-sstable-文件序列化"},{"categories":null,"content":" 2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. 2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 dat","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#211-总干事-tablebuilder"},{"categories":null,"content":" 2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. 2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 dat","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#2111-tablebuilder-的存储小助手-rep"},{"categories":null,"content":" 2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. 2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 dat","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#212-写-data-blocks"},{"categories":null,"content":" 2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. 2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 dat","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#213-写-metafilter-block"},{"categories":null,"content":" 2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. 2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 dat","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#214-写-meta-index-block"},{"categories":null,"content":" 2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. 2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 dat","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#215-写-data-index-block"},{"categories":null,"content":" 2.1 sstable 文件序列化完成该工作的是 class leveldb::TableBuilder, 该类负责构造 sstable 文件. 具体构造和写入顺序为: 写 data blocks 写 meta blocks(目前仅有过滤器) 写 meta-index block 写 data-index block 写 footer 每个分段也都有类似 XXBuilder 的类, 具体构造时会被 TableBuilder 调用. 除此之外, 还有一个类似的地方, 就是每个 XXBuilder 主要干活的基本都叫做 Add() 和 Finish() , 前者负责将具体数据添加到自己分段中, 后者负责将本段的元数据追加到自己分段尾部从而完成分段构造. 具体执行过程中, 各个 XXBuilder 有交叉的地方. 典型地, BlockBuilder 构造 data block 时会将自己的 BlockHandle 保存到 index block, 同时会将自己的 key 添加到 filter block 的相关状态里. 具体下面详述. 2.1.1 总干事 TableBuilder该类是构造 sstable 的入口, 外部(如 leveldb::BuildTable() 方法在被 leveldb::DBImpl::WriteLevel0Table() 方法调用将 memtable 转为 sstable 的时候)直接循环调用该类的 Add() 方法来向 sstable 追加 k,v 数据, 追加完毕后调用该类 Finish() 方法做收尾工作. 下面列出 TableBuilder 比较核心的成员: // 该类用于构造 sstable(sorted string table) 文件. // // 如果用户从多个线程调用该类的 const 方法, 线程安全; // 如果从多个线程调用非 const 方法, 则需要依赖外部同步设施确保线程安全. class LEVELDB_EXPORT TableBuilder { public: // 将一对 追加到正在构造的 table 中. // 要求 1: key 必须大于任何之前已经添加过的 keys, // 因为该文件是有序的. // 要求 2: 还没调用过 Finish() 或者 Abandon(), // 调用了这两个方法表示 table 对应文件被关掉了. void Add(const Slice\u0026 key, const Slice\u0026 value); // 该方法由 Add() 和 Finish() 调用, 将缓冲的 data block 写入文件. // 要求: 还没调用过 Finish() 或者 Abandon(). void Flush(); // 完成 table 构建. 该方法返回后停止使用在构造方法中传入的文件. // 要求: 还没调用过 Finish() 或者 Abandon(). // table 构成: data blocks, filter block, metaindex block, index block Status Finish(); private: // 将 data block 内容根据设置进行压缩, 然后写入文件; // 同时将 data block 在 table 偏移量和 size 设置到 // handle 中, 写完 block 会将其 handle 写入 // index block. void WriteBlock(BlockBuilder* block, BlockHandle* handle); // 将 block 及其 trailer(注意这个 trailer 不是 block 内部的 trailer) // 写入 table 对应的文件, // 并将 block 对应的 BlockHandle 内容保存到 handle 中. // 写失败时该方法只将错误状态记录到 r-\u003estatus, 不做其它任何处理. // 该方法由 WriteBlock 调用. void WriteRawBlock(const Slice\u0026 data, CompressionType, BlockHandle* handle); // 这个结构体很重要, 是 Table 实际存储数据的结构体, 下面单独开辟一节讲述. struct Rep; // 存储构造过程中的 table Rep* rep_; }; 主要方法有以下两个: void BlockBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) 负责向 TableBuilder 对象添加 (key, value), 该工作主要由 class leveldb::BlockBuilder::Add() 方法完成. void leveldb::TableBuilder::Finish() 负责将整个 Table 序列化为一个 sstable 文件并写入磁盘. Add() 方法在构造 data block 和 index block 时用到了 BlockBuilder 对应方法, Add() 实现如下: // 将一对 追加到正在构造的 table 中. // 该方法追加数据时会同时影响到 data block, data index block, // meta block 的构造. void TableBuilder::Add(const Slice\u0026 key, const Slice\u0026 value) { Rep* r = rep_; // 确保之前没有调用过 Finish() 或者 Abandon() assert(!r-\u003eclosed); if (!ok()) return; // 如果该条件成立则说明之前调用过 Add 添加过数据了 if (r-\u003enum_entries \u003e 0) { // 确保待添加的 key 大于之前已添加过的全部 keys assert(r-\u003eoptions.comparator-\u003eCompare(key, Slice(r-\u003elast_key)) \u003e 0); } // 需要构造一个新的 data block if (r-\u003epending_index_entry) { // 与上面紧邻的这个判断条件构成不变式, 为空表示 // 已经将写满的 data block flush 到文件了. assert(r-\u003edata_block.empty()); // 为 pending index entry 选一个合适的 key. // 下面这个函数调用结束, last_key 可能不变, 也可能长度更短(省空间)但是值更大, // 但不会 \u003e= 要追加的 key. 因为进入该方法之前关于两个参数 // 已经有了一个约束: 第一个字符串肯定小于第二个字符串, 这个上面有断言保证了. // // 为何这么做? 因为在查询数据时, 是先在 data-index block 中定位包含该数据的 // 目标 data block, 然后再转入目标 data block 中进行查找. 第一个定位靠的 // 就是这里的 last_key, 它和 data block 对应的 handle 一起构成了 data block // 在 data-index block 中的数据项. 第一个定位主要过程就是在 data-index block // 上查找第一个大于等于要查询数据的数据项, 具体见 TwoLevelIterator::Seek(). r-\u003eoptions.comparator-\u003eFindShortestSeparator(\u0026r-\u003elast_key, key); // 用于存储序列化后的 BlockHandle std::string handle_encoding; // 将刚刚 flush 过的 data block 对应的 BlockHandle 序列化 r-\u003epending_handle.EncodeTo(\u0026handle_encoding); // data index block 构造相关: // 为刚刚 flush 过的 data block 在 index block 增加一个数据项, // last_key 肯定大于等于其全部所有的 keys 且小于新的 // data block 的第一个 key. r-\u003eindex_block.Add(r-\u003elast_key, Slice(handle_encoding)); // 增加过 index entry 后, 可以将其置为 false 了. r-\u003epending_index_entry = false; } // meta block 构造相关: // 如果该 table 存在 filter block, 则将该 key 加入. // (filter block 可以用于快速定位 key 是否存在于 table 中). // 加入的 key 在 FilterBlockBuilder 中使用. if (r-\u003efilter_block != nullptr) { r-\u003efilter_block-\u003eAddKey(key); } // 用新 key 更新 last_key r-\u003elast_key.assign(key.data(), key.size()); r-\u003enum_entries++; // data block 相关: // 将 key,value 添加到 dat","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#216-写-footer"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#22-sstable-文件反序列化"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#221-总干事-table-类"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#2211-table-类的小助手之一-rep"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#2212-table-类的小助手之二-block"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#222-读-footer"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#223-读-data-index-block"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#224-读-meta-index-block"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#225-读-meta-block"},{"categories":null,"content":" 2.2 sstable 文件反序列化class leveldb::Table 可以看做是 sstable 文件的反序列化表示. 它负责对 sstable 进行反序列化并解析其内容, 该类是对 sstable 文件的抽象, 具体底层存储由 Table 的 helper 类 struct leveldb::Table::Rep 负责. 该类并不直接被客户代码调用, 用户调用 DBImpl::Get() 查询某个 key 的时候, 如果不在 memtable, 则会查询 sstable 文件, 此时会调用 VersionSet::current_::Get(), 并进而调用 leveldb::TableCache::Get() 查询被缓存的 Table 对象, 如果还没缓存文件对应的 Table 对象, 则会先读取然后将其加入缓存, 这里的读取操作就是 Table::Open() 方法提供的反序列化功能. 拿到 Table 对象后, 会调用其 InternalGet() 查询数据. 2.2.1 总干事 Table 类Table 是 sstable 文件反序列化后的内存形式, 包括 data blocks, data-index block, filter block 等, 核心成员如下: // Table 是不可变且持久化的. // Table 可以被多个线程在不依赖外部同步设施的情况下安全地访问. class LEVELDB_EXPORT Table { public: // 打开一个保存在 file 中 [0..file_size) 里的 // 有序 table, 并读取必要的 metadata 数据项 // 以从该 table 检索数据. // // 如果成功, 返回 OK 并将 *table 设置为新打开 // 的 table. 当不再使用该 table 时候, 客户端负责删除之. // 如果在初始化 table 出错, 将 *table 设置 // 为 nullptr 并返回 non-OK. // 而且, 在 table 打开期间, 客户端要确保数据源持续有效, // 即当 table 在使用过程中, *file 必须保持有效. static Status Open(const Options\u0026 options, RandomAccessFile* file, uint64_t file_size, Table** table); // 返回一个基于该 table 内容的迭代器. // 该方法返回的结果默认是无效的(在使用该迭代器之前, // 调用者在使用前必须调用其中一个 Seek 方法来 // 使迭代器生效.) Iterator* NewIterator(const ReadOptions\u0026) const; private: struct Rep; Rep* rep_; // Seek(key) 找到某个数据项则会自动 // 调用 (*handle_result)(arg, ...); // 如果过滤器明确表示不能做则不会调用. friend class TableCache; Status InternalGet( const ReadOptions\u0026, const Slice\u0026 key, void* arg, void (*handle_result)(void* arg, const Slice\u0026 k, const Slice\u0026 v)); void ReadMeta(const Footer\u0026 footer); void ReadFilter(const Slice\u0026 filter_handle_value); }; 读取 sstable 的入口为 Table::Open() 方法, 读取过程和 sstable 布局密切相关: 读 footer(这是文件入口), 读 data-index block, 再读 meta-index block 和 meta block. 没错, 该方法没有读取 data block. 该方法最后返回一个 class leveldb::Table 对象, 该对象会被调用方用作查询数据使用. 具体代码如下: // 将 file 表示的 sstable 文件反序列化为 Table 对象, 具体保存 // 实际内容的是 Table::rep_. // // 如果成功, 返回 OK 并将 *table 设置为新打开的 table. // 当不再使用该 table 时候, 需要调用方负责删除之. // 如果初始化 table 出错, 将 *table 设置为 nullptr 并返回 non-OK. // 注意, 在 table 打开期间, 调用方要确保数据源即 file 持续有效. Status Table::Open(const Options\u0026 options, RandomAccessFile* file, uint64_t size, Table** table) { /** * 1 解析 footer: 它是 sstable 的入口. */ *table = nullptr; // 每个 table 文件末尾是一个固定长度的 footer if (size \u003c Footer::kEncodedLength) { return Status::Corruption(\"file is too short to be an sstable\"); } char footer_space[Footer::kEncodedLength]; Slice footer_input; // 读取 footer, 放到 footer_input Status s = file-\u003eRead(size - Footer::kEncodedLength, Footer::kEncodedLength, \u0026footer_input, footer_space); if (!s.ok()) return s; Footer footer; // 解析 footer s = footer.DecodeFrom(\u0026footer_input); if (!s.ok()) return s; /** * 2 解析 data-index block: * 根据已解析的 Footer, 解析出 index block(它保存了指向全部 data blocks 的索引) * 存储到 index_block_contents. */ BlockContents index_block_contents; if (s.ok()) { ReadOptions opt; if (options.paranoid_checks) { opt.verify_checksums = true; } // 读取 index block, 它对应的 BlockHandle 存储在 footer 里面 s = ReadBlock(file, opt, footer.index_handle(), \u0026index_block_contents); } if (s.ok()) { // 已经成功读取了 Footer 和 index block, 是时候读取 data 了. Block* index_block = new Block(index_block_contents); Rep* rep = new Table::Rep; rep-\u003eoptions = options; rep-\u003efile = file; // filter-index block 对应的指针 (二级索引), 解析 footer 时候就拿到了. rep-\u003emetaindex_handle = footer.metaindex_handle(); // data-index block // (注意它只是一个索引, 即 data blocks 的索引, // 真正使用的时候是基于 data-index block 做二级迭代器来进行查询, // 一级索引跨度大, 二级索引粒度小, 可以快速定位数据, // 具体见 Table::NewIterator() 方法) rep-\u003eindex_block = index_block; // 如果调用方要求缓存这个 table, 则为其分配缓存 id rep-\u003ecache_id = (options.block_cache ? options.block_cache-\u003eNewId() : 0); // 接下来跟 filter 相关的两个成员将在下面 ReadMeta 进行填充. rep-\u003efilter_data = nullptr; rep-\u003efilter = nullptr; *table = new Table(rep); /** * 3 解析 meta-index block 和 meta block: * 根据已解析的 Footer 所包含的 metaindex block 指针, * 解析出 metaindex block, 再基于此解析出 mate block * 存储到 Table::rep_. */ // 读取并解析 filter block 到 table::rep_, // 它一般为布隆过滤器, 可以加速数据查询过程. (*table)-\u003eReadMeta(footer); } // 是的, 该方法没有解析 data blocks. return s; } 总结下, 该方法主要干了下面三件事: 先解析 sstable 文件结尾的 Foote","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#226-读-block-内容的通用方法"},{"categories":null,"content":" 2.3 Table 和两级迭代器的结合前面讲过了, 打开 sstable 文件后会生成对应的 Table 对象, 该对象会被放到 TableCache 缓存中. 如果要访问其内容, 需要一个迭代器, 该工作通过 leveldb::Iterator *leveldb::Table::NewIterator 完成: // 先为 data-index block 数据项构造一个迭代器 index_iter, // 然后基于 index_iter 查询时, 为其指向的具体 data block // 构造一个迭代器 data_iter, 进而可以迭代该 data block 里 // 的全部数据项. // 这样就构成了一个两级迭代器, 从而实现遍历全部 data blocks 的数据项. Iterator* Table::NewIterator(const ReadOptions\u0026 options) const { return NewTwoLevelIterator( rep_-\u003eindex_block-\u003eNewIterator(rep_-\u003eoptions.comparator), \u0026Table::BlockReader, const_cast\u003cTable*\u003e(this), options); } 返回的迭代器为一个 leveldb::\u003cunnamed\u003e::TwoLevelIterator, 该迭代器处于匿名的命名空间所以未直接对外暴露, 仅能通过返回的指针访问其从 class leveldb::Iterator 继承的方法. 每个 sstable 文件对应一个两级迭代器, 然后将全部 sstable 对应的两级迭代器级联起来, 就相当于为整个 leveldb 构造了一个迭代器(见 leveldb::Version::AddIterators(), 后续会详解该类), 从而实现在整个 leveldb 上轻松实现迭代或者查询. –End– ","date":"2021-05-29","objectID":"/leveldb-annotations-5-sstable/:2:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之五: SSTable 设计与实现","uri":"/leveldb-annotations-5-sstable/#23-table-和两级迭代器的结合"},{"categories":null,"content":"本文基于内部分享 \u003c“抄\"能力养成系列 – MapReduce: 分布式计算系统设计与实现\u003e 整理. 2003 年开始 Google 陆续放出三套系统的设计(GFS/MapReduce/Bigtable), 在互联网届掀起云计算狂潮一直影响至今. MapReduce 作为老二出场, 因为它的实现依赖于之前分享的 GFS 作为存储. 该论文一出, 便直接催生了 Hadoop 另一个重量级同名框架 MapReduce 的诞生. 时光荏苒, 虽然后面又出现了 spark/flink, 但是 MapReduce 在批处理领域的地位至今牢固. 下面就让我们一起看看 MapReduce 的设计, 希望为各位后续系统研发提供灵感. (Salute to Jeff). ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#"},{"categories":null,"content":" 1 简要介绍这个模型说起来真是简单至极, 非常符合直觉, 就是说给非互联网行业的人, 也能听明白. 该模型跟函数式语言中的 map-reduce 理念基本一样, 不过这个是分布式的. map 用于处理原始记录, 输出中间 \u003ck, v\u003e; reduce 基于 k 把中间数据合并, 输出 \u003ck, List\u003cv\u003e\u003e. 并行化 容错 数据分发 负载均衡 最主要的容错机制就是支持重跑任务. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#1-简要介绍"},{"categories":null,"content":" 2 编程模型用户要写的就是 Map 函数和 Reduce 函数. Map 负责将输入加工成中间 kv; MapReduce 库负责将同一个 k 的全部 v 收集好发给 Reduce; Reduce 接收中间数据, 然后基于 k, 合并 v, 一般输出一个或零个值. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#2-编程模型"},{"categories":null,"content":" 2.1 举例以单词计数为例, 用户需要干的就是实现自己的 map 函数和 reduce 函数. 剩下的事情由框架负责. // key: 文档名 // value: 文档内容 map(String key, String value): // 遍历文档, 每个词输出一个键值对 for each word w in value: EmitIntermediate(w, \"1\"); // key: 单词 // value: 计数值构成的列表 reduce(String key, Iterator values): // 累加器 int result = 0; // 遍历每个计数值, 将其累加起来 for each v in values: result += ParseInt(v); // 得到每个单词出现的次数 Emit(AsString(result)); ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#21-举例"},{"categories":null,"content":" 2.2 输入输出类型 // 输入为两个参数, k1 类型的键, v1 类型的值; // 返回值是一个键值对, 每个键值对是一个 \u003ck2 类型的键, v2 类型的值\u003e, // 整体效果上看相当于输出了一个键值对列表. map (k1,v1) → list(k2,v2) // 输入为两个参数, k2 类型的键, 以及其对应的 v2 类型的值的列表; // 返回值是 v2 类型的值, 效果上看相当于输出了一个列表. reduce (k2,list(v2)) → list(v2) ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#22-输入输出类型"},{"categories":null,"content":" 2.3 使用场景举例 分布式 grep map 函数处理模式匹配, 一旦匹配输出一行; reduce 函数是一个等价函数(啥也不干), 将中间结果拷贝到输出. URL 访问计数 map 函数负责处理每个页面的请求日志, 每处理一行便输出 \u003cURL, 1\u003e. reduce 函数负责将 URL 一样的值累加, 返回的是 \u003cURL, 累加值\u003e web 站点链接反转 map 函数针对每个在叫 source 的页面中发现的链接 target, 输出 \u003ctarget, source\u003e. reduce 将每个 target 对应的 source 收集为一个列表并输出, 形如 \u003ctarget, list\u003csource\u003e\u003e. Term-Vector per Host 背景: 检索词向量是对一个文档或者文档集合中最重要的单词及其词频的统计, 形式为 \u003cword, frequency\u003e 列表. map 函数针对每个输入文档, 输出 \u003chostname, term vector\u003e, 其中 hostname 是从文档对应的 URL 中抽取得到. reduce 函数负责处理给定 host 的每个文档的检索词向量, 它将这些词向量加在在一起去除低频检索词, 输出 \u003chostname, term vector\u003e 键值对. 倒排索引 map 函数解析每个文档, 输出一各 \u003cword, document ID\u003e 序列. reduce 函数接受给定单词的全部键值对, 将对应的 document IDs 排序, 输出一个 \u003cword, list(document ID)\u003e 键值对. 分布式排序 map 函数从每个 record 抽取 key, 输出一个 \u003ckey, record\u003e 键值对. reduce 函数原封不动地输出键值对. 该计算场景依赖于后面将要描述的分区设施和排序属性. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:2:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#23-使用场景举例"},{"categories":null,"content":" 3 实现模型很简单, 具体实现取决于硬件环境. 以 Google 为例(快二十年前的数据了): 双核 x86 处理器, 运行 Linux, 2-4GB 内存. 普通商用网络, 100Mb/s. 几百上千台上述机器构成的集群. 数据就保存在计算节点上, 普通的 IDE 磁盘. 不过这些数据由 GFS 管理, 确保高可用. 用户将 job 提交到调度系统. 每个 job 由多个 tasks 构成, 每个 job 被调度器映射到集群内的一组机器. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#3-实现"},{"categories":null,"content":" 3.1 执行概览系统自动将输入数据自动切割成 M 份, 然后在对应机器上部署多个 Mapper, 每个 Mapper 负责处理若干份数据. Mapper 处理输入生成中间数据, 通过分区函数(比如 hash(key) mod R)将中间数据的键空间分成 R 份, 并在其之上部署 Reducer. 具体的分区函数和分成几份, 由用户负责指定. Figure 1 显示了一个 MapReduce 操作的执行概览. 用户程序调用 MapReduce 函数, 然后接下来框架内部陆续发生如下动作: MapReduce 将输入切分成 M 份, 并在一组机器上启动多个用户程序拷贝(fork). 上一步的 fork, 其中有 M 个 map workers, R 个 reduce workers, 还有一个特殊的作为 Master 负责分配任务. map worker 负责读取和解析输入的 key/value 并传给用户定义的 Map 函数, 后者输出中间状态的 key’/value’, 这些中间数据起初被缓存到内存中. map worker 缓存的中间数据会被周期性的写到本地磁盘, 同时会被划分成 R 个分区(如 hash(key’) mod R), 注意由于分区函数无法保证原空间和像空间一一映射, 所以每个分区的 key’ 可能不唯一(比如 R 为 7, 则 key’ 为 70 和 700 的落在同一个分区内). 这些中间数据 key’/value’ 的位置会被上报给 Master, 它会负责把这些位置信息转发给 reduce workers, 每个 reduce worker 负责一个分区. 尤其注意一点, 这一步的中间数据会被写到 map tasks 的本地磁盘, 而不是 GFS. 当 reduce worker 收到上面提到的位置信息的时候, 它发起一个 RPC 读取那个 map workers 磁盘缓存的数据. 当数据都被读取过来之后, reduce worker 根据中间 key’ 对数据进行排序, 于是相同的 keys 就会被排列到一起. 之所以需要排序, 是因为会有不同的 keys 落到同一个 reduce worker(毕竟像 hash(key’) mod R 这种算法无法保证原空间和像空间是一一映射). 如果数据大到无法装进内存, reduce worker 就会采用外部排序算法. reduce worker 迭代排序后的数据, 针对每个唯一 key’, 它会把其连同对应的一组 value’ 传给用户编写的 Reduce 函数, 该函数输出会被追加到当前 reduce 分区的文件中. 注意, 不同于 map tasks, reduce worker 的输出是写到 GFS. 当全部 map tasks 和 reduce tasks 执行完成后, master 就会唤醒用户程序. MapReduce 调用返回至用户代码. 执行成功后, mapreduce 结果保存到了 R 个输出文件中(每个 reduce 任务一个输出文件). 一般用户无需合并这 R 个文件, 因为这些文件会被作为下个阶段的 MapReduce 调用的输入, 或者作为其它可以处理多个输入文件的分布式应用的输入. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#31-执行概览"},{"categories":null,"content":" 3.2 Master 节点的作用master 保存着 map tasks 和 reduce tasks 的状态信息以及它们对应的机器 id. master 是一个将中间文件位置信息从 map 传递到 reduce 的中介. master 保存 map tasks 生成的 R 个中间文件区域的位置信息和大小, 因为 map task 成功完成而产生的这些信息的更新都会被 master 接收并增量推送给正在执行的 reduce tasks. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#32-master-节点的作用"},{"categories":null,"content":" 3.3 容错 3.3.1 worker 故障(这里的 worker 指的是机器.) master 周期性 ping 各个 worker 来检活, 如果故障了, 则 master 会在其它机器上重新调度其上跑的 tasks. 故障机器上运行中的 map task 或者 reduce task 会被被重置为 idle 状态, 因此可以在其它 workers 上再次被调度. 失败的 map tasks 会在其它机器上调度重新执行一遍, 因为它们的输出都在故障机器本地磁盘上, 所以这些数据就丢了; 但是 reduce tasks 失败后在其它机器上被调度后无需从头重新执行, 因为它们的输出在类似 GFS 的分布式文件系统中, 继续从失败处继续运行即可. 如果 map task 换机器重新执行, 那么这个情况会被告知给全部 reduce workers, 毕竟这个 map task 输出的中间数据可能会覆盖全部 reduce workers 对应的分区. MapReduce 可以容忍大批机器集体故障几分钟, 只需将故障机器上跑的任务重新在其它机器上重新调度执行就可以保证进度进行下去. 3.3.2 master 故障除了心跳以外, 周期性 checkpoint 是提升容错的另一个利器. master 可以周期性的 checkpointing 自己的状态, 如果失效, 则从最后一个 checkpoint 重启新的 master 即可. 即使是单一 master 架构, 但也容易失效, 如果失效, 客户端可以选择重试 MapReduce 计算. 3.3.3 故障存在场景下的语义保证当用户提供的 map 和 reduce 算子是其输入值的确定性函数时(绝大多数计算场景都这样)，即输入确定则输出也是确定的, 我们的分布式实现产生的输出与单体程序的无故障顺序执行所产生的输出相同。但达成这一点依赖于 map 和 reduce 的输出能原子化地提交, 下面详述. 每个进行中的 task 会生成自己的私有临时文件: 每个 reduce task 会生成一个文件; 每个 map task 会生成 R 个文件, 每个文件对应一个 reduce task. 其中, map task 被重新调度会丢弃之前的输出会重新从头计算, 成功完成后会将自己生成的 R 个文件的名字上报给 master, master 会将其记录到本地. reduce task 完成后会将自己的临时输出文件重命名为最终输出文件, 这个重命名过程是原子化的, 看过之前 GFS 分享的应该很清楚. 如果同样的 reduce task 因为故障被在多个机器上先后执行, 那么同一个最终输出文件会被重命名多次. 但由于输出都一样, 所以文件内容也都一样. 我们依赖底层文件系统如 GFS 提供的原子化重命名操作来保证最终的文件状态与同样的 reduce task 只运行一次结果相同. 前面说的都是算子是确定性函数的情形, 如果算子具有不确定性呢? 针对不确定性情形, 我们提供了比较弱但是仍合理的语义. 当存在不确定算子时, 某个 reduce task $R_{1}$ 的输出等价于一个不确定程序顺序执行时的输出, 而另一个 reduce task $R_{2}$ 的输出可能对应前述不确定程序的另一个顺序执行的输出. 考虑 map task $M$ 和 reduce tasks $R_{1}$ 和 $R_{2}$, 令 $e(R_{i})$ 代表 $R_{i}$ 的执行过程(一次恰好仅有一个该执行过程, 因为只有 task 故障了才会执行另一个). 因为 $e(R_{1})$ 可能读取了 $M$ 某次执行的输出, 而 $e(R_{2})$ 可能读取了 $M$ 的另一次执行的输出, 所以弱语义就保证了. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#33-容错"},{"categories":null,"content":" 3.3 容错 3.3.1 worker 故障(这里的 worker 指的是机器.) master 周期性 ping 各个 worker 来检活, 如果故障了, 则 master 会在其它机器上重新调度其上跑的 tasks. 故障机器上运行中的 map task 或者 reduce task 会被被重置为 idle 状态, 因此可以在其它 workers 上再次被调度. 失败的 map tasks 会在其它机器上调度重新执行一遍, 因为它们的输出都在故障机器本地磁盘上, 所以这些数据就丢了; 但是 reduce tasks 失败后在其它机器上被调度后无需从头重新执行, 因为它们的输出在类似 GFS 的分布式文件系统中, 继续从失败处继续运行即可. 如果 map task 换机器重新执行, 那么这个情况会被告知给全部 reduce workers, 毕竟这个 map task 输出的中间数据可能会覆盖全部 reduce workers 对应的分区. MapReduce 可以容忍大批机器集体故障几分钟, 只需将故障机器上跑的任务重新在其它机器上重新调度执行就可以保证进度进行下去. 3.3.2 master 故障除了心跳以外, 周期性 checkpoint 是提升容错的另一个利器. master 可以周期性的 checkpointing 自己的状态, 如果失效, 则从最后一个 checkpoint 重启新的 master 即可. 即使是单一 master 架构, 但也容易失效, 如果失效, 客户端可以选择重试 MapReduce 计算. 3.3.3 故障存在场景下的语义保证当用户提供的 map 和 reduce 算子是其输入值的确定性函数时(绝大多数计算场景都这样)，即输入确定则输出也是确定的, 我们的分布式实现产生的输出与单体程序的无故障顺序执行所产生的输出相同。但达成这一点依赖于 map 和 reduce 的输出能原子化地提交, 下面详述. 每个进行中的 task 会生成自己的私有临时文件: 每个 reduce task 会生成一个文件; 每个 map task 会生成 R 个文件, 每个文件对应一个 reduce task. 其中, map task 被重新调度会丢弃之前的输出会重新从头计算, 成功完成后会将自己生成的 R 个文件的名字上报给 master, master 会将其记录到本地. reduce task 完成后会将自己的临时输出文件重命名为最终输出文件, 这个重命名过程是原子化的, 看过之前 GFS 分享的应该很清楚. 如果同样的 reduce task 因为故障被在多个机器上先后执行, 那么同一个最终输出文件会被重命名多次. 但由于输出都一样, 所以文件内容也都一样. 我们依赖底层文件系统如 GFS 提供的原子化重命名操作来保证最终的文件状态与同样的 reduce task 只运行一次结果相同. 前面说的都是算子是确定性函数的情形, 如果算子具有不确定性呢? 针对不确定性情形, 我们提供了比较弱但是仍合理的语义. 当存在不确定算子时, 某个 reduce task $R_{1}$ 的输出等价于一个不确定程序顺序执行时的输出, 而另一个 reduce task $R_{2}$ 的输出可能对应前述不确定程序的另一个顺序执行的输出. 考虑 map task $M$ 和 reduce tasks $R_{1}$ 和 $R_{2}$, 令 $e(R_{i})$ 代表 $R_{i}$ 的执行过程(一次恰好仅有一个该执行过程, 因为只有 task 故障了才会执行另一个). 因为 $e(R_{1})$ 可能读取了 $M$ 某次执行的输出, 而 $e(R_{2})$ 可能读取了 $M$ 的另一次执行的输出, 所以弱语义就保证了. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#331-worker-故障"},{"categories":null,"content":" 3.3 容错 3.3.1 worker 故障(这里的 worker 指的是机器.) master 周期性 ping 各个 worker 来检活, 如果故障了, 则 master 会在其它机器上重新调度其上跑的 tasks. 故障机器上运行中的 map task 或者 reduce task 会被被重置为 idle 状态, 因此可以在其它 workers 上再次被调度. 失败的 map tasks 会在其它机器上调度重新执行一遍, 因为它们的输出都在故障机器本地磁盘上, 所以这些数据就丢了; 但是 reduce tasks 失败后在其它机器上被调度后无需从头重新执行, 因为它们的输出在类似 GFS 的分布式文件系统中, 继续从失败处继续运行即可. 如果 map task 换机器重新执行, 那么这个情况会被告知给全部 reduce workers, 毕竟这个 map task 输出的中间数据可能会覆盖全部 reduce workers 对应的分区. MapReduce 可以容忍大批机器集体故障几分钟, 只需将故障机器上跑的任务重新在其它机器上重新调度执行就可以保证进度进行下去. 3.3.2 master 故障除了心跳以外, 周期性 checkpoint 是提升容错的另一个利器. master 可以周期性的 checkpointing 自己的状态, 如果失效, 则从最后一个 checkpoint 重启新的 master 即可. 即使是单一 master 架构, 但也容易失效, 如果失效, 客户端可以选择重试 MapReduce 计算. 3.3.3 故障存在场景下的语义保证当用户提供的 map 和 reduce 算子是其输入值的确定性函数时(绝大多数计算场景都这样)，即输入确定则输出也是确定的, 我们的分布式实现产生的输出与单体程序的无故障顺序执行所产生的输出相同。但达成这一点依赖于 map 和 reduce 的输出能原子化地提交, 下面详述. 每个进行中的 task 会生成自己的私有临时文件: 每个 reduce task 会生成一个文件; 每个 map task 会生成 R 个文件, 每个文件对应一个 reduce task. 其中, map task 被重新调度会丢弃之前的输出会重新从头计算, 成功完成后会将自己生成的 R 个文件的名字上报给 master, master 会将其记录到本地. reduce task 完成后会将自己的临时输出文件重命名为最终输出文件, 这个重命名过程是原子化的, 看过之前 GFS 分享的应该很清楚. 如果同样的 reduce task 因为故障被在多个机器上先后执行, 那么同一个最终输出文件会被重命名多次. 但由于输出都一样, 所以文件内容也都一样. 我们依赖底层文件系统如 GFS 提供的原子化重命名操作来保证最终的文件状态与同样的 reduce task 只运行一次结果相同. 前面说的都是算子是确定性函数的情形, 如果算子具有不确定性呢? 针对不确定性情形, 我们提供了比较弱但是仍合理的语义. 当存在不确定算子时, 某个 reduce task $R_{1}$ 的输出等价于一个不确定程序顺序执行时的输出, 而另一个 reduce task $R_{2}$ 的输出可能对应前述不确定程序的另一个顺序执行的输出. 考虑 map task $M$ 和 reduce tasks $R_{1}$ 和 $R_{2}$, 令 $e(R_{i})$ 代表 $R_{i}$ 的执行过程(一次恰好仅有一个该执行过程, 因为只有 task 故障了才会执行另一个). 因为 $e(R_{1})$ 可能读取了 $M$ 某次执行的输出, 而 $e(R_{2})$ 可能读取了 $M$ 的另一次执行的输出, 所以弱语义就保证了. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#332-master-故障"},{"categories":null,"content":" 3.3 容错 3.3.1 worker 故障(这里的 worker 指的是机器.) master 周期性 ping 各个 worker 来检活, 如果故障了, 则 master 会在其它机器上重新调度其上跑的 tasks. 故障机器上运行中的 map task 或者 reduce task 会被被重置为 idle 状态, 因此可以在其它 workers 上再次被调度. 失败的 map tasks 会在其它机器上调度重新执行一遍, 因为它们的输出都在故障机器本地磁盘上, 所以这些数据就丢了; 但是 reduce tasks 失败后在其它机器上被调度后无需从头重新执行, 因为它们的输出在类似 GFS 的分布式文件系统中, 继续从失败处继续运行即可. 如果 map task 换机器重新执行, 那么这个情况会被告知给全部 reduce workers, 毕竟这个 map task 输出的中间数据可能会覆盖全部 reduce workers 对应的分区. MapReduce 可以容忍大批机器集体故障几分钟, 只需将故障机器上跑的任务重新在其它机器上重新调度执行就可以保证进度进行下去. 3.3.2 master 故障除了心跳以外, 周期性 checkpoint 是提升容错的另一个利器. master 可以周期性的 checkpointing 自己的状态, 如果失效, 则从最后一个 checkpoint 重启新的 master 即可. 即使是单一 master 架构, 但也容易失效, 如果失效, 客户端可以选择重试 MapReduce 计算. 3.3.3 故障存在场景下的语义保证当用户提供的 map 和 reduce 算子是其输入值的确定性函数时(绝大多数计算场景都这样)，即输入确定则输出也是确定的, 我们的分布式实现产生的输出与单体程序的无故障顺序执行所产生的输出相同。但达成这一点依赖于 map 和 reduce 的输出能原子化地提交, 下面详述. 每个进行中的 task 会生成自己的私有临时文件: 每个 reduce task 会生成一个文件; 每个 map task 会生成 R 个文件, 每个文件对应一个 reduce task. 其中, map task 被重新调度会丢弃之前的输出会重新从头计算, 成功完成后会将自己生成的 R 个文件的名字上报给 master, master 会将其记录到本地. reduce task 完成后会将自己的临时输出文件重命名为最终输出文件, 这个重命名过程是原子化的, 看过之前 GFS 分享的应该很清楚. 如果同样的 reduce task 因为故障被在多个机器上先后执行, 那么同一个最终输出文件会被重命名多次. 但由于输出都一样, 所以文件内容也都一样. 我们依赖底层文件系统如 GFS 提供的原子化重命名操作来保证最终的文件状态与同样的 reduce task 只运行一次结果相同. 前面说的都是算子是确定性函数的情形, 如果算子具有不确定性呢? 针对不确定性情形, 我们提供了比较弱但是仍合理的语义. 当存在不确定算子时, 某个 reduce task $R_{1}$ 的输出等价于一个不确定程序顺序执行时的输出, 而另一个 reduce task $R_{2}$ 的输出可能对应前述不确定程序的另一个顺序执行的输出. 考虑 map task $M$ 和 reduce tasks $R_{1}$ 和 $R_{2}$, 令 $e(R_{i})$ 代表 $R_{i}$ 的执行过程(一次恰好仅有一个该执行过程, 因为只有 task 故障了才会执行另一个). 因为 $e(R_{1})$ 可能读取了 $M$ 某次执行的输出, 而 $e(R_{2})$ 可能读取了 $M$ 的另一次执行的输出, 所以弱语义就保证了. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#333-故障存在场景下的语义保证"},{"categories":null,"content":" 3.4 数据局部性由 GFS 管理的输入数据就保存在 MapReduce 集群的磁盘上. MapReduce master 在调度 map 任务时会把输入文件的位置信息也考虑进来, 尽量把 map 任务调度到对应数据副本所在机器上, 如果该项尝试失败, 则将 map 任务调度到离着输入数据比较近的(同一局域网或同一个交换机连接的网络)机器上. 在大型 MapReduce 计算过程中, 数据局部性可以极大地减少网络消耗. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#34-数据局部性"},{"categories":null,"content":" 3.5 任务颗粒度map 任务数 M 和 reduce 任务数 R 加起来要远大于 worker 机器数. 一般一个 worker 同时执行多个任务, 这可以提升动态负载均衡. master 要做出 $O(M + R)$ 调度策略, 在内存中持有 $O(M * R)$ 个状态(每个 map/reduce 对对应状态大约一个字节). 用户一般想要控制 R 的大小, 因为每个 reduce 任务单独输出一个文件, 控制 R 可以控制最终文件个数. 我们倾向于选择大的 M 以使得每个 map 任务处理的数据量在 16MB 到 64MB 之间, 令 R 为 workers 的一个很小的倍数. 比如, 如果有 2,000 workers, 那么 R 选为 5,000, 而 M 选为 200,000. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#35-任务颗粒度"},{"categories":null,"content":" 3.6 后备任务拖长 mapreduce 计算时间的就是最后完成 map 或者 reduce 任务的机器. 原因一般是机器某些硬件比较差, 比如磁盘 IO 很慢; 或者集群调度系统(除了调度 MR 任务也调度其它的)把很多任务调度到了最后几个任务所在机器上导致资源争用严重. 我们有一个通用的缓解拖后腿问题的机制: 当一个 mapreduce 操作接近完成的时候 master 就会针对仍处于执行阶段的任务调度对应的后备任务, 不管主任务还是后备任务结束, 相关任务就会被标记为完成. 该机制大幅减少了大型 mapreduce 任务的时间消耗, 而资源消耗仅增加几个点. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:3:6","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#36-后备任务"},{"categories":null,"content":" 4 调优尽管对大部分需求, 写写 Map 和 Reduce 函数就够了, 但还是发现了一些可以优化的地方. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#4-调优"},{"categories":null,"content":" 4.1 分区函数用户在指定 reduce tasks 个数(也即输出文件个数) R 的值的时候, 也可以指定分区函数, 即如何根据中间数据 key 将数据分散到这 R 个文件. 默认的分区函数就是 hash(key) mod R. 用户可以基于具体需求指定分区函数, 比如当中间 key 是 URL 时候, 如果想把同一个网站的数据刚到同一个文件, 则可以这样 hash(Hostname(urlkey)) mod R. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#41-分区函数"},{"categories":null,"content":" 4.2 顺序保证前面讲了, reduce task 在调用 Reduce 函数之前会将本分区数据就行排序, 所以可以保证一个分区内的中间数据会按照升序处理. 这使得为每个分区生成有序输出文件变得简单, 而且也使得针对在每个分区文件进行随机 key 查询变得高效(有序就可以二分查找了). ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#42-顺序保证"},{"categories":null,"content":" 4.3 合并函数(combiner function)在某些情况下, Reduce 函数满足交换性和结合性, 比如 word counting, 此时可以在每个中间 kv 通过网络传输给 Reduce 函数之前做一件事情, 即允许用户指定一个可选的 Combiner 函数, 它负责在网络传输前先对部分数据进行合并再发送, 这可以大幅减少网络数据交互量. Combiner 函数在执行 map task 的机器上运行, 因为它针对的是 map 生成的数据. 一般情况下, Combiner 函数代码与 Reduce 函数代码就是同一份. 这两者唯一不同就是 MapReduce 库如何处理它们的输出上. Reduce 函数的输出被写到最终输出文件上, 但是 Combiner 函数输出被写到一个中间文件, 该文件内容将会被发给 reduce task. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#43-合并函数combiner-function"},{"categories":null,"content":" 4.4 输入输出类型MapReduce 库支持读取多种不同的文件类型. 比如 “text” 类型, 把每一行当作一个 key/value 对: key 是文件偏移量, value 是偏移处一行文件内容. 不管哪种文件类型, 库里对应的代码都知道如何把数据切分成有意义的范围给对应的 map task 去处理. 当然用户也可以实现相应接口来定制支持特定的文件类型. MapReduce 对输出类型的支持也像输入一样灵活. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#44-输入输出类型"},{"categories":null,"content":" 4.5 跳过坏记录有时候用户编写的代码或者依赖的第三方库有 bugs, 导致 mapreduce 任务处理不了某些数据总是挂掉, 第一种情况还好说, 第三方库的就不好搞了. 这时候你可能希望能跳过这些记录. 怎么做到呢? 每个工作进程可以安装一个信号处理器用来捕获段错误或者总线错误. 在调用用户编写的 Map 或者 Reduce 之前, MapReduce 库存储一个序列号到全局变量中. 如果用户代码生成一个信号, 那么前面提到的信号处理器就会发送一个包含前述序列号的 UDP 包给 master. 如果 master 发现, 针对某个记录已经收到不止一次上报了, 它就会在重新执行挂掉的 Map/Reduce task 时跳过这条记录. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#45-跳过坏记录"},{"categories":null,"content":" 4.6 状态信息master 除了做任务调度, 还提供了一个 HTTP server, 用户可以访问它来获取整个集群的状态统计信息. 比如计算进展, 每个 task 的输出等等. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:6","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#46-状态信息"},{"categories":null,"content":" 4.7 全局计数器MapReduce 库提供了计数器设施, 用户可以利用它在 Map/Reduce 函数中针对一些事件进行统计, 比如在单词计数应用中针对大写的单词进行统计: Counter* uppercase; uppercase = GetCounter(\"uppercase\"); map(String name, String contents): for each word w in contents: // 如果当前单词大写, 则将全局计数器加 1 if (IsCapitalized(w)): uppercase-\u003eIncrement(); EmitIntermediate(w, \"1\"); 这些计数值会周期性地随心跳响应传递到 master, 然后 master 进行聚合, 聚合时 master 会去重(比如重复调度执行的任务或为了加速完成而启动的后备任务都会造成重复计数). 用户可以从前面提到的 http server 页面查看值的变化. ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:4:7","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#47-全局计数器"},{"categories":null,"content":" 5 性能下面以排序程序为例, 说明下各阶段数据传输速率比较以及后备任务对性能的影响. 如上图所示, 横向分为三部分, 分别是正常执行情况, 无后备任务执行情况, 手动干掉 200 个进程的执行情况. 其中, 每种执行情况纵向列出三个指标, 分别是输入数据速率, 排序数据速率, 输出数据速率. 具体每个图, 横轴是时间, 纵轴是速率. a 列上图是数据读取速率, 显著快于下面的排序和输出, 这全都拜前面提到的数据局部性所赐. a 列中图是排序, 可以看到第一个 map task 完成后即启动了排序. 第一个高峰是 1700 个 reduce tasks 执行盛况(这个 sort 程序用了 1700 台机器, 每个机器执行不超过 1 个 reduce task.), 大约 300 秒后第一批数据排序完成, 然后对剩下数据进行排序, 大约 600 秒时完成全部排序任务. 从该图可以看出数据传输速率高于下面的输出, 原因是下面输出要写到 GFS 多副本, 比较耗时. a 列下图是输出, 输出就是 reduce tasks 将数据写入到 GFS. 可以看到第一批数据排序完成到开始输出有一个延迟, 原因是这段时间内机器忙着排序中间数据. b 列下图显示最终完成时间要显著多余 a 列, 原因是最后 5 个 reduce tasks 严重拖后腿了, 整体耗时增长 44%. 这可以看出后备任务的对性能的显著提升. c 列上图显示显示手动干掉 200 个进程(机器还正常运行)后速率变成负的了, 原因是部分 map tasks 丢了, 需要重跑. 集群调度器快速在这些机器上重新运行相关任务, 最后 c 列下图显示仅仅比正常情况多了 5% 耗时. –End– ","date":"2021-03-03","objectID":"/mapreduce-design-and-implementation/:5:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"MapReduce: 分布式计算系统设计与实现","uri":"/mapreduce-design-and-implementation/#5-性能"},{"categories":null,"content":"迭代器的设计和实现是 leveldb 的精华之一. 前几篇文章都多少提到了迭代器的使用, 本篇让我们深入一下迭代器的设计实现, 也为接下来的几篇剖析打下基础. ","date":"2021-02-05","objectID":"/leveldb-annotations-4-iterator/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之四: 迭代器设计与实现","uri":"/leveldb-annotations-4-iterator/#"},{"categories":null,"content":" 1 迭代器接口设计迭代器接口类为 leveldb::Iterator, 位于 include/leveldb/iterator.h 和 table/iterator.cc. (实现位于 table 目录, 是因为接下来要介绍的 sstable 是迭代器重度用户.) 迭代器接口定义比较简洁, 主要方法为指向合法性判断, 前后移动, 定位(开头/末尾/任意), 提取数据项 key/value 等等. 唯一的数据成员为清理函数列表头节点. 具体如下: class LEVELDB_EXPORT Iterator { public: Iterator(); // 禁用复制构造 Iterator(const Iterator\u0026) = delete; // 禁用赋值构造 Iterator\u0026 operator=(const Iterator\u0026) = delete; virtual ~Iterator(); // 一个迭代器要么指向 key/value 对, 要么指向非法位置. // 当且仅当第一种情况才为 valid. virtual bool Valid() const = 0; // 将迭代器移动到数据源的第一个 key/value 对. // 当前仅当数据源不空时, 调用完该方法再调用 Valid() 为 true. virtual void SeekToFirst() = 0; // 将迭代器移动到数据源的最后一个 key/value 对. // 当前仅当数据源不空时, 调用完该方法再调用 Valid() 为 true. virtual void SeekToLast() = 0; // 将迭代器指向移动到数据源 target 位置或之后的第一个 key. // 当且仅当移动后的位置存在数据项时, 调用 Valid() 才为 true. virtual void Seek(const Slice\u0026 target) = 0; // 将迭代器移动到数据源下一个数据项. // 当且仅当迭代器未指向数据源最后一个数据项时, 调用完该方法后调用 Valid() 结果为 true. // 注意: 调用该方法前提是迭代器当前指向必须 valid. virtual void Next() = 0; // 将迭代器移动到数据源前一个数据项. // 当且仅当迭代器未指向数据源第一个数据项时, 调用完该方法后调用 Valid() 结果为 true. // 注意: 调用该方法前提是迭代器当前指向必须 valid. virtual void Prev() = 0; // 返回当前迭代器指向的数据项的 key, Slice 类型, 如果使用迭代器进行修改则会反映到 // 已返回的 key 上面. // 注意: 调用该方法前提是迭代器当前执行必须 valid. virtual Slice key() const = 0; // 返回当前迭代器指向的数据项的 value, Slice 类型, 如果使用迭代器进行修改则会反映到 // 已返回的 value 上面. // 注意: 调用该方法前提是迭代器当前执行必须 valid. virtual Slice value() const = 0; // 发生错误返回之; 否则返回 ok. virtual Status status() const = 0; // 我们允许调用方注册一个带两个参数的回调函数, 当迭代器析构时该函数会被自动调用. using CleanupFunction = void (*)(void* arg1, void* arg2); // 我们允许客户端注册 CleanupFunction 类型的回调函数, 在迭代器被销毁的时候会调用它们(可以注册多个). // 注意, 跟前面的方法不同, RegisterCleanup 不是抽象的, 客户端不应该覆写他们. void RegisterCleanup(CleanupFunction function, void* arg1, void* arg2); private: // 清理函数被维护在一个单向链表上, 其中头节点被 inlined 到迭代器中. // 该类用于保存用户注册的清理函数, 一个清理函数对应一个该类对象, 全部对象被维护在一个单向链表上. struct CleanupNode { // 清理函数及其两个参数 CleanupFunction function; void* arg1; void* arg2; // 下个清理函数 CleanupNode* next; // 判断清理函数是否为空指针. bool IsEmpty() const { return function == nullptr; } // 运行调用方通过 Iterator::RegisterCleanup 注册的清理函数 void Run() { assert(function != nullptr); (*function)(arg1, arg2); } }; // 清理函数列表的头节点 CleanupNode cleanup_head_; } Iterator 本身实现了三个方法, 分别是构造方法, 析构方法, 以及清理函数注册方法. 下面是非抽象方法的实现: // 构造方法, 初始化唯一数据成员 Iterator::Iterator() { cleanup_head_.function = nullptr; cleanup_head_.next = nullptr; } Iterator::~Iterator() { // 析构时调用已注册的清理函数 if (!cleanup_head_.IsEmpty()) { // 线性的, 如果在该迭代器上注册的清理函数太多了应该会影响性能, 但总要做释放操作, 时间总归省不了. cleanup_head_.Run(); for (CleanupNode* node = cleanup_head_.next; node != nullptr; ) { node-\u003eRun(); CleanupNode* next_node = node-\u003enext; delete node; node = next_node; } } } // 将用户定制的清理函数挂到单向链表上, 待迭代器销毁时挨个调用(见 ~Iterator()). void Iterator::RegisterCleanup(CleanupFunction func, void* arg1, void* arg2) { assert(func != nullptr); CleanupNode* node; if (cleanup_head_.IsEmpty()) { node = \u0026cleanup_head_; } else { node = new CleanupNode(); // 新节点插到 head 后面 node-\u003enext = cleanup_head_.next; cleanup_head_.next = node; } node-\u003efunction = func; node-\u003earg1 = arg1; node-\u003earg2 = arg2; } ","date":"2021-02-05","objectID":"/leveldb-annotations-4-iterator/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之四: 迭代器设计与实现","uri":"/leveldb-annotations-4-iterator/#1-迭代器接口设计"},{"categories":null,"content":" 1.1 迭代器实现一例下面以 sstable 的 block 为例示意一下迭代器的实现. 关键部分就是将 block 作为迭代器数据源, 基于 block 构造和查询原理实现迭代器的前后移动, 定位等操作. 里面涉及了一些成员看不懂也不用管, 在介绍 sstable 时会解释. 具体类为 leveldb::Block::Iter, 代码位于 table/block.cc 文件: class Block::Iter : public Iterator { private: // 迭代时使用的比较器 const Comparator* const comparator_; // 指向 block 的指针 const char* const data_; // block 的 restart 数组 // (每个元素 32 位固定长度, 保存着每个 restart 在 block 里的偏移量) // 在 block 里的起始偏移量 uint32_t const restarts_; // restart 数组元素个数(每个元素都是 uint32_t 类型) uint32_t const num_restarts_; // current_ 表示当前数据项在 data_ 里的偏移量, // 如果迭代器无效则该值大于等于 restarts_ // 即 restart array 在 block 的起始偏移量 // (restart array 位于 block 后部, 数据项在 block 前半部分) uint32_t current_; // restart block 的索引值, current_ 指向的数据项落在该 block uint32_t restart_index_; // current_ 所指数据项的 key std::string key_; // current_ 所指数据项的 value Slice value_; // 当前迭代器对应的状态 Status status_; inline int Compare(const Slice\u0026 a, const Slice\u0026 b) const { return comparator_-\u003eCompare(a, b); } // 返回 current_ 所指数据项的下一个数据项的偏移量. // 根据 Block 布局我们可以知道, value 位于每个数据项最后, // 所以 value 之后第一个字节即为下一个数据项起始位置. inline uint32_t NextEntryOffset() const { return (value_.data() + value_.size()) - data_; } // 返回索引值为 index 的 restart 在 block 中的起始偏移量 uint32_t GetRestartPoint(uint32_t index) { assert(index \u003c num_restarts_); return DecodeFixed32(data_ + restarts_ + index * sizeof(uint32_t)); } // 将迭代器移动到索引值为 index 的 restart 对应的偏移量位置. // 注意, 此方法只调整了 current_ 对应的 value_, 此时两者不再保持一致; // current_ 与 key_ 仍然保持一致性. void SeekToRestartPoint(uint32_t index) { key_.clear(); restart_index_ = index; // current_ 和 key_ 指向后续会被 ParseNextKey() 校正. // ParseNextKey() 从 value_ 末尾开始, 所以这里需要设置好, 为何从 value_ // 末尾开始呢? 根据 Block 布局我们可以知道, value 位于每个数据项最后, // 所以 value 之后第一个字节即为下一个数据项起始位置. uint32_t offset = GetRestartPoint(index); // 将 value 数据起始地址设置为 offset 对应的 restart 起始位置, // value_ 这么设置是为了方便 ParseNextKey(). value_ = Slice(data_ + offset, 0); } public: Iter(const Comparator* comparator, const char* data, uint32_t restarts, uint32_t num_restarts) : comparator_(comparator), data_(data), restarts_(restarts), num_restarts_(num_restarts), current_(restarts_), restart_index_(num_restarts_) { assert(num_restarts_ \u003e 0); } virtual bool Valid() const { return current_ \u003c restarts_; } virtual Status status() const { return status_; } virtual Slice key() const { assert(Valid()); return key_; } virtual Slice value() const { assert(Valid()); return value_; } virtual void Next() { // 向后移动前提是当前指向合法 assert(Valid()); ParseNextKey(); } // 将 current 指向当前数据项前一个数据项. // 如果 current 指向的已经是 block 第 0 个数据项, 则无须移动了; virtual void Prev() { // 向前移动前提是当前指向合法 assert(Valid()); // 倒着扫描, 直到 current_ 之前的一个 restart point. // current_ 大于等于所处 restart 段起始地址, 下面要做的 // 是寻找 current_ 之前的一个 restart point. // 把 current_ 当前取值作为原点. const uint32_t original = current_; // 下面循环干一件事, 定位 current_ 前一个数据项, 具体分两种情况： // - 如果 current_ 大于所处 restart 段起始地址, 不进行循环, // 到下面去直接定位 current_ 前一个数据项即可. // - 如果 current_ 等于所处 restart 段起始地址, // - 如果当前 restart 不是 block 的首个 restart, // 则 current_ 前一个数据项肯定位于前一个 restart 最后一个位置 // - 如果当前 restart 是 block 的首个 restart, // 则 current_ 就是 block 首个数据项, 所以没有所谓前一个数据项了 // - 没有其它情况. // 循环能够执行的唯一条件就是相等 while (GetRestartPoint(restart_index_) \u003e= original) { // 倒到开头的 restart point 了, 没法再向前倒了, 也就是没有 pre 了. if (restart_index_ == 0) { // current_ 置为同 restarts_, // 即使得它位于 block 首个 restart 的首个数据项处. current_ = restarts_; // 将 restart_index_ 置为 restart point 个数, // 这个索引是越界的. restart_index_ = num_restarts_; return; } // 倒车, 请注意. restart_index_--; } // 粗粒度移动, 即先将 current_ 移动到指定 restart 分段 SeekToRestartPoint(restart_index_); do { // 细粒度移动, 将 current_ 移动到 original (current_ 移动之前的值)的前一个数据项 } while (ParseNextKey() \u0026\u0026 NextEntryOffset() \u003c original); } // 寻找 block 中第一个 key 大于等于 target 的数据项. // 先通过二分法在 restart 段级定位查找目标段, 存在 // key \u003c target 且是最后一个 restart 段; // 然后在目标段进行线性查找找到第一个 key 大约等于 target 的数据项. // 如果存在则 current_ 指向该目标数据项; 否则 current_ 指向 // 一个非法数据项. // 调用者需要检查返回结果以确认是否找到了. virtual void Seek(const Slice\u0026 target) { // 在 restart array 中进行二分查找, 找到最后一个 // 存在 key 小于 target 的 restart, 注意是小于","date":"2021-02-05","objectID":"/leveldb-annotations-4-iterator/:1:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之四: 迭代器设计与实现","uri":"/leveldb-annotations-4-iterator/#11-迭代器实现一例"},{"categories":null,"content":" 2 双层迭代器设计双层迭代器, 对应的类为 class leveldb::\u003cunnamed\u003e::TwoLevelIterator, 位于 table/two_level_iterator.cc 文件. 它的父类为 leveldb::Iterator, 所以表现出来的性质是一样的. 该类设计比较巧妙, 这主要是由 sstable 文件结构决定的. 具体地, 要想在 sstable 文件中找到某个 key/value 对, 肯定先要找到它所属的 data Block, 而要找到 data Block 就要先在 index block 找到其对应的 BlockHandle. 双层迭代器就是这个寻找过程的实现. 该类包含两个迭代器封装： 一个是 index_iter_, 它指向 index block 数据项. 针对每个 data block 都有一个对应的 entry 包含在 index block 中, entry 包含一个 key/value 对, 其中： key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 的第一个 key 的(比较拗口)字符串; value 是指向一个对应 data block 的 BlockHandle. 另一个是 data_iter_, 它指向 data block 包含的数据项. 至于这个 data block 是否与 index_iter_ 所指数据项对应 data block 一致, 那要看实际情况, 不过即使不一致也无碍. 示意图如下: 这两个迭代器, 可以把 index_iter 看作钟表的时针, 指向具体小时, 可以把 data_iter_ 看作更精细的分针, 指向当前小时的具体分钟. 两个指针一起配合精确定位到我们要查询的数据. 这么说其实就能大体上猜出来, 迭代器前后移动, 定位等等这些方法是如何实现的了, 简单说就是先移动 index_iter_ 再移动 data_iter_. 以 Seek() 方法举例来说: // 根据 target 将 index_iter 和 data_iter 移动到对应位置 void TwoLevelIterator::Seek(const Slice\u0026 target) { // 因为 index block 每个数据项的 key 是对应 data block 中最大的那个 key, // 所以 index block 数据项也是有序的, 不过比较\"宏观\" . // 先找到目标 data block index_iter_.Seek(target); // 根据 index_iter_ 设置 data_iter_ InitDataBlock(); // 然后在目标 data block 找到目标数据项 if (data_iter_.iter() != nullptr) data_iter_.Seek(target); // data_iter_.iter() 为空则直接向前移动找到第一个不为空的 // data block 的第一个数据项. SkipEmptyDataBlocksForward(); } 可以看出, 双层迭代器设计具有分形的思想, 迭代器是由迭代器构成的. 其它方法实现原理类似, 不再赘述. –End– ","date":"2021-02-05","objectID":"/leveldb-annotations-4-iterator/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之四: 迭代器设计与实现","uri":"/leveldb-annotations-4-iterator/#2-双层迭代器设计"},{"categories":null,"content":"本文基于内部分享 \u003c“抄\"能力养成系列 – GFS 设计\u003e 整理. 2003 年开始 Google 陆续放出三套系统的设计(GFS/MapReduce/Bigtable), 在互联网届掀起云计算狂潮一直影响至今. 该论文一出, 便催生了 Hadoop 中的 HDFS 的诞生. GFS 作为发轫, 目前许多业界知名的分布式系统设计仍然有着它的影子. 下面就让我们一起看看 GFS 的设计, 希望为各位后续系统研发提供灵感。(Salute to Jeff). ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:0:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#"},{"categories":null,"content":" 1 GFS 诞生背景 1, 组件失效是常态而非例外 2, 按传统标准, 现实世界的文件太大了, 一般都几个 GB, 个数多了单机存不下. 3, 大多数文件通过 append 而非覆盖进行修改, 所以追加操作是性能优化和需要原子保证的焦点. 4, 将上层应用和文件系统 API 联合设计可增加灵活性, 让整个系统都受益. 比如通过放松 GFS 的一致性要求以大规模简化文件系统还不给应用增加负担. 同时也引入了一个原子化的 append 操作以让多个客户端针对同一个文件并发执行 append 操作而无需额外的同步措施. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:1:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#1-gfs-诞生背景"},{"categories":null,"content":" 2 GFS 设计概览","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#2-gfs-设计概览"},{"categories":null,"content":" 2.1 几点假设 硬件便宜爱故障, 监控完善恢复快. 文件个数适中, 以大文件为主, 支持但不优化小文件存储(tradeoff). 负载主要是大数据量的流式读取和小数据量的随机读取, 后者可以合并重排减少随机. 另一个重要负载是大数据量顺序 append 数据到文件, 也支持随机写但没做优化, 文件一旦写完几乎不再修改. 设计良好且高效实现的单文件并发 append 操作. GFS 文件经常用于生产者消费者队列或多路合并. 持续稳定的高带宽比时延更加重要. 大多数应用更看重高吞吐而非某个读写操作的响应时间. (long-fat network) ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#21-几点假设"},{"categories":null,"content":" 2.2 接口 GFS 支持 create/delete/open/close/read/write 操作但不支持 POSIX 规范(GlusterFS 支持). Snapshot: 就是以一个很低的消耗创建一个文件或者目录树的拷贝. record append 操作: 允许多个客户端并发地向同一个文件追加数据, 每个追加操作都是原子的, 这为实现生产者-消费者队列或者多路合并提供了便利, 因为多个客户端同时写同一个文件无需加锁. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#22-接口"},{"categories":null,"content":" 2.3 架构Figure 1 为 GFS 架构一览. 一个 GFS 集群由 1 个 master 和多个 chunkserver 构成, 集群可同时被多个 client 访问. Client 代表应用和 master 还有 chunkserver 通信. 其中文件相关的有: 文件被切成固定大小的 chunks(每个 chunk 还会被切成 blocks, 后述). 每个chunk 都有一个全局不可变的 64 bit 的 chunk handle, 这个 handle 由 master 在 chunk 创建时分配. 读写 chunk 时需要指定 handle 和字节范围. 为了高可用, 每个 chunk 会被在多个 chunkserver 上保存各保存一个副本, 默认三副本. master 相关的有: master 保存整个文件系统的元信息, 包括命名空间/访问控制信息/从 files 到 chunks 的映射/chunks 当前位置. master 也控制系统层面的活动如 chunk 租约管理/孤儿 chunks 的垃圾回收/chunkservers 之间的 chunk 迁移. master 和各个 chunkservers 通信(心跳)以发送指令和收集状态信息. client 和 chunkserver 都不会缓存文件数据: 就 client 而言, 一方面原因是应用程序一般流式拉取巨大文件或者数据太大无法进行缓存, 另一方面因为无需处理缓存一致性问题可以简化 client 和整个系统的复杂度. 针对 chunkserver 而言, 因为数据本来就存在 chunkserver 本地磁盘, 可以直接使用 linux 的 buffer 来实现经常被访问的数据的缓存, 单机的缓存就交给单机操作系统来处理了. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#23-架构"},{"categories":null,"content":" 2.4 单 master 集群各组件交互重点注意, GFS 一个集群只有一个 Master(关于它的高可用后面描述). master 和 chunkserver 之间周期性的心跳交互. client 读取时先问 master 数据存在哪个 chunkserver, 拿到信息后缓存到本地(有超时时间限制), 然后直接和 chunkserver 通信. client 把上层应用指定的文件名和偏移量翻译成 chunk index, 发给 master. master 返回 handle 和 location, client 用文件名和 index 做 key 缓存该信息. 直到元信息缓存失效或者应用重新打开文件, client 都无需和 master 交互, 而是直接和 chunkserver 交互. 这可以大幅降低 master 负载. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:4","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#24-单-master-集群各组件交互重点"},{"categories":null,"content":" 2.5 chunk 大小设计 chunk 大小非常关键, GFS 选的是 64MB. 比传统的文件块大多了. 采用 lazy space 分配策略, 可以避免如此大的 chunk size 导致的空间浪费. 每个 chunk 都作为一个普通的文件存储在 chunkserver. chunk size 选的大有如下好处: 1, 大幅减少了 client 和 master 的交互, 因为典型的应用就是顺序读取大文件, chunk 大则要访问的 chunks 就少, 就不用频繁与 master交互. client 侧可以为数 TB 数据集数据缓存它们对应的 chunk 元信息. 2, 因为 chunk 很大,可以覆盖很多操作, 这样 client 跟其所在 chunkserver 长时间维持一个持久 TCP 连接而无需频繁与多个 chunkserver 新建链接, 这就减少了网络开销. 3, 因为 chunk 很大, 个数就会减少, 则对应的元信息也相应减少, 这样 master 可以在内存缓存全部元信息. 这么大的 chunk size 也有缺点, 就是对小文件不友好: 因为一个小文件可能只对应一个 chunk, 如果针对小文件并发操作很多, 那么它就会成为热点(redis hotkey 与之类似.). 因为实际应用中以大文件读写为主所以问题不严重. 目前解决办法就是针对热点小文件提升其副本因子, 并且把访问该文件的客户端交错启动. 长远的解决办法是允许客户端能从其它客户端而不仅仅是从 chunkserver 读取同样的数据. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:5","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#25-chunk-大小设计"},{"categories":null,"content":" 2.6 元信息master 保存三种类型的元信息: 文件和 chunk 命名空间 file-to-chunk mapping 每个 chunk 副本的位置信息 master 把全部元信息都保存在的内存中: 前两种元信息会被 master 通过本地日志持久化变更同时备份到远程机器确保可用性. chunk 位置信息不会被持久化, 而是在 master 启动以及 chunkserver 加入集群时询问 chunkserver 有关 chunk 的位置信息. chunkserver 保存的元信息有两个: 每个 block 的校验和, 以及 chunk 版本号. 在 master 上, 每个文件对应的元信息大约 100 字节, 这也符合 master 内存不会成为系统容量瓶颈的预期. 而且这 100 字节大部分是文件名(已经过前缀压缩). 其它元信息还有文件归属/权限, 从文件到 chunks 的映射, chunk 副本的位置信息, 每个 chunk 的当前版本. 针对 chunk 还会保存一个引用计数用于实现 COW(copy-on-write). master 和 chunkserver 每个都保存大约 50 到 100 MB 数据, 加载非常快, 但是由于 master 启动时要拉取chunk location 所以启动需要额外 30 到 60 秒. 自从 master 内存结构改成二叉搜索树以后, 命名空间搜索不再是瓶颈. 2.6.1 元信息都保存在内存中master 把元信息存在内存, 操作就会很快. master 可以在后台高效地周期性扫描整个状态空间以实现: chunk GC, 应对 chunkserver 失效重新分配副本, 为实现负载和磁盘空间均衡在 chunkservers 间迁移 chunks. master 把全部信息保存到内存有个不好的地方就是集群数据量受限于 master 内存大小. 不过因为每个 chunk(64MB 大) 对应元信息不超过 64 字节, 所以实践中不是啥问题. 大多数 chunks 都是满的因为大多数文件各自都包含多个 chunks, 可能仅每个文件对应的最后一个 chunk 不满, 这就让元信息性价比非常高. 同样, 针对每个文件, 它的文件命名空间数据也不超过 64 字节, 因为其存储的文件名通过使用前缀压缩非常紧凑. 当然, 想支持更大的数据集合, 给 master 加内存就行了, 便宜又快捷. 2.6.2 chunk 位置信息虽然 master 不持久化 chunk 位置信息, 但是因为它负责 chunks 布局同时通过心跳监控 chunkserver, 所以它能保持这些位置信息时刻保持更新. 其实我们开始也尝试持久化 chunk 位置信息, 后来发现还是 master 启动后周期性拉取更简单. 这消除了因为 chunkserver 加入或离开集群/改名/失效/重启等等而保持 master 和 chunkservers 数据同步的问题. 2.6.3 操作日志master 上的操作日志包含了关键元信息的变更历史. 这对 GFS来说至关重要. 这个日志作为逻辑时间线定义了并发操作的顺序. 文件和 chunks 以及它们的版本号, 可以永久通过它们被创建的逻辑时间被唯一识别. 元信息变更被持久化到本地和远程多台机器后, 相关变更才对客户端可见. master 启动时会读取操作日志并进行重放以恢复系统状态, 所以操作日志不能太大否则启动时间会非常长. 为了避免日志文件过大, master 会在日志文件超过一定大小后 checkpoint 自己当前状态, 这样下次启动时只需从本地磁盘加载和重放最近一次 checkpoint 就可以恢复到最新状态. 因为构造 checkpoint 需要花时间, 为了避免阻塞后续处理, 方法如下: master 切换到新的日志文件并在一个单独的线程中创建 checkpoint. 新的 checkpoint 包含日志切换前的全部变更. 大约一分钟左右就可以为一个拥有几百万文件的集群创建一个 checkpoint. 创建完成后它将被写入本地和远程磁盘. master 恢复过程只需最近的 checkpoint 和创建该 checkpoint 后的日志文件. 老的 checkpoints 和日志文件都会被删除. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:6","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#26-元信息"},{"categories":null,"content":" 2.6 元信息master 保存三种类型的元信息: 文件和 chunk 命名空间 file-to-chunk mapping 每个 chunk 副本的位置信息 master 把全部元信息都保存在的内存中: 前两种元信息会被 master 通过本地日志持久化变更同时备份到远程机器确保可用性. chunk 位置信息不会被持久化, 而是在 master 启动以及 chunkserver 加入集群时询问 chunkserver 有关 chunk 的位置信息. chunkserver 保存的元信息有两个: 每个 block 的校验和, 以及 chunk 版本号. 在 master 上, 每个文件对应的元信息大约 100 字节, 这也符合 master 内存不会成为系统容量瓶颈的预期. 而且这 100 字节大部分是文件名(已经过前缀压缩). 其它元信息还有文件归属/权限, 从文件到 chunks 的映射, chunk 副本的位置信息, 每个 chunk 的当前版本. 针对 chunk 还会保存一个引用计数用于实现 COW(copy-on-write). master 和 chunkserver 每个都保存大约 50 到 100 MB 数据, 加载非常快, 但是由于 master 启动时要拉取chunk location 所以启动需要额外 30 到 60 秒. 自从 master 内存结构改成二叉搜索树以后, 命名空间搜索不再是瓶颈. 2.6.1 元信息都保存在内存中master 把元信息存在内存, 操作就会很快. master 可以在后台高效地周期性扫描整个状态空间以实现: chunk GC, 应对 chunkserver 失效重新分配副本, 为实现负载和磁盘空间均衡在 chunkservers 间迁移 chunks. master 把全部信息保存到内存有个不好的地方就是集群数据量受限于 master 内存大小. 不过因为每个 chunk(64MB 大) 对应元信息不超过 64 字节, 所以实践中不是啥问题. 大多数 chunks 都是满的因为大多数文件各自都包含多个 chunks, 可能仅每个文件对应的最后一个 chunk 不满, 这就让元信息性价比非常高. 同样, 针对每个文件, 它的文件命名空间数据也不超过 64 字节, 因为其存储的文件名通过使用前缀压缩非常紧凑. 当然, 想支持更大的数据集合, 给 master 加内存就行了, 便宜又快捷. 2.6.2 chunk 位置信息虽然 master 不持久化 chunk 位置信息, 但是因为它负责 chunks 布局同时通过心跳监控 chunkserver, 所以它能保持这些位置信息时刻保持更新. 其实我们开始也尝试持久化 chunk 位置信息, 后来发现还是 master 启动后周期性拉取更简单. 这消除了因为 chunkserver 加入或离开集群/改名/失效/重启等等而保持 master 和 chunkservers 数据同步的问题. 2.6.3 操作日志master 上的操作日志包含了关键元信息的变更历史. 这对 GFS来说至关重要. 这个日志作为逻辑时间线定义了并发操作的顺序. 文件和 chunks 以及它们的版本号, 可以永久通过它们被创建的逻辑时间被唯一识别. 元信息变更被持久化到本地和远程多台机器后, 相关变更才对客户端可见. master 启动时会读取操作日志并进行重放以恢复系统状态, 所以操作日志不能太大否则启动时间会非常长. 为了避免日志文件过大, master 会在日志文件超过一定大小后 checkpoint 自己当前状态, 这样下次启动时只需从本地磁盘加载和重放最近一次 checkpoint 就可以恢复到最新状态. 因为构造 checkpoint 需要花时间, 为了避免阻塞后续处理, 方法如下: master 切换到新的日志文件并在一个单独的线程中创建 checkpoint. 新的 checkpoint 包含日志切换前的全部变更. 大约一分钟左右就可以为一个拥有几百万文件的集群创建一个 checkpoint. 创建完成后它将被写入本地和远程磁盘. master 恢复过程只需最近的 checkpoint 和创建该 checkpoint 后的日志文件. 老的 checkpoints 和日志文件都会被删除. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:6","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#261-元信息都保存在内存中"},{"categories":null,"content":" 2.6 元信息master 保存三种类型的元信息: 文件和 chunk 命名空间 file-to-chunk mapping 每个 chunk 副本的位置信息 master 把全部元信息都保存在的内存中: 前两种元信息会被 master 通过本地日志持久化变更同时备份到远程机器确保可用性. chunk 位置信息不会被持久化, 而是在 master 启动以及 chunkserver 加入集群时询问 chunkserver 有关 chunk 的位置信息. chunkserver 保存的元信息有两个: 每个 block 的校验和, 以及 chunk 版本号. 在 master 上, 每个文件对应的元信息大约 100 字节, 这也符合 master 内存不会成为系统容量瓶颈的预期. 而且这 100 字节大部分是文件名(已经过前缀压缩). 其它元信息还有文件归属/权限, 从文件到 chunks 的映射, chunk 副本的位置信息, 每个 chunk 的当前版本. 针对 chunk 还会保存一个引用计数用于实现 COW(copy-on-write). master 和 chunkserver 每个都保存大约 50 到 100 MB 数据, 加载非常快, 但是由于 master 启动时要拉取chunk location 所以启动需要额外 30 到 60 秒. 自从 master 内存结构改成二叉搜索树以后, 命名空间搜索不再是瓶颈. 2.6.1 元信息都保存在内存中master 把元信息存在内存, 操作就会很快. master 可以在后台高效地周期性扫描整个状态空间以实现: chunk GC, 应对 chunkserver 失效重新分配副本, 为实现负载和磁盘空间均衡在 chunkservers 间迁移 chunks. master 把全部信息保存到内存有个不好的地方就是集群数据量受限于 master 内存大小. 不过因为每个 chunk(64MB 大) 对应元信息不超过 64 字节, 所以实践中不是啥问题. 大多数 chunks 都是满的因为大多数文件各自都包含多个 chunks, 可能仅每个文件对应的最后一个 chunk 不满, 这就让元信息性价比非常高. 同样, 针对每个文件, 它的文件命名空间数据也不超过 64 字节, 因为其存储的文件名通过使用前缀压缩非常紧凑. 当然, 想支持更大的数据集合, 给 master 加内存就行了, 便宜又快捷. 2.6.2 chunk 位置信息虽然 master 不持久化 chunk 位置信息, 但是因为它负责 chunks 布局同时通过心跳监控 chunkserver, 所以它能保持这些位置信息时刻保持更新. 其实我们开始也尝试持久化 chunk 位置信息, 后来发现还是 master 启动后周期性拉取更简单. 这消除了因为 chunkserver 加入或离开集群/改名/失效/重启等等而保持 master 和 chunkservers 数据同步的问题. 2.6.3 操作日志master 上的操作日志包含了关键元信息的变更历史. 这对 GFS来说至关重要. 这个日志作为逻辑时间线定义了并发操作的顺序. 文件和 chunks 以及它们的版本号, 可以永久通过它们被创建的逻辑时间被唯一识别. 元信息变更被持久化到本地和远程多台机器后, 相关变更才对客户端可见. master 启动时会读取操作日志并进行重放以恢复系统状态, 所以操作日志不能太大否则启动时间会非常长. 为了避免日志文件过大, master 会在日志文件超过一定大小后 checkpoint 自己当前状态, 这样下次启动时只需从本地磁盘加载和重放最近一次 checkpoint 就可以恢复到最新状态. 因为构造 checkpoint 需要花时间, 为了避免阻塞后续处理, 方法如下: master 切换到新的日志文件并在一个单独的线程中创建 checkpoint. 新的 checkpoint 包含日志切换前的全部变更. 大约一分钟左右就可以为一个拥有几百万文件的集群创建一个 checkpoint. 创建完成后它将被写入本地和远程磁盘. master 恢复过程只需最近的 checkpoint 和创建该 checkpoint 后的日志文件. 老的 checkpoints 和日志文件都会被删除. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:6","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#262-chunk-位置信息"},{"categories":null,"content":" 2.6 元信息master 保存三种类型的元信息: 文件和 chunk 命名空间 file-to-chunk mapping 每个 chunk 副本的位置信息 master 把全部元信息都保存在的内存中: 前两种元信息会被 master 通过本地日志持久化变更同时备份到远程机器确保可用性. chunk 位置信息不会被持久化, 而是在 master 启动以及 chunkserver 加入集群时询问 chunkserver 有关 chunk 的位置信息. chunkserver 保存的元信息有两个: 每个 block 的校验和, 以及 chunk 版本号. 在 master 上, 每个文件对应的元信息大约 100 字节, 这也符合 master 内存不会成为系统容量瓶颈的预期. 而且这 100 字节大部分是文件名(已经过前缀压缩). 其它元信息还有文件归属/权限, 从文件到 chunks 的映射, chunk 副本的位置信息, 每个 chunk 的当前版本. 针对 chunk 还会保存一个引用计数用于实现 COW(copy-on-write). master 和 chunkserver 每个都保存大约 50 到 100 MB 数据, 加载非常快, 但是由于 master 启动时要拉取chunk location 所以启动需要额外 30 到 60 秒. 自从 master 内存结构改成二叉搜索树以后, 命名空间搜索不再是瓶颈. 2.6.1 元信息都保存在内存中master 把元信息存在内存, 操作就会很快. master 可以在后台高效地周期性扫描整个状态空间以实现: chunk GC, 应对 chunkserver 失效重新分配副本, 为实现负载和磁盘空间均衡在 chunkservers 间迁移 chunks. master 把全部信息保存到内存有个不好的地方就是集群数据量受限于 master 内存大小. 不过因为每个 chunk(64MB 大) 对应元信息不超过 64 字节, 所以实践中不是啥问题. 大多数 chunks 都是满的因为大多数文件各自都包含多个 chunks, 可能仅每个文件对应的最后一个 chunk 不满, 这就让元信息性价比非常高. 同样, 针对每个文件, 它的文件命名空间数据也不超过 64 字节, 因为其存储的文件名通过使用前缀压缩非常紧凑. 当然, 想支持更大的数据集合, 给 master 加内存就行了, 便宜又快捷. 2.6.2 chunk 位置信息虽然 master 不持久化 chunk 位置信息, 但是因为它负责 chunks 布局同时通过心跳监控 chunkserver, 所以它能保持这些位置信息时刻保持更新. 其实我们开始也尝试持久化 chunk 位置信息, 后来发现还是 master 启动后周期性拉取更简单. 这消除了因为 chunkserver 加入或离开集群/改名/失效/重启等等而保持 master 和 chunkservers 数据同步的问题. 2.6.3 操作日志master 上的操作日志包含了关键元信息的变更历史. 这对 GFS来说至关重要. 这个日志作为逻辑时间线定义了并发操作的顺序. 文件和 chunks 以及它们的版本号, 可以永久通过它们被创建的逻辑时间被唯一识别. 元信息变更被持久化到本地和远程多台机器后, 相关变更才对客户端可见. master 启动时会读取操作日志并进行重放以恢复系统状态, 所以操作日志不能太大否则启动时间会非常长. 为了避免日志文件过大, master 会在日志文件超过一定大小后 checkpoint 自己当前状态, 这样下次启动时只需从本地磁盘加载和重放最近一次 checkpoint 就可以恢复到最新状态. 因为构造 checkpoint 需要花时间, 为了避免阻塞后续处理, 方法如下: master 切换到新的日志文件并在一个单独的线程中创建 checkpoint. 新的 checkpoint 包含日志切换前的全部变更. 大约一分钟左右就可以为一个拥有几百万文件的集群创建一个 checkpoint. 创建完成后它将被写入本地和远程磁盘. master 恢复过程只需最近的 checkpoint 和创建该 checkpoint 后的日志文件. 老的 checkpoints 和日志文件都会被删除. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:6","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#263-操作日志"},{"categories":null,"content":" 2.7 GFS 的一致性模型GFS 提供了一个弱一致性模型, 相对简单且容易高效实现. 2.7.1 GFS 提供的保证文件命名空间变更, 如创建文件, 是原子的. 它们均仅由 master 处理. master 的操作日志为这些操作定义了全局顺序. consistent: 针对某个文件区域, 如果全部客户端看到的数据是一致的, 不管它们是从哪个副本读取的数据, 我们就说这个文件区域数据是一致的. defined: 针对经历过数据变更的某个文件区域, 如果它是 consistent 的, 并且客户端能看到前述变更的完整内容, 即可预测, 不会因并发而随机那么我们就说这个文件区域是确定的. 注意, defined 和 consistent 是两个层面的东西, 只要写成功, 那么 GFS 的 2PC 能保证 consistent, 也就是 defined 包含了 consistent. 但如果发生了并发变更, 比如多个客户端针对某个文件区域同一个偏移并发写, 此时不同于追加, 这种写操作是互相覆盖的, 最终结果是 undefined 的, 即我们无法预先知晓该偏移处结果是什么, 但 GFS 可以保证各个副本都是同样的 undefined 状态. 上面 Table 1 展示了 GFS 两种典型的变更操作 write(覆盖写) 和 record append(记录追加)在串行和并发情况下完成后对应的文件区域的状态. 数据变更包括 write(在指定偏移处写入, 如覆盖写)和 record append(即在文件尾部原子地追加记录). GFS 会保证一系列成功变更后的文件是 defined 的, 措施如下: 1, 以同样的顺序将变更应用到 chunk 全部副本. 2, 使用 chunk 版本号来检测过期的副本, 相关 chunkserver可能因为下过线错过某些变更. 过期副本不会对外服务而是会被尽快 GC 掉. master 借助心跳计算每个chunkserver 上数据的校验和以检测数据是否损坏.发现损坏后会尽快从好的副本恢复数据. 如果全部副本丢失, 那么chunk 就不可恢复了. 这种情况下应用得到的响应是数据丢失而非损坏. 2.7.2 一致性模型在应用端实现写 依赖于追加而非覆写(相比于覆写, 追加更加高效同时对应用错误更富有弹性), 应用程序一般就是一个文件从头 append 到尾; 要么等数据写完后原子地将文件重命名为一个永久性的名字, 要么周期性地 checkpoint 写了多少字节了同时还可以附加一个应用层校验和. checkpoint 让 writers 可以在重启后增量写入而不用重写全部数据, 同时避免 readers 处理已成功写入但不完整的内容(站在应用层角度). 读 只验证和处理最后一个 checkpoint 之前的文件区域, 这些区域处于 defined 状态. record-append 操作的 append-at-least-once 语义确保了不丢数据, 然后由 readers 负责处理重复数据. 由于 record 包含了一个附加的校验和, 所以 readers 可以此校验 records. readers 还可以通过每个记录的唯一 ID 过滤掉重复的 records. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:7","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#27-gfs-的一致性模型"},{"categories":null,"content":" 2.7 GFS 的一致性模型GFS 提供了一个弱一致性模型, 相对简单且容易高效实现. 2.7.1 GFS 提供的保证文件命名空间变更, 如创建文件, 是原子的. 它们均仅由 master 处理. master 的操作日志为这些操作定义了全局顺序. consistent: 针对某个文件区域, 如果全部客户端看到的数据是一致的, 不管它们是从哪个副本读取的数据, 我们就说这个文件区域数据是一致的. defined: 针对经历过数据变更的某个文件区域, 如果它是 consistent 的, 并且客户端能看到前述变更的完整内容, 即可预测, 不会因并发而随机那么我们就说这个文件区域是确定的. 注意, defined 和 consistent 是两个层面的东西, 只要写成功, 那么 GFS 的 2PC 能保证 consistent, 也就是 defined 包含了 consistent. 但如果发生了并发变更, 比如多个客户端针对某个文件区域同一个偏移并发写, 此时不同于追加, 这种写操作是互相覆盖的, 最终结果是 undefined 的, 即我们无法预先知晓该偏移处结果是什么, 但 GFS 可以保证各个副本都是同样的 undefined 状态. 上面 Table 1 展示了 GFS 两种典型的变更操作 write(覆盖写) 和 record append(记录追加)在串行和并发情况下完成后对应的文件区域的状态. 数据变更包括 write(在指定偏移处写入, 如覆盖写)和 record append(即在文件尾部原子地追加记录). GFS 会保证一系列成功变更后的文件是 defined 的, 措施如下: 1, 以同样的顺序将变更应用到 chunk 全部副本. 2, 使用 chunk 版本号来检测过期的副本, 相关 chunkserver可能因为下过线错过某些变更. 过期副本不会对外服务而是会被尽快 GC 掉. master 借助心跳计算每个chunkserver 上数据的校验和以检测数据是否损坏.发现损坏后会尽快从好的副本恢复数据. 如果全部副本丢失, 那么chunk 就不可恢复了. 这种情况下应用得到的响应是数据丢失而非损坏. 2.7.2 一致性模型在应用端实现写 依赖于追加而非覆写(相比于覆写, 追加更加高效同时对应用错误更富有弹性), 应用程序一般就是一个文件从头 append 到尾; 要么等数据写完后原子地将文件重命名为一个永久性的名字, 要么周期性地 checkpoint 写了多少字节了同时还可以附加一个应用层校验和. checkpoint 让 writers 可以在重启后增量写入而不用重写全部数据, 同时避免 readers 处理已成功写入但不完整的内容(站在应用层角度). 读 只验证和处理最后一个 checkpoint 之前的文件区域, 这些区域处于 defined 状态. record-append 操作的 append-at-least-once 语义确保了不丢数据, 然后由 readers 负责处理重复数据. 由于 record 包含了一个附加的校验和, 所以 readers 可以此校验 records. readers 还可以通过每个记录的唯一 ID 过滤掉重复的 records. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:7","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#271-gfs-提供的保证"},{"categories":null,"content":" 2.7 GFS 的一致性模型GFS 提供了一个弱一致性模型, 相对简单且容易高效实现. 2.7.1 GFS 提供的保证文件命名空间变更, 如创建文件, 是原子的. 它们均仅由 master 处理. master 的操作日志为这些操作定义了全局顺序. consistent: 针对某个文件区域, 如果全部客户端看到的数据是一致的, 不管它们是从哪个副本读取的数据, 我们就说这个文件区域数据是一致的. defined: 针对经历过数据变更的某个文件区域, 如果它是 consistent 的, 并且客户端能看到前述变更的完整内容, 即可预测, 不会因并发而随机那么我们就说这个文件区域是确定的. 注意, defined 和 consistent 是两个层面的东西, 只要写成功, 那么 GFS 的 2PC 能保证 consistent, 也就是 defined 包含了 consistent. 但如果发生了并发变更, 比如多个客户端针对某个文件区域同一个偏移并发写, 此时不同于追加, 这种写操作是互相覆盖的, 最终结果是 undefined 的, 即我们无法预先知晓该偏移处结果是什么, 但 GFS 可以保证各个副本都是同样的 undefined 状态. 上面 Table 1 展示了 GFS 两种典型的变更操作 write(覆盖写) 和 record append(记录追加)在串行和并发情况下完成后对应的文件区域的状态. 数据变更包括 write(在指定偏移处写入, 如覆盖写)和 record append(即在文件尾部原子地追加记录). GFS 会保证一系列成功变更后的文件是 defined 的, 措施如下: 1, 以同样的顺序将变更应用到 chunk 全部副本. 2, 使用 chunk 版本号来检测过期的副本, 相关 chunkserver可能因为下过线错过某些变更. 过期副本不会对外服务而是会被尽快 GC 掉. master 借助心跳计算每个chunkserver 上数据的校验和以检测数据是否损坏.发现损坏后会尽快从好的副本恢复数据. 如果全部副本丢失, 那么chunk 就不可恢复了. 这种情况下应用得到的响应是数据丢失而非损坏. 2.7.2 一致性模型在应用端实现写 依赖于追加而非覆写(相比于覆写, 追加更加高效同时对应用错误更富有弹性), 应用程序一般就是一个文件从头 append 到尾; 要么等数据写完后原子地将文件重命名为一个永久性的名字, 要么周期性地 checkpoint 写了多少字节了同时还可以附加一个应用层校验和. checkpoint 让 writers 可以在重启后增量写入而不用重写全部数据, 同时避免 readers 处理已成功写入但不完整的内容(站在应用层角度). 读 只验证和处理最后一个 checkpoint 之前的文件区域, 这些区域处于 defined 状态. record-append 操作的 append-at-least-once 语义确保了不丢数据, 然后由 readers 负责处理重复数据. 由于 record 包含了一个附加的校验和, 所以 readers 可以此校验 records. readers 还可以通过每个记录的唯一 ID 过滤掉重复的 records. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:7","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#272-一致性模型在应用端实现"},{"categories":null,"content":" 2.8 系统交互GFS 在设计的时候就在尽量做到最小化 master 参与各种操作, 给 master 减负. 下面描述 client/master/chunkserver 三者之间如何交互以实现数据变更/原子化记录追加/快照. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:8","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#28-系统交互"},{"categories":null,"content":" 2.8.1 lease 和 mutation 顺序chunk 的每个变更都会反映到每个副本上, 具体如下: master 通过选择一个副本颁发一个 lease 将其指定为 primary. 然后 primary 为 chunk 的并发变更选择一个串行的顺序, 全部副本都遵循该顺序应用变更. 因此全局变更顺序首先由 master 授权 lease 的顺序以及每个 lease 生效期 primary 指定序列号时确定. lease 机制可以最小化 master 的管理开销, 因为针对 chunk 的一部分管理工作让 primary 承担了. 每个 lease 都有一个初始的 60 秒超时, 但是只要 chunk 仍在变更, primary 可以发送请求给 master 要求延长 lease 有效期, 这个过程是通过 heartbeat 实现的. 当然 master 可以在 lease 过期之前吊销它, 比如当 mast 想禁止对正在改名的文件进行变更时. 当 master 失去与primary 通信时, master 可以授权新的 lease 给另一个副本. Figure 2 为执行写操作时的控制流, 具体如下: 1, client 询问 master 哪个 chunkserver 持有要访问的 chunk 的 lease 以及其它副本的位置. 如果没人持有 lease, master 会选一个副本授权之. 2, master 返回 primary 的 id 以及和其它副本的位置, client 会缓存这些信息, 仅当 primary 不可用或者 primary 明确告知不再持有 lease 时再和 master 通信. 3, client 以任意顺序将数据推给全部副本. 每个 chunkserver 将数据首先保存到内部的 LRU 缓存. 图中将控制流和数据流解耦, 基于网络拓扑(该拓扑不关心谁是 primary)调度数据流可以改善性能, 具体后述. 4, 当全部副本确认收到数据后, client 再发送一个写请求给 primary,该请求标记了上一步推送给全部副本的数据, primary 会为(可能来自多个 clients 的)全部变更分配连续的序列号,然后 primary 按照此顺序应用这些变更. 5, 应用本次变更后, primary 将该写请求转发给全部 secondary 副本,这些副本以同样的顺序应用变更. 6, secondary 副本回复 primary 自己已完成操作. 7, primary 回复 client 本次写操作成功 or 失败. 如果只有 primary 和部分 secondary 成功, 则再返回第一步重试之前会尝试重复 3-7. 从 3 到 7 其实就是 2PC (two-phase commit). 如果一个写操作跨多个 chunk, 那么客户端就会将这个写操作分裂成多个写操作. 它们都按照前面描述的步骤执行, 但是可能和其它客户端的写操作交织在一起并发执行. 因此同一个文件区域可能被多个客户端相互覆盖写入. 尽管这个文件区域的全部副本最终是 consistent, 但是最终结果我们无法预知是什么, 所以最终是 consistent 但 undefined. 2.8.2 数据流将数据流和控制流解耦, 让我们可以更高效地利用网络. 下面看看 GFS 是怎么做的. 就像控制流是从 client-\u003eprimary-\u003esecondaries 管道化传输, 数据流从 client 到各个 replicas 也是管道化传输. 具体地, client 挑选离自己最近的 chunkserver (注意这里根本不管谁是 primary, 只关注网络拓扑), 将数据发给它, 然后 这个 chunk server 一边接收 client 的数据一边转发给离自己最近的 chunkserver, 依此类推 … 从而链式完成数据从 client 到每个副本所在 chunkserver 的发送. 最近距离计算: 这个管道化传输过程中, 各个节点通过 IP 地址计算前面提到的 “最近”距离的计算, 比如同一个局域网跟定比跨网段的要更近一些. 管道化传输好处: 这种管道化传输尽可能地利用了每个 chunkserver 的出站带宽, 也最小化了 TCP 连接的时延. 2.8.3 原子化的记录追加操作传统的写操作, 需要指定要写入到的文件偏移量. 并发执行该类操作写同样的区域并不是串行化的, 被写入区域最后状态包含多个客户端的数据片段. GFS 提供了原子化追加操作, 叫 record append, GFS 会确保至少一次写的语义, 将数据原子化地追加到文件末尾. 这类似于在 unix 编程中, 采用 O_APPEND 模式打开文件而且没有竟态条件. record append 作为一种 mutation, 执行流程同 Figure 2, 但在 primary 那有些许不同: 1, 首先 client 肯定是将数据 push 给文件的最后一个 chunk(因为是 appende 嘛). 2, primary 检查追加这个记录后 chunk 是否超过 64MB, 2.1, 如果超过, 则将这个chunk 填满无效数据, 同时告诉 secondaries 也这么干, 然后告诉客户端说满了, 请将数据写入下个chunk. 为了避免产生过多这类因填充导致的空间碎片, 我们要求 record 大小最多为 chunk 的四分之一. 2.2, 如果不超过, primary 将数据写入本地副本,然后告知secondaries 将数据写入到同样的偏移处, 然后响应客户端. 上述过程, 在任何副本写失败, 客户端就要重试该写入操作. 当然,这个重试会导致 chunk 不同副本数据不一致, 但是 GFS 保证的不是字节级别的一致性, 而是记录级别的, 它保证至少一次 append, 就能保证各副本数据一致, 虽然重复记录但是可以通过 id 去重. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:9","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#281-lease-和-mutation-顺序"},{"categories":null,"content":" 2.8.1 lease 和 mutation 顺序chunk 的每个变更都会反映到每个副本上, 具体如下: master 通过选择一个副本颁发一个 lease 将其指定为 primary. 然后 primary 为 chunk 的并发变更选择一个串行的顺序, 全部副本都遵循该顺序应用变更. 因此全局变更顺序首先由 master 授权 lease 的顺序以及每个 lease 生效期 primary 指定序列号时确定. lease 机制可以最小化 master 的管理开销, 因为针对 chunk 的一部分管理工作让 primary 承担了. 每个 lease 都有一个初始的 60 秒超时, 但是只要 chunk 仍在变更, primary 可以发送请求给 master 要求延长 lease 有效期, 这个过程是通过 heartbeat 实现的. 当然 master 可以在 lease 过期之前吊销它, 比如当 mast 想禁止对正在改名的文件进行变更时. 当 master 失去与primary 通信时, master 可以授权新的 lease 给另一个副本. Figure 2 为执行写操作时的控制流, 具体如下: 1, client 询问 master 哪个 chunkserver 持有要访问的 chunk 的 lease 以及其它副本的位置. 如果没人持有 lease, master 会选一个副本授权之. 2, master 返回 primary 的 id 以及和其它副本的位置, client 会缓存这些信息, 仅当 primary 不可用或者 primary 明确告知不再持有 lease 时再和 master 通信. 3, client 以任意顺序将数据推给全部副本. 每个 chunkserver 将数据首先保存到内部的 LRU 缓存. 图中将控制流和数据流解耦, 基于网络拓扑(该拓扑不关心谁是 primary)调度数据流可以改善性能, 具体后述. 4, 当全部副本确认收到数据后, client 再发送一个写请求给 primary,该请求标记了上一步推送给全部副本的数据, primary 会为(可能来自多个 clients 的)全部变更分配连续的序列号,然后 primary 按照此顺序应用这些变更. 5, 应用本次变更后, primary 将该写请求转发给全部 secondary 副本,这些副本以同样的顺序应用变更. 6, secondary 副本回复 primary 自己已完成操作. 7, primary 回复 client 本次写操作成功 or 失败. 如果只有 primary 和部分 secondary 成功, 则再返回第一步重试之前会尝试重复 3-7. 从 3 到 7 其实就是 2PC (two-phase commit). 如果一个写操作跨多个 chunk, 那么客户端就会将这个写操作分裂成多个写操作. 它们都按照前面描述的步骤执行, 但是可能和其它客户端的写操作交织在一起并发执行. 因此同一个文件区域可能被多个客户端相互覆盖写入. 尽管这个文件区域的全部副本最终是 consistent, 但是最终结果我们无法预知是什么, 所以最终是 consistent 但 undefined. 2.8.2 数据流将数据流和控制流解耦, 让我们可以更高效地利用网络. 下面看看 GFS 是怎么做的. 就像控制流是从 client-\u003eprimary-\u003esecondaries 管道化传输, 数据流从 client 到各个 replicas 也是管道化传输. 具体地, client 挑选离自己最近的 chunkserver (注意这里根本不管谁是 primary, 只关注网络拓扑), 将数据发给它, 然后 这个 chunk server 一边接收 client 的数据一边转发给离自己最近的 chunkserver, 依此类推 … 从而链式完成数据从 client 到每个副本所在 chunkserver 的发送. 最近距离计算: 这个管道化传输过程中, 各个节点通过 IP 地址计算前面提到的 “最近”距离的计算, 比如同一个局域网跟定比跨网段的要更近一些. 管道化传输好处: 这种管道化传输尽可能地利用了每个 chunkserver 的出站带宽, 也最小化了 TCP 连接的时延. 2.8.3 原子化的记录追加操作传统的写操作, 需要指定要写入到的文件偏移量. 并发执行该类操作写同样的区域并不是串行化的, 被写入区域最后状态包含多个客户端的数据片段. GFS 提供了原子化追加操作, 叫 record append, GFS 会确保至少一次写的语义, 将数据原子化地追加到文件末尾. 这类似于在 unix 编程中, 采用 O_APPEND 模式打开文件而且没有竟态条件. record append 作为一种 mutation, 执行流程同 Figure 2, 但在 primary 那有些许不同: 1, 首先 client 肯定是将数据 push 给文件的最后一个 chunk(因为是 appende 嘛). 2, primary 检查追加这个记录后 chunk 是否超过 64MB, 2.1, 如果超过, 则将这个chunk 填满无效数据, 同时告诉 secondaries 也这么干, 然后告诉客户端说满了, 请将数据写入下个chunk. 为了避免产生过多这类因填充导致的空间碎片, 我们要求 record 大小最多为 chunk 的四分之一. 2.2, 如果不超过, primary 将数据写入本地副本,然后告知secondaries 将数据写入到同样的偏移处, 然后响应客户端. 上述过程, 在任何副本写失败, 客户端就要重试该写入操作. 当然,这个重试会导致 chunk 不同副本数据不一致, 但是 GFS 保证的不是字节级别的一致性, 而是记录级别的, 它保证至少一次 append, 就能保证各副本数据一致, 虽然重复记录但是可以通过 id 去重. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:9","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#282-数据流"},{"categories":null,"content":" 2.8.1 lease 和 mutation 顺序chunk 的每个变更都会反映到每个副本上, 具体如下: master 通过选择一个副本颁发一个 lease 将其指定为 primary. 然后 primary 为 chunk 的并发变更选择一个串行的顺序, 全部副本都遵循该顺序应用变更. 因此全局变更顺序首先由 master 授权 lease 的顺序以及每个 lease 生效期 primary 指定序列号时确定. lease 机制可以最小化 master 的管理开销, 因为针对 chunk 的一部分管理工作让 primary 承担了. 每个 lease 都有一个初始的 60 秒超时, 但是只要 chunk 仍在变更, primary 可以发送请求给 master 要求延长 lease 有效期, 这个过程是通过 heartbeat 实现的. 当然 master 可以在 lease 过期之前吊销它, 比如当 mast 想禁止对正在改名的文件进行变更时. 当 master 失去与primary 通信时, master 可以授权新的 lease 给另一个副本. Figure 2 为执行写操作时的控制流, 具体如下: 1, client 询问 master 哪个 chunkserver 持有要访问的 chunk 的 lease 以及其它副本的位置. 如果没人持有 lease, master 会选一个副本授权之. 2, master 返回 primary 的 id 以及和其它副本的位置, client 会缓存这些信息, 仅当 primary 不可用或者 primary 明确告知不再持有 lease 时再和 master 通信. 3, client 以任意顺序将数据推给全部副本. 每个 chunkserver 将数据首先保存到内部的 LRU 缓存. 图中将控制流和数据流解耦, 基于网络拓扑(该拓扑不关心谁是 primary)调度数据流可以改善性能, 具体后述. 4, 当全部副本确认收到数据后, client 再发送一个写请求给 primary,该请求标记了上一步推送给全部副本的数据, primary 会为(可能来自多个 clients 的)全部变更分配连续的序列号,然后 primary 按照此顺序应用这些变更. 5, 应用本次变更后, primary 将该写请求转发给全部 secondary 副本,这些副本以同样的顺序应用变更. 6, secondary 副本回复 primary 自己已完成操作. 7, primary 回复 client 本次写操作成功 or 失败. 如果只有 primary 和部分 secondary 成功, 则再返回第一步重试之前会尝试重复 3-7. 从 3 到 7 其实就是 2PC (two-phase commit). 如果一个写操作跨多个 chunk, 那么客户端就会将这个写操作分裂成多个写操作. 它们都按照前面描述的步骤执行, 但是可能和其它客户端的写操作交织在一起并发执行. 因此同一个文件区域可能被多个客户端相互覆盖写入. 尽管这个文件区域的全部副本最终是 consistent, 但是最终结果我们无法预知是什么, 所以最终是 consistent 但 undefined. 2.8.2 数据流将数据流和控制流解耦, 让我们可以更高效地利用网络. 下面看看 GFS 是怎么做的. 就像控制流是从 client-\u003eprimary-\u003esecondaries 管道化传输, 数据流从 client 到各个 replicas 也是管道化传输. 具体地, client 挑选离自己最近的 chunkserver (注意这里根本不管谁是 primary, 只关注网络拓扑), 将数据发给它, 然后 这个 chunk server 一边接收 client 的数据一边转发给离自己最近的 chunkserver, 依此类推 … 从而链式完成数据从 client 到每个副本所在 chunkserver 的发送. 最近距离计算: 这个管道化传输过程中, 各个节点通过 IP 地址计算前面提到的 “最近”距离的计算, 比如同一个局域网跟定比跨网段的要更近一些. 管道化传输好处: 这种管道化传输尽可能地利用了每个 chunkserver 的出站带宽, 也最小化了 TCP 连接的时延. 2.8.3 原子化的记录追加操作传统的写操作, 需要指定要写入到的文件偏移量. 并发执行该类操作写同样的区域并不是串行化的, 被写入区域最后状态包含多个客户端的数据片段. GFS 提供了原子化追加操作, 叫 record append, GFS 会确保至少一次写的语义, 将数据原子化地追加到文件末尾. 这类似于在 unix 编程中, 采用 O_APPEND 模式打开文件而且没有竟态条件. record append 作为一种 mutation, 执行流程同 Figure 2, 但在 primary 那有些许不同: 1, 首先 client 肯定是将数据 push 给文件的最后一个 chunk(因为是 appende 嘛). 2, primary 检查追加这个记录后 chunk 是否超过 64MB, 2.1, 如果超过, 则将这个chunk 填满无效数据, 同时告诉 secondaries 也这么干, 然后告诉客户端说满了, 请将数据写入下个chunk. 为了避免产生过多这类因填充导致的空间碎片, 我们要求 record 大小最多为 chunk 的四分之一. 2.2, 如果不超过, primary 将数据写入本地副本,然后告知secondaries 将数据写入到同样的偏移处, 然后响应客户端. 上述过程, 在任何副本写失败, 客户端就要重试该写入操作. 当然,这个重试会导致 chunk 不同副本数据不一致, 但是 GFS 保证的不是字节级别的一致性, 而是记录级别的, 它保证至少一次 append, 就能保证各副本数据一致, 虽然重复记录但是可以通过 id 去重. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:9","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#283-原子化的记录追加操作"},{"categories":null,"content":" 2.9 Snapshot 快照snapshot 操作用于制作当前文件和目录的快照, 速度非常快. snapshot 用的是 COW (copy-on-write) 技术实现, 具体如下: 当master 收到 snapshot 请求时, 它首先吊销要进行 snapshot 的文件相关 chunks 的 lease, 这样后续针对这些 chunks 的写操作强迫 client 先和 master 交互, 这就为创建快照争取了时间. lease 吊销后, master 将 snapshot 操作记录到日志中, 然后将其应用到内存状态上制作快照. 新创建的 snapshot 文件指向源文件同样的 chunks, 计数加一. 此时如果客户端要写这些 chunks, master 就会察觉引用计数大于 1, 此时触发 COW 操作. COW 过程: master 指示每个拥有 chunk C 副本的 chunkserver 都新建一个 chunk C‘, 而且 C’ 创建在同 C 一样的 server 上, 这可以使得 COW 在本地而非通过网络进行. 创建完 C‘ 并拷贝完数据后, 集群就有两组一样的 chunk 副本了, 剩下的处理流程就同之前一样了, master 针对 C’ 授权 lease 给某个副本并响应客户端, 客户端开始写入数据. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:2:10","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#29-snapshot-快照"},{"categories":null,"content":" 3 Master 行为Master 执行全部和命名空间有关的操作. 另外, master 管理着整个系统的 chunk 复制: chunk 布局决策 创建新 chunks 和其副本 协调多种系统级别的活动以保障 chunks 副本健全, 各个 chunkserver 的负载均衡, 回收未使用的存储空间 下面挨个讨论上述话题. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#3-master-行为"},{"categories":null,"content":" 3.1 命名空间管理和锁机制GFS 允许多个操作并发, 通过锁来将其串行化. 读锁, 防止目标被删除/重命名/snapshotted; 写锁, 防止并发创建同名目标文件. 同 Unix 不同, GFS 没有为每个目录设计一个保存其文件列表的数据结构. GFS 的命名空间可以看作一个速查表, 保存了从全路径名(文件名或目录名, 可以看作一个前缀树)到元数据的映射. 通过前缀压缩, 可以使得整个表被保存到内存中. 命名空间树上面的每个节点都关联了一个读写锁. 由于没有真正的目录结构, 所以这使得我们可以并发地在同一个目录下创建文件, 只要获取这个所谓的目录的读锁(防止目标被修改)同时获取目标文件的写锁(防止在同一个目录下生成同名文件)即可. 为避免死锁, 加锁顺序全局一致: 先按照命名空间树不同层次对锁排序, 如果在同一层则按照字典序对锁排序. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#31-命名空间管理和锁机制"},{"categories":null,"content":" 3.2 chunk 副本布局GFS 的分布式体现了多个层次: 1, 首先 GFS 集群一般涉及上百台 chunkservers 2, 这些 chunkservers 一般分布在多个机架. 所以同一个 chunk 的副本布局不但要考虑多 chunkservers 还要考虑这些 chunkservers 不能集中在同一个机架, 这么做就为了实现: 可靠性 可用性 最大化网络带宽使用率(机架进出带宽可能小于这个机架上的全部机器各自网卡带宽加总.), 比如热点副本集中到同一个 chunkserver 或者同一个机架, 并发读写就会出现瓶颈, 这在后面测量部分会再细述. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#32-chunk-副本布局"},{"categories":null,"content":" 3.3 副本的新建, 重复制与再平衡 3.3.1 副本新建三种情况下新建副本: 1, 新建 chunk 时 2, 重复制 3, 重平衡 新建 chunk 时新副本安置在哪儿, 主要考虑下面几点: 1, 尽量把新副本放在磁盘使用率低于平均水平的 chunkserver 上, 这样各个机器会逐渐平均. 2, 虽然副本创建操作本身消耗不多, 但它预示着大量的写流量即将到达. 所以新建副本时会尽量让每个 chunkserver 近期的副本(不区分 chunk)新建数尽量的差不多, 避免写流量涌入少数 chunkserver. 3, 就像前一节讨论的, 让副本尽量分布在多个机架上. 3.3.2 副本重复制当 chunk 副本因子低于设定时, master 就会触发重复制. 如果有多个 chunk 满足条件, 则执行优先级就是: 1, 哪个 chunk 距离目标副本因子越远就谁优先重复制; 2, 还有就是活跃的文件 chunks 优先级高于被删除文件的 chunks. 3, 另外就是优先那些当前阻塞住客户端的 chunks. 重复制后的副本放到哪个 chunkserver, 原则同副本新建所描述: 1, 让每个磁盘空间使用率尽量相同; 2, 限制同一个 chunkserver 上同时活跃的 clone 操作; 3, 另副本尽量分散到各个机架. 为了避免副本 clone 流量压倒客户端流量, master 会限制同时活跃的 clone操作个数. 同时, 每个 chunkserver 会限制自己用于 clone 的带宽, 方法就是限制自己到其它源 chunkserver 的读请求数目. 3.3.3 副本重平衡master 会为了更好的磁盘使用率和负载均衡而周期性地做副本 rebalance. master 做 rebalance 时布局标准同重复制. rebalance 选择要移除哪个 chunkserver 的副本时倾向于那些空闲空间低于平均值的 chunkserver, 目的也是最终拉平各个 chunkserver 的磁盘使用. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#33-副本的新建-重复制与再平衡"},{"categories":null,"content":" 3.3 副本的新建, 重复制与再平衡 3.3.1 副本新建三种情况下新建副本: 1, 新建 chunk 时 2, 重复制 3, 重平衡 新建 chunk 时新副本安置在哪儿, 主要考虑下面几点: 1, 尽量把新副本放在磁盘使用率低于平均水平的 chunkserver 上, 这样各个机器会逐渐平均. 2, 虽然副本创建操作本身消耗不多, 但它预示着大量的写流量即将到达. 所以新建副本时会尽量让每个 chunkserver 近期的副本(不区分 chunk)新建数尽量的差不多, 避免写流量涌入少数 chunkserver. 3, 就像前一节讨论的, 让副本尽量分布在多个机架上. 3.3.2 副本重复制当 chunk 副本因子低于设定时, master 就会触发重复制. 如果有多个 chunk 满足条件, 则执行优先级就是: 1, 哪个 chunk 距离目标副本因子越远就谁优先重复制; 2, 还有就是活跃的文件 chunks 优先级高于被删除文件的 chunks. 3, 另外就是优先那些当前阻塞住客户端的 chunks. 重复制后的副本放到哪个 chunkserver, 原则同副本新建所描述: 1, 让每个磁盘空间使用率尽量相同; 2, 限制同一个 chunkserver 上同时活跃的 clone 操作; 3, 另副本尽量分散到各个机架. 为了避免副本 clone 流量压倒客户端流量, master 会限制同时活跃的 clone操作个数. 同时, 每个 chunkserver 会限制自己用于 clone 的带宽, 方法就是限制自己到其它源 chunkserver 的读请求数目. 3.3.3 副本重平衡master 会为了更好的磁盘使用率和负载均衡而周期性地做副本 rebalance. master 做 rebalance 时布局标准同重复制. rebalance 选择要移除哪个 chunkserver 的副本时倾向于那些空闲空间低于平均值的 chunkserver, 目的也是最终拉平各个 chunkserver 的磁盘使用. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#331-副本新建"},{"categories":null,"content":" 3.3 副本的新建, 重复制与再平衡 3.3.1 副本新建三种情况下新建副本: 1, 新建 chunk 时 2, 重复制 3, 重平衡 新建 chunk 时新副本安置在哪儿, 主要考虑下面几点: 1, 尽量把新副本放在磁盘使用率低于平均水平的 chunkserver 上, 这样各个机器会逐渐平均. 2, 虽然副本创建操作本身消耗不多, 但它预示着大量的写流量即将到达. 所以新建副本时会尽量让每个 chunkserver 近期的副本(不区分 chunk)新建数尽量的差不多, 避免写流量涌入少数 chunkserver. 3, 就像前一节讨论的, 让副本尽量分布在多个机架上. 3.3.2 副本重复制当 chunk 副本因子低于设定时, master 就会触发重复制. 如果有多个 chunk 满足条件, 则执行优先级就是: 1, 哪个 chunk 距离目标副本因子越远就谁优先重复制; 2, 还有就是活跃的文件 chunks 优先级高于被删除文件的 chunks. 3, 另外就是优先那些当前阻塞住客户端的 chunks. 重复制后的副本放到哪个 chunkserver, 原则同副本新建所描述: 1, 让每个磁盘空间使用率尽量相同; 2, 限制同一个 chunkserver 上同时活跃的 clone 操作; 3, 另副本尽量分散到各个机架. 为了避免副本 clone 流量压倒客户端流量, master 会限制同时活跃的 clone操作个数. 同时, 每个 chunkserver 会限制自己用于 clone 的带宽, 方法就是限制自己到其它源 chunkserver 的读请求数目. 3.3.3 副本重平衡master 会为了更好的磁盘使用率和负载均衡而周期性地做副本 rebalance. master 做 rebalance 时布局标准同重复制. rebalance 选择要移除哪个 chunkserver 的副本时倾向于那些空闲空间低于平均值的 chunkserver, 目的也是最终拉平各个 chunkserver 的磁盘使用. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#332-副本重复制"},{"categories":null,"content":" 3.3 副本的新建, 重复制与再平衡 3.3.1 副本新建三种情况下新建副本: 1, 新建 chunk 时 2, 重复制 3, 重平衡 新建 chunk 时新副本安置在哪儿, 主要考虑下面几点: 1, 尽量把新副本放在磁盘使用率低于平均水平的 chunkserver 上, 这样各个机器会逐渐平均. 2, 虽然副本创建操作本身消耗不多, 但它预示着大量的写流量即将到达. 所以新建副本时会尽量让每个 chunkserver 近期的副本(不区分 chunk)新建数尽量的差不多, 避免写流量涌入少数 chunkserver. 3, 就像前一节讨论的, 让副本尽量分布在多个机架上. 3.3.2 副本重复制当 chunk 副本因子低于设定时, master 就会触发重复制. 如果有多个 chunk 满足条件, 则执行优先级就是: 1, 哪个 chunk 距离目标副本因子越远就谁优先重复制; 2, 还有就是活跃的文件 chunks 优先级高于被删除文件的 chunks. 3, 另外就是优先那些当前阻塞住客户端的 chunks. 重复制后的副本放到哪个 chunkserver, 原则同副本新建所描述: 1, 让每个磁盘空间使用率尽量相同; 2, 限制同一个 chunkserver 上同时活跃的 clone 操作; 3, 另副本尽量分散到各个机架. 为了避免副本 clone 流量压倒客户端流量, master 会限制同时活跃的 clone操作个数. 同时, 每个 chunkserver 会限制自己用于 clone 的带宽, 方法就是限制自己到其它源 chunkserver 的读请求数目. 3.3.3 副本重平衡master 会为了更好的磁盘使用率和负载均衡而周期性地做副本 rebalance. master 做 rebalance 时布局标准同重复制. rebalance 选择要移除哪个 chunkserver 的副本时倾向于那些空闲空间低于平均值的 chunkserver, 目的也是最终拉平各个 chunkserver 的磁盘使用. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#333-副本重平衡"},{"categories":null,"content":" 3.4 垃圾回收 3.4.1 惰性回收一个文件被删除后, GFS 不会立即回收它所占用地物理空间, 而是通过周期性地 GC 在文件和 chunk 两个层次进行惰性回收. 我们发现这么做使得整个系统更加简单也更加可靠. 3.4.2 回收机制文件层面: 同其他操作, 删除操作会先记录到 master 日志. 然后被删除文件被标记为包含删除时间戳的隐藏名. master 周期性扫描文件系统命名空间时移除那些被标记超过三天(可配置)的文件. 真正移除之前这些文件仍可读, 甚至可以将名字改成正常名字表示不再删除. 当隐藏文件被移除后, 它对应的内存中的元数据也会被擦除, 这相当于断开了这个文件同其 chunks 的连接. Chunk 层面: 同样地, 针对 chunk 命名空间的周期性扫描, master 会将从任何文件都不可达的 chunks 标记为孤儿 chunks. chunkserver 在和 master 周期性的心跳消息中上报自己持有的 chunks, master 会回复那些不再存在的 chunks 标识, chunkserver 收到后自主删除这些 chunks 对应的副本. 3.4.3 几点讨论虽然在编程语言中, 分布式垃圾回收非常难, 但是针对 GFS 却很简单: 我们可以很容易地识别到 chunks 的引用: 它们就在 file-to-chunk mappings 中, 这些信息由 master 专门维护. 我们也可以很容易识别全部 chunk 副本: 它们就是 chunkserver 指定目录下的 linux 文件. 这些副本对 master 来说都不是垃圾. GC 相比 eager deletion 的好处: 1, 在组件容易故障的分布式系统中更简单可靠. 如果发删除消息则可能丢失, 需要重发送, 维护起来很复杂. GC 提供了统一可靠的清理不再有用的副本的方式. 2, 将 GC 纳入到周期性后台任务(其它还有命名空间扫描和心跳), 批量处理摊销了消耗. 而且这种后台任务仅当 master 相对空闲时候才做可以保证对客户端的响应. 3, GC 的惰性也避免了因误删除(意外且不可逆)导致的数据安全问题. 最大的缺点: 不能在存储紧张时候及时回收. 但 GFS 支持连续两次删除立即回收空间. 另外, GFS 支持为不同的命名空间指定副本策略(如副本因子)和删除策略. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:4","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#34-垃圾回收"},{"categories":null,"content":" 3.4 垃圾回收 3.4.1 惰性回收一个文件被删除后, GFS 不会立即回收它所占用地物理空间, 而是通过周期性地 GC 在文件和 chunk 两个层次进行惰性回收. 我们发现这么做使得整个系统更加简单也更加可靠. 3.4.2 回收机制文件层面: 同其他操作, 删除操作会先记录到 master 日志. 然后被删除文件被标记为包含删除时间戳的隐藏名. master 周期性扫描文件系统命名空间时移除那些被标记超过三天(可配置)的文件. 真正移除之前这些文件仍可读, 甚至可以将名字改成正常名字表示不再删除. 当隐藏文件被移除后, 它对应的内存中的元数据也会被擦除, 这相当于断开了这个文件同其 chunks 的连接. Chunk 层面: 同样地, 针对 chunk 命名空间的周期性扫描, master 会将从任何文件都不可达的 chunks 标记为孤儿 chunks. chunkserver 在和 master 周期性的心跳消息中上报自己持有的 chunks, master 会回复那些不再存在的 chunks 标识, chunkserver 收到后自主删除这些 chunks 对应的副本. 3.4.3 几点讨论虽然在编程语言中, 分布式垃圾回收非常难, 但是针对 GFS 却很简单: 我们可以很容易地识别到 chunks 的引用: 它们就在 file-to-chunk mappings 中, 这些信息由 master 专门维护. 我们也可以很容易识别全部 chunk 副本: 它们就是 chunkserver 指定目录下的 linux 文件. 这些副本对 master 来说都不是垃圾. GC 相比 eager deletion 的好处: 1, 在组件容易故障的分布式系统中更简单可靠. 如果发删除消息则可能丢失, 需要重发送, 维护起来很复杂. GC 提供了统一可靠的清理不再有用的副本的方式. 2, 将 GC 纳入到周期性后台任务(其它还有命名空间扫描和心跳), 批量处理摊销了消耗. 而且这种后台任务仅当 master 相对空闲时候才做可以保证对客户端的响应. 3, GC 的惰性也避免了因误删除(意外且不可逆)导致的数据安全问题. 最大的缺点: 不能在存储紧张时候及时回收. 但 GFS 支持连续两次删除立即回收空间. 另外, GFS 支持为不同的命名空间指定副本策略(如副本因子)和删除策略. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:4","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#341-惰性回收"},{"categories":null,"content":" 3.4 垃圾回收 3.4.1 惰性回收一个文件被删除后, GFS 不会立即回收它所占用地物理空间, 而是通过周期性地 GC 在文件和 chunk 两个层次进行惰性回收. 我们发现这么做使得整个系统更加简单也更加可靠. 3.4.2 回收机制文件层面: 同其他操作, 删除操作会先记录到 master 日志. 然后被删除文件被标记为包含删除时间戳的隐藏名. master 周期性扫描文件系统命名空间时移除那些被标记超过三天(可配置)的文件. 真正移除之前这些文件仍可读, 甚至可以将名字改成正常名字表示不再删除. 当隐藏文件被移除后, 它对应的内存中的元数据也会被擦除, 这相当于断开了这个文件同其 chunks 的连接. Chunk 层面: 同样地, 针对 chunk 命名空间的周期性扫描, master 会将从任何文件都不可达的 chunks 标记为孤儿 chunks. chunkserver 在和 master 周期性的心跳消息中上报自己持有的 chunks, master 会回复那些不再存在的 chunks 标识, chunkserver 收到后自主删除这些 chunks 对应的副本. 3.4.3 几点讨论虽然在编程语言中, 分布式垃圾回收非常难, 但是针对 GFS 却很简单: 我们可以很容易地识别到 chunks 的引用: 它们就在 file-to-chunk mappings 中, 这些信息由 master 专门维护. 我们也可以很容易识别全部 chunk 副本: 它们就是 chunkserver 指定目录下的 linux 文件. 这些副本对 master 来说都不是垃圾. GC 相比 eager deletion 的好处: 1, 在组件容易故障的分布式系统中更简单可靠. 如果发删除消息则可能丢失, 需要重发送, 维护起来很复杂. GC 提供了统一可靠的清理不再有用的副本的方式. 2, 将 GC 纳入到周期性后台任务(其它还有命名空间扫描和心跳), 批量处理摊销了消耗. 而且这种后台任务仅当 master 相对空闲时候才做可以保证对客户端的响应. 3, GC 的惰性也避免了因误删除(意外且不可逆)导致的数据安全问题. 最大的缺点: 不能在存储紧张时候及时回收. 但 GFS 支持连续两次删除立即回收空间. 另外, GFS 支持为不同的命名空间指定副本策略(如副本因子)和删除策略. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:4","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#342-回收机制"},{"categories":null,"content":" 3.4 垃圾回收 3.4.1 惰性回收一个文件被删除后, GFS 不会立即回收它所占用地物理空间, 而是通过周期性地 GC 在文件和 chunk 两个层次进行惰性回收. 我们发现这么做使得整个系统更加简单也更加可靠. 3.4.2 回收机制文件层面: 同其他操作, 删除操作会先记录到 master 日志. 然后被删除文件被标记为包含删除时间戳的隐藏名. master 周期性扫描文件系统命名空间时移除那些被标记超过三天(可配置)的文件. 真正移除之前这些文件仍可读, 甚至可以将名字改成正常名字表示不再删除. 当隐藏文件被移除后, 它对应的内存中的元数据也会被擦除, 这相当于断开了这个文件同其 chunks 的连接. Chunk 层面: 同样地, 针对 chunk 命名空间的周期性扫描, master 会将从任何文件都不可达的 chunks 标记为孤儿 chunks. chunkserver 在和 master 周期性的心跳消息中上报自己持有的 chunks, master 会回复那些不再存在的 chunks 标识, chunkserver 收到后自主删除这些 chunks 对应的副本. 3.4.3 几点讨论虽然在编程语言中, 分布式垃圾回收非常难, 但是针对 GFS 却很简单: 我们可以很容易地识别到 chunks 的引用: 它们就在 file-to-chunk mappings 中, 这些信息由 master 专门维护. 我们也可以很容易识别全部 chunk 副本: 它们就是 chunkserver 指定目录下的 linux 文件. 这些副本对 master 来说都不是垃圾. GC 相比 eager deletion 的好处: 1, 在组件容易故障的分布式系统中更简单可靠. 如果发删除消息则可能丢失, 需要重发送, 维护起来很复杂. GC 提供了统一可靠的清理不再有用的副本的方式. 2, 将 GC 纳入到周期性后台任务(其它还有命名空间扫描和心跳), 批量处理摊销了消耗. 而且这种后台任务仅当 master 相对空闲时候才做可以保证对客户端的响应. 3, GC 的惰性也避免了因误删除(意外且不可逆)导致的数据安全问题. 最大的缺点: 不能在存储紧张时候及时回收. 但 GFS 支持连续两次删除立即回收空间. 另外, GFS 支持为不同的命名空间指定副本策略(如副本因子)和删除策略. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:4","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#343-几点讨论"},{"categories":null,"content":" 3.5 过期副本检测chunkserver 挂掉或者下线就会导致其上的 chunks 过期. master 维护着一个 chunk 版本号来识别最新和过期副本. master 授权一个新的 lease 时就会递增 chunk 版本号并通知其它副本, 并且 master 和这些副本会持久化这个版本号. 当某个副本挂掉重启后, 它会上报自己的 chunks 和版本号给 master, master就能检测到过期. 如果 master 发现有版本号比自己记录的还要大, 则认为自己授权 lease 时候出错并将该版本号作为最新版本号. master 在响应客户端哪个 chunkserver 持有它所请求的 chunk 时会在响应中包含版本号以让客户端进行校验; master 在指示某个 chunkserver 去另一个 chunkserver 拷贝数据时也会告诉它当前的版本号, 供其校验. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:3:5","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#35-过期副本检测"},{"categories":null,"content":" 4 容错与检测设计这个系统最大的挑战时应对频繁的组件失效. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#4-容错与检测"},{"categories":null,"content":" 4.1 高可用 4.1.1 保持高可用的两个手段 1, 快速恢复,不管之前如何下线的, master 和 chunkservers 可以几秒内即可重启并恢复数据. 2, 多副本, 默认三副本. Master 复制master 的操作日志和 checkpoint 会被保存到多台机器. 一个更新操作仅当其被记录到 master 本地和远程机器上才算被提交. master 挂了, 监控系统会立即在其它机器上启动一个 master 并快速从日志恢复状态. clients 用的是域名访问 master, 所以可以快速感知这个变化. 另外 GFS 还提供了影子 masters, 它们只提供了读操作, 而且数据可能会稍微落后 primary master 一秒. 针对那些不怎么变动的文件或者应用不太在乎稍微过期的数据的时候, 这些影子 masters 可以为 primary master 分担一些读请求. 影子 masters 会拉取日志副本并应用保持自己更新, 而且它们也会在启动时查询 chunkservers 获取 chunk 副本位置信息(因为这些信息不会被持久化到日志只能自己去主动去查), 也和 chunkservers 保持心跳交换信息以监控它们的状态. 当然副本布局/删除等等变更操作还是由 primary master 负责, 影子 masters 只读. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#41-高可用"},{"categories":null,"content":" 4.1 高可用 4.1.1 保持高可用的两个手段 1, 快速恢复,不管之前如何下线的, master 和 chunkservers 可以几秒内即可重启并恢复数据. 2, 多副本, 默认三副本. Master 复制master 的操作日志和 checkpoint 会被保存到多台机器. 一个更新操作仅当其被记录到 master 本地和远程机器上才算被提交. master 挂了, 监控系统会立即在其它机器上启动一个 master 并快速从日志恢复状态. clients 用的是域名访问 master, 所以可以快速感知这个变化. 另外 GFS 还提供了影子 masters, 它们只提供了读操作, 而且数据可能会稍微落后 primary master 一秒. 针对那些不怎么变动的文件或者应用不太在乎稍微过期的数据的时候, 这些影子 masters 可以为 primary master 分担一些读请求. 影子 masters 会拉取日志副本并应用保持自己更新, 而且它们也会在启动时查询 chunkservers 获取 chunk 副本位置信息(因为这些信息不会被持久化到日志只能自己去主动去查), 也和 chunkservers 保持心跳交换信息以监控它们的状态. 当然副本布局/删除等等变更操作还是由 primary master 负责, 影子 masters 只读. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#411-保持高可用的两个手段"},{"categories":null,"content":" 4.1 高可用 4.1.1 保持高可用的两个手段 1, 快速恢复,不管之前如何下线的, master 和 chunkservers 可以几秒内即可重启并恢复数据. 2, 多副本, 默认三副本. Master 复制master 的操作日志和 checkpoint 会被保存到多台机器. 一个更新操作仅当其被记录到 master 本地和远程机器上才算被提交. master 挂了, 监控系统会立即在其它机器上启动一个 master 并快速从日志恢复状态. clients 用的是域名访问 master, 所以可以快速感知这个变化. 另外 GFS 还提供了影子 masters, 它们只提供了读操作, 而且数据可能会稍微落后 primary master 一秒. 针对那些不怎么变动的文件或者应用不太在乎稍微过期的数据的时候, 这些影子 masters 可以为 primary master 分担一些读请求. 影子 masters 会拉取日志副本并应用保持自己更新, 而且它们也会在启动时查询 chunkservers 获取 chunk 副本位置信息(因为这些信息不会被持久化到日志只能自己去主动去查), 也和 chunkservers 保持心跳交换信息以监控它们的状态. 当然副本布局/删除等等变更操作还是由 primary master 负责, 影子 masters 只读. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#master-复制"},{"categories":null,"content":" 4.2 数据完备性chunkserver 靠校验和来检测数据是否损坏, 而不是靠比对各个副本,那不太可行. 每个 chunk 被切分成多个 64KB 大小的 blocks. 每个 block 都有一个对应的 32 bit 校验和. 校验和也会被保存到内存中同时会和日志一起持久化. chunkserver 响应请求者数据之前会计算校验和(有点性能消耗)并和存储的校验和比对, 如果不一致则响应错误并上报给 master, 请求者会去其它副本读数据, 同时 master 会指示 chunkserver去从其他副本恢复数据, 然后删掉损坏的副本. 空闲时, chunkserver 会扫描和校验不活跃的 chunks, 检测损坏的 chunks 并上报. master 就会指示创建新的 chunks 并删除损坏的 chunks. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:2","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#42-数据完备性"},{"categories":null,"content":" 4.3 诊断工具诊断问题只能靠日志, GFS 诊断日志记录了 chunkserver 上下线等重大事件和全部 RPC 请求响应. 可以通过日志来重建完整的交互历史来诊断问题. 日志的开销很小, 尤其和得到的好处比起来. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:4:3","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#43-诊断工具"},{"categories":null,"content":" 5 测量接下来, 我们看看 GFS 架构和实现中的瓶颈, 以及真实集群中的若干数字. 注意, 这都是 2003 年的数据, 我们要观察的是 GFS 这么大系统的度量方法以及它如何在当年那种相比现在硬件条件那么差的情况下发挥其价值的. ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:5:0","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#5-测量"},{"categories":null,"content":" 5.1 测量用的微基准测试集群由 1 master, 2 master replicas, 16 chunkservers, 16 clients 构成. 这么搭就为了方便测试, 真实的集群有几百个 chunkservers 和几百个 clients 构成. 所有机器都是双核 1.4GHz, 2GB 内存, 两个每分钟 5400 转的 80GB 磁盘, 一个 100Mbps 的全双工以太网. master 和 chunkserver 共 19 台机器都连接到同一个 HP 2524 交换机上, 16 个 clients 连接到另一个交换机上, 两个交换机通过 1Gbps 链路连接. 5.1.1 reads 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.a 显示 N 个 clients 的读取速率聚合以及它的理论上限. 当两个交换机之间的 1Gbps 跑满的时候达到理论上限 125MB/s; 当每个 client 的 100Mbps 跑满时, 单个 client 机器达到理论上限 12.5MB/s. 当仅有一个 client 读取时, 观察到的读取速率为 10MB/s, 大约是每个 client 上限的 80%(即 10/12.5). 当 16 个 clients 一起读取时, 观察到的聚合读取速率为 94MB/s, 大约为理论上限的 75%(即 94/125); 或者 每个 client 大约 6MB/s. 从 80% 降到 75% 的原因是: 当读取客户端变多, 多个客户端同时从同一个 chunkserver 读取的概率也变大, 从而导致 chunkserver 的网卡带宽被多个客户端争用的更厉害了. 5.1.2 writes 测试N 个 clients 同时写多个 N 个不同的文件. 每个 client 写 1GB 数据到一个新文件, 每次写入 1MB. 聚合写入速率和理论上限如 3.b 所示. 理论上限为 67MB/s, 因为我们需要将每个字节写到 16 个 chunkservers 中的 3 个里, 每个 chunkserver 入口带宽上限为 12.5MB/s. 如果单个 client 写入, 则观察到的写入速率为 6.3MB/s, 大约是上限的一半. 罪魁祸首是网络协议栈, 因为它与我们把数据 push 给 chunk 副本的管道化方案不太搭配. 数据从一个副本传播给另一个副本延迟降低了整体的写入速率. 如果是 16 个 clients 一起写入, 观察到的聚合写入速率为 35MB/s(每个 client 平均 2.2MB/s), 大约是理论上限的一半(即 35/67). 就像读取测试中, 随着执行写操作的 clients 增多, 则同一个 chunkserver 被多个 clients 并发写的几率增大, 此 chunkserver 的入口带宽被更多 clients 争用, 而且 16 个 clients 的写要比读竞争更激烈, 因为一个字节要写到三个 chunkserver. 5.1.3 record appends 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.c 显示了记录追加操作的性能. N 个 clients 同时向同一个文件追加. 性能被存储最后一个 chunk 的 chunkservers 的网络带宽限制住了, 与 clients 个数无关. 当只有一个 client 写入时, 速率为 6.0MB/s, 当 16 个 clients 一起写入时, 速率降到了 4.8MB/s, 主要原因是网络拥塞和抖动. 实际使用中, 我们的应用程序倾向于并发生成若干前述文件而不是一个. 换句话说, N 个 clients 同时向 M 个共享文件追加, N 和 M 都是几千级别的. 因此前面提到的网络拥塞在实际中不是啥大问题, 因为 client 写入一个 chunkserver 时, 当一个文件忙的时候, 可以写另一个文件. –End– ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:5:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#51-测量用的微基准"},{"categories":null,"content":" 5.1 测量用的微基准测试集群由 1 master, 2 master replicas, 16 chunkservers, 16 clients 构成. 这么搭就为了方便测试, 真实的集群有几百个 chunkservers 和几百个 clients 构成. 所有机器都是双核 1.4GHz, 2GB 内存, 两个每分钟 5400 转的 80GB 磁盘, 一个 100Mbps 的全双工以太网. master 和 chunkserver 共 19 台机器都连接到同一个 HP 2524 交换机上, 16 个 clients 连接到另一个交换机上, 两个交换机通过 1Gbps 链路连接. 5.1.1 reads 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.a 显示 N 个 clients 的读取速率聚合以及它的理论上限. 当两个交换机之间的 1Gbps 跑满的时候达到理论上限 125MB/s; 当每个 client 的 100Mbps 跑满时, 单个 client 机器达到理论上限 12.5MB/s. 当仅有一个 client 读取时, 观察到的读取速率为 10MB/s, 大约是每个 client 上限的 80%(即 10/12.5). 当 16 个 clients 一起读取时, 观察到的聚合读取速率为 94MB/s, 大约为理论上限的 75%(即 94/125); 或者 每个 client 大约 6MB/s. 从 80% 降到 75% 的原因是: 当读取客户端变多, 多个客户端同时从同一个 chunkserver 读取的概率也变大, 从而导致 chunkserver 的网卡带宽被多个客户端争用的更厉害了. 5.1.2 writes 测试N 个 clients 同时写多个 N 个不同的文件. 每个 client 写 1GB 数据到一个新文件, 每次写入 1MB. 聚合写入速率和理论上限如 3.b 所示. 理论上限为 67MB/s, 因为我们需要将每个字节写到 16 个 chunkservers 中的 3 个里, 每个 chunkserver 入口带宽上限为 12.5MB/s. 如果单个 client 写入, 则观察到的写入速率为 6.3MB/s, 大约是上限的一半. 罪魁祸首是网络协议栈, 因为它与我们把数据 push 给 chunk 副本的管道化方案不太搭配. 数据从一个副本传播给另一个副本延迟降低了整体的写入速率. 如果是 16 个 clients 一起写入, 观察到的聚合写入速率为 35MB/s(每个 client 平均 2.2MB/s), 大约是理论上限的一半(即 35/67). 就像读取测试中, 随着执行写操作的 clients 增多, 则同一个 chunkserver 被多个 clients 并发写的几率增大, 此 chunkserver 的入口带宽被更多 clients 争用, 而且 16 个 clients 的写要比读竞争更激烈, 因为一个字节要写到三个 chunkserver. 5.1.3 record appends 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.c 显示了记录追加操作的性能. N 个 clients 同时向同一个文件追加. 性能被存储最后一个 chunk 的 chunkservers 的网络带宽限制住了, 与 clients 个数无关. 当只有一个 client 写入时, 速率为 6.0MB/s, 当 16 个 clients 一起写入时, 速率降到了 4.8MB/s, 主要原因是网络拥塞和抖动. 实际使用中, 我们的应用程序倾向于并发生成若干前述文件而不是一个. 换句话说, N 个 clients 同时向 M 个共享文件追加, N 和 M 都是几千级别的. 因此前面提到的网络拥塞在实际中不是啥大问题, 因为 client 写入一个 chunkserver 时, 当一个文件忙的时候, 可以写另一个文件. –End– ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:5:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#511-reads-测试"},{"categories":null,"content":" 5.1 测量用的微基准测试集群由 1 master, 2 master replicas, 16 chunkservers, 16 clients 构成. 这么搭就为了方便测试, 真实的集群有几百个 chunkservers 和几百个 clients 构成. 所有机器都是双核 1.4GHz, 2GB 内存, 两个每分钟 5400 转的 80GB 磁盘, 一个 100Mbps 的全双工以太网. master 和 chunkserver 共 19 台机器都连接到同一个 HP 2524 交换机上, 16 个 clients 连接到另一个交换机上, 两个交换机通过 1Gbps 链路连接. 5.1.1 reads 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.a 显示 N 个 clients 的读取速率聚合以及它的理论上限. 当两个交换机之间的 1Gbps 跑满的时候达到理论上限 125MB/s; 当每个 client 的 100Mbps 跑满时, 单个 client 机器达到理论上限 12.5MB/s. 当仅有一个 client 读取时, 观察到的读取速率为 10MB/s, 大约是每个 client 上限的 80%(即 10/12.5). 当 16 个 clients 一起读取时, 观察到的聚合读取速率为 94MB/s, 大约为理论上限的 75%(即 94/125); 或者 每个 client 大约 6MB/s. 从 80% 降到 75% 的原因是: 当读取客户端变多, 多个客户端同时从同一个 chunkserver 读取的概率也变大, 从而导致 chunkserver 的网卡带宽被多个客户端争用的更厉害了. 5.1.2 writes 测试N 个 clients 同时写多个 N 个不同的文件. 每个 client 写 1GB 数据到一个新文件, 每次写入 1MB. 聚合写入速率和理论上限如 3.b 所示. 理论上限为 67MB/s, 因为我们需要将每个字节写到 16 个 chunkservers 中的 3 个里, 每个 chunkserver 入口带宽上限为 12.5MB/s. 如果单个 client 写入, 则观察到的写入速率为 6.3MB/s, 大约是上限的一半. 罪魁祸首是网络协议栈, 因为它与我们把数据 push 给 chunk 副本的管道化方案不太搭配. 数据从一个副本传播给另一个副本延迟降低了整体的写入速率. 如果是 16 个 clients 一起写入, 观察到的聚合写入速率为 35MB/s(每个 client 平均 2.2MB/s), 大约是理论上限的一半(即 35/67). 就像读取测试中, 随着执行写操作的 clients 增多, 则同一个 chunkserver 被多个 clients 并发写的几率增大, 此 chunkserver 的入口带宽被更多 clients 争用, 而且 16 个 clients 的写要比读竞争更激烈, 因为一个字节要写到三个 chunkserver. 5.1.3 record appends 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.c 显示了记录追加操作的性能. N 个 clients 同时向同一个文件追加. 性能被存储最后一个 chunk 的 chunkservers 的网络带宽限制住了, 与 clients 个数无关. 当只有一个 client 写入时, 速率为 6.0MB/s, 当 16 个 clients 一起写入时, 速率降到了 4.8MB/s, 主要原因是网络拥塞和抖动. 实际使用中, 我们的应用程序倾向于并发生成若干前述文件而不是一个. 换句话说, N 个 clients 同时向 M 个共享文件追加, N 和 M 都是几千级别的. 因此前面提到的网络拥塞在实际中不是啥大问题, 因为 client 写入一个 chunkserver 时, 当一个文件忙的时候, 可以写另一个文件. –End– ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:5:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#512-writes-测试"},{"categories":null,"content":" 5.1 测量用的微基准测试集群由 1 master, 2 master replicas, 16 chunkservers, 16 clients 构成. 这么搭就为了方便测试, 真实的集群有几百个 chunkservers 和几百个 clients 构成. 所有机器都是双核 1.4GHz, 2GB 内存, 两个每分钟 5400 转的 80GB 磁盘, 一个 100Mbps 的全双工以太网. master 和 chunkserver 共 19 台机器都连接到同一个 HP 2524 交换机上, 16 个 clients 连接到另一个交换机上, 两个交换机通过 1Gbps 链路连接. 5.1.1 reads 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.a 显示 N 个 clients 的读取速率聚合以及它的理论上限. 当两个交换机之间的 1Gbps 跑满的时候达到理论上限 125MB/s; 当每个 client 的 100Mbps 跑满时, 单个 client 机器达到理论上限 12.5MB/s. 当仅有一个 client 读取时, 观察到的读取速率为 10MB/s, 大约是每个 client 上限的 80%(即 10/12.5). 当 16 个 clients 一起读取时, 观察到的聚合读取速率为 94MB/s, 大约为理论上限的 75%(即 94/125); 或者 每个 client 大约 6MB/s. 从 80% 降到 75% 的原因是: 当读取客户端变多, 多个客户端同时从同一个 chunkserver 读取的概率也变大, 从而导致 chunkserver 的网卡带宽被多个客户端争用的更厉害了. 5.1.2 writes 测试N 个 clients 同时写多个 N 个不同的文件. 每个 client 写 1GB 数据到一个新文件, 每次写入 1MB. 聚合写入速率和理论上限如 3.b 所示. 理论上限为 67MB/s, 因为我们需要将每个字节写到 16 个 chunkservers 中的 3 个里, 每个 chunkserver 入口带宽上限为 12.5MB/s. 如果单个 client 写入, 则观察到的写入速率为 6.3MB/s, 大约是上限的一半. 罪魁祸首是网络协议栈, 因为它与我们把数据 push 给 chunk 副本的管道化方案不太搭配. 数据从一个副本传播给另一个副本延迟降低了整体的写入速率. 如果是 16 个 clients 一起写入, 观察到的聚合写入速率为 35MB/s(每个 client 平均 2.2MB/s), 大约是理论上限的一半(即 35/67). 就像读取测试中, 随着执行写操作的 clients 增多, 则同一个 chunkserver 被多个 clients 并发写的几率增大, 此 chunkserver 的入口带宽被更多 clients 争用, 而且 16 个 clients 的写要比读竞争更激烈, 因为一个字节要写到三个 chunkserver. 5.1.3 record appends 测试N 个 clients 同时从系统读写. 每个 client 从一个 320GB 的文件集合中读取一个随机选择的 4MB 区域. 全部 chunkservers 共 32GB 内存, 所以我们期待最多可以有 10% 的几率命中 linux buffer cache. Figure 3.c 显示了记录追加操作的性能. N 个 clients 同时向同一个文件追加. 性能被存储最后一个 chunk 的 chunkservers 的网络带宽限制住了, 与 clients 个数无关. 当只有一个 client 写入时, 速率为 6.0MB/s, 当 16 个 clients 一起写入时, 速率降到了 4.8MB/s, 主要原因是网络拥塞和抖动. 实际使用中, 我们的应用程序倾向于并发生成若干前述文件而不是一个. 换句话说, N 个 clients 同时向 M 个共享文件追加, N 和 M 都是几千级别的. 因此前面提到的网络拥塞在实际中不是啥大问题, 因为 client 写入一个 chunkserver 时, 当一个文件忙的时候, 可以写另一个文件. –End– ","date":"2020-12-11","objectID":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/:5:1","series":null,"tags":["gfs","distributed system","paper"],"title":"GFS: 一个高可用可扩展的分布式文件系统","uri":"/gfs-%E4%B8%80%E4%B8%AA%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/#513-record-appends-测试"},{"categories":null,"content":"本文基于内部分享 \u003c“抄\"能力养成系列 – Gorilla 的设计和实现\u003e 整理. Gorilla 是 Facebook 于 2015 年开放的一个快速, 可扩展的, 内存式时序数据库. 它的一些设计理念影响了后来的 Prometheus. 本文就其设计和实现进行深入分析希望能为各位后续在系统研发中提供灵感. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:0:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#"},{"categories":null,"content":" 1 Gorilla 诞生背景 针对一个大型系统, 监控和分析上千万个打点数据(measurements)是一件很有挑战的事情. 解决这个事情, 一个有效手段就是将这些打点数据存储到时序数据库(TSDB) 中. 而设计一个 TSDB 的关键挑战是如何让效率(efficiency)/扩展性(scalability)/可靠性(reliability)三者达成一个有效平衡. Facebook 的 Gorilla 就是一个达成这种平衡的 TSDB. Gorilla 的设计基于这样一个洞察: 监控系统的用户不会太关注一个个单独的数据点, 而是关注聚合分析; 对于分析一个现有的问题, 最近的数据点比老的数据更有价值. Gorilla 为了读写的高可用而优化, 即使以丢失部分写操作为代价. 为了提升查询效率, fb 激进地采用压缩技术, 如 delta-of-delta 和浮点数 XOR, 来压缩存储大小, 以实现将 Gorilla 数据保存到内存中. 最终效果是将查询时延减少 73X, 同时提升查询吞吐 14X, 两项均为和传统的基于 HBase 的时序数据库比较. 这个结果解锁了很多监控和调试功能, 比如可以在 Gorilla 上执行时序关联搜索. Gorilla 也能很优雅地处理从单点到整个集群故障. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:1:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#1-gorilla-诞生背景"},{"categories":null,"content":" 2 Gorilla 简介Figure 1 是 Facebook 内部的监控系统, 名叫 ODS(Operational Data Store, fb 内部广泛使用的一个老的监控系统), 其中 Gorilla 作为一个 write-through(即 cache 和 back store 同时修改保证多用户一致性) cache. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#2-gorilla-简介"},{"categories":null,"content":" 2.1 需要满足的几个限制 写密集. 每秒可以写入上千万数据点, 一个查询可以毫秒级响应. 读比写少几个数量级, 主要是一些自动化监控系统或者用户查询. 快速识别系统的重大状态变迁. 出现任何问题时都会导致系统状态发生变化, Gorilla 支持针对很小的窗口(几十秒)进行聚合, 以快速识别重大状态变化并触发自动化修复. 高可用. 即使多个数据中心通信断开, 每个 DC(位于不同 region 的 DataCenter, 下同) 都能本地实例读写. 容错. 写入数据被复制到多个 DC, 确保某个 DC 挂掉数据仍在. 以上限制 Gorilla 均满足, 而且可以做到绝大多数查询在几十个毫秒内返回. 另外, 统计发现, 针对 ODS 的至少 85% 的查询涉及过去 26 个小时的数据. 这就暗示我们如果将之前基于磁盘的存储改为内存式将会更好地服务用户. 再进一步, 我们将这个内存式数据库作为持久性磁盘存储系统的 cache, 我们就既可以获得高速插入速率, 同时还能获得数据持久性. 2015 年春天, fb 的监控系统就生成了超过二十亿个 counter 类型的时间序列, 每秒产生大约 1200 万个数据点, 每天产生 1 万亿个数据点. 每个数据点 16 字节, 那就占用 16TB 内存, 这太多了. 但是通过采用基于 XOR 的浮点数压缩技术, 平均每个数据点 1.37 字节, 大小减少 12x. 为了满足可靠性, 我们在不同 region 的 DC 都部署了 Gorilla 实例, 每个数据点都会写入每个 DC 的实例, 但是这多个副本并不保证一致性. 查询请求会被路由到最近的 DC. 以上基于一个观察, 即独立的数据点丢失不会对数据聚合结果产生大的影响, 除非不同 Gorilla 实例数据差异很大. 目前 Gorilla 在 fb 用于生产环境, 而且和其它系统如 hive/scuba 一起检测和分析问题. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#21-需要满足的几个限制"},{"categories":null,"content":" 2.2 现状(注: 2015)fb 内部有几百个系统, 分布在多个 DC, 这些系统的健康状况和性能是需要监测的, ODS 就是 fb 监控系统的重要组成部分. ODS 由 TSDB, 查询服务以及检测和告警服务构成; 其中 TSDB 是构建在 HBase 上的. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#22-现状注-2015"},{"categories":null,"content":" 2.3 已有监控系统的查询性能问题早在 2013 年, fb 的监控团队就意识到基于 hbase 的 TSDB 无法扩展应对将来的查询负载. 其中 90 分位数长达几秒, 这对于依赖 tsdb 的自动化监控系统非常不友好. 一个针对稀疏数据的大点的查询甚至会超时, 因为 hbase 是针对写操作优化的. 虽然 hbase 表现不行, 但是也不能整个替换掉, 因为 ODS 的 hbase 存了 2PB 的数据. 而且 fb 的数据仓库, hive 也不能胜任, 因为它的查询比 ODS 还要慢几个数量级, 而查询时延和效率是我们主要关注的. 接下来能做的就是内存式 cache 了. (ODS 其实本来就有 write-through cache, 只不过是用于制图系统.) 开始也考虑过基于 Memcached 来做, 但是因为针对已有时间序列追加新数据需要一个读写周期, 会导致 memcached 服务器产生极其高的流量, 所以否掉了这个方案. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:2:3","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#23-已有监控系统的查询性能问题"},{"categories":null,"content":" 3 Gorilla 的设计目标 通过唯一 key 可以识别 20 亿时间序列. 每分钟可追加 7 亿数据点(时间戳+具体值). 可保存 26 个小时的数据. 可支持每秒 4000 查询峰值. 一个毫秒内完成读操作. 支持 15 秒粒度的时间序列(即一分钟四个数据点). 两个内存式副本(不能部署在同一个地方, 以应对故障). 即使挂掉一个实例, 仍然可以正常支持查询. 可以快速扫描整个内存数据的能力. 支持每年至少 2x 的负载增长. Gorilla 聚焦如何实时收集和存储海量数据. Gorilla 可以作为其它 tsdb 的 write-through cache. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:3:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#3-gorilla-的设计目标"},{"categories":null,"content":" 4 Gorilla 与现有的 TSDB 比较(注:2015年)","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:4:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#4-gorilla-与现有的-tsdb-比较注2015年"},{"categories":null,"content":" 4.1 OpenTSDB 它基于 HBase, 无降采样功能. 数据模型丰富, 针对一个时序有一组 key-value 对即 tags, Gorilla 仅有一个字符串 key 而且依赖更高级的工具抽取和识别时序的元数据. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:4:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#41-opentsdb"},{"categories":null,"content":" 4.2 Whiper(Graphite) 数据存储在本地磁盘, 所以查询速度不够快. 数据格式为 whisper, 即 RRD 风格. 该文件格式要求时间序列数据都带固定间隔的时间戳. 如果时间戳间隔固定, Gorilla 表现更好(压缩率更高), 但是 Gorilla 也可以处理随机变化的时间间隔. 每个时间序列保存到一个单独的文件中, 一段时间后新采集的数据会覆盖老的数据, 毕竟是 Round-Robin Database. Gorilla 也采用类似的方式, 仅在内存保存最近的数据. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:4:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#42-whipergraphite"},{"categories":null,"content":" 4.3 InfluxDB 比 OpenTSDB 数据类型更加丰富, 每个数据点都有一组丰富的标签, 这也导致它更占磁盘. 无需 hbase/hadoop 就能做到水平扩展. 数据保存到本地磁盘, 所以查询不如内存式的快. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:4:3","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#43-influxdb"},{"categories":null,"content":" 5 Gorilla 架构","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:5:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#5-gorilla-架构"},{"categories":null,"content":" 5.1 概况 内存式, 作为后端 hbase 的 write-through cache. 每个数据点是一个三元组\u003c字符串形式的 key, 64 比特的时间戳, 双精度浮点数类型的测量值\u003e. 所采用的压缩算法, 可以将 16 字节的数据点压缩到平均 1.37 字节, 减少 12x. 内存数据结构既支持快速的全量数据扫描, 也支持高效地查询单个时序. key 作为时序唯一标识, 写入时也是基于此在众多 Gorilla 实例之间做 sharding. 所以仅仅通过增加新机器就能实现 Gorilla 集群的水平扩展, 新写入的数据会被 sharding 到新机器上. 之所以这么简单, 就是因为 Gorilla 整体是一个 share-nothing 架构, 专注于水平扩展能力.(疑问, 一致性哈希咋做的? 论文没讲, 但提到了 ShardManager, 应该是它负责的). Gorilla 可以处理单点故障, 网络分区甚至整个 DC 挂掉, 方法就是每个时间序列都被写入到两个不同地理 regions 中的实例中. 一旦检测到宕机, 针对目标序列的全部查询会被自动切换到另一个 region 的实例, 这个过程用户感觉不出来. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:5:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#51-概况"},{"categories":null,"content":" 5.2 时间序列压缩Gorilla 对压缩算法的诉求: 支持针对浮点数进行压缩而非整数 支持在数据流上进行压缩, 而不是针对静态完整的数据集 无损, 保持整个时序精度不变 Gorilla 的压缩算法受科学计算中的浮点数压缩方案启发, 该方案利用当前值与前一个值的 XOR 比较来生成 delta 编码. Gorilla 不支持跨时间序列进行压缩, 而是在每个时序内进行压缩. 时间戳和具体数值各自独立进行压缩, 压缩时都用到了前一个数据点的信息. Figure 2 是 Gorilla 的压缩过程图示. 2.a 显示了一个数据流, 每个点由两个 64 比特的数构成, 一个是时间戳一个是具体测量值. Gorilla 基于时间将数据流分成 block, 划分时会按照每两小时进行对齐. 可以看到每一个 block 以一个简单的 header 开始, 它含有所在 block 对齐的时间戳, 例子中是凌晨两点. 2.b 是基于 delta-of-delta 压缩后的数据, 可以看到 delta-of-delta 等于 60-62=-2, 用 ‘10’ 两个比特表示(后面详述表示规则), 接下来 7 个比特存储测量值, 一共用了 9 比特. 2.c 显示的是使用 XOR 算法压缩的浮点数, 可以看到当前值与前一个值 XOR 后的结果, 它有 11 个前导零, 整个结果仅有一个有效位 1, 这可以编码为 2 比特的 ‘11’. 存储当前浮点数测量值 24 共用了 14 比特. 具体编码细节后面详述. 5.2.1 时间戳压缩 我们注意到, 在 ODS 中, 绝大多数数据点都是以固定时间间隔到达的. 后面可以看到这是一个非常重要的洞察. 我们不会将时间戳完整存储, 而是采用 delta-of-delta. 假设一个 delta 序列为 60, 60, 59, 61, 那么 delta-of-delta 就是 0, -1, 2. 然后我们按照下面的算法使用变长编码来编码这些 delta-of-delta: block header 存储起始时间戳, 记为 $t_{-1}$, 它以两小时时间窗口来对齐. 当前 block 中第一个数据点的时间戳记为 $t_0$, 真正存储的是它减去 $t_{-1}$ 得到的 delta(只有第一个时间戳对应的存储值为 delta 而非 delta-of-delta, 因为前面就一个起始时间戳), 以 14 比特存储. 接下来针对每个时间戳 $t_n$: 计算其对应的 delta-of-delta: $D=(t_n-t_{n-1})-(t_{n-1}-t_{n-2})$ 如果 $D$ 为 0, 则用一个比特存储 ‘0’ 如果 $D$ 在$[-63, 64]$ 之间, 用 2 个比特存储 ‘10’ , 然后接下来 7 比特存储 $D$ 的具体值. 如果 $D$ 在$[-255, 256]$ 之间, 用 3 个比特存储 ‘110’, 然后接下来 9 比特存储 $D$ 的具体值. 如果 $D$ 在$[-2047, 2048]$ 之间, 用 4 个比特存储 ‘1110’, 然后接下来 12 比特存储 $D$ 的具体值. 其它情况, 用 4 个比特存储 ‘1111’, 然后接下来 32 比特存储 $D$ 的具体值. 上面的取值范围都是通过从生产系统统计出来的, 这几个范围可以帮助达到最大压缩率. 从 Figure 3 观察压缩效果: 可以发现大约 96% 的时间戳可以压缩到 1 个比特. 5.2.2 测量值压缩 Gorilla 只允许存储双精度浮点数的测量值. 根据统计, 时间序列中相邻的数据点大多数时候相差不大, 如果相邻的数据点对应的测量值的符号部分/指数部分/小数部分前半截都差不多一样, 那么我们就可以计算当前值和前一个值的 XOR 来存储而不是采用 delta 编码方案. 当前值和前一个值进行 XOR 后按照下面变长编码方案来存储: 第一个值不压缩 如果 XOR 结果为 0, 则仅存 1 比特的 ‘0’ 如果 XOR 结果非 0, 计算其前导零和后缀零个数, 存储 1 比特 ‘1’, 然后后面跟着下面的 a 或者 b: a. 首先是 1 比特的控制位 ‘0’, 如果当前 XOR 结果的前导零个数和后缀零个数与前一个 XOR 结果的对应部分一样, 仅存储当前 XOR 结果的中间有效位部分(该部分开头和结尾都为 1). 正是因为前述的特性, 可以让我们省去存储前导零长度和后缀零长度的空间, 根据前一个 XOR 结果就能复原当前这个. b. 首先是 1 比特的控制位 ‘1’, 接下来 5 比特为前导零个数, 接下来 6 比特为当前 XOR 结果的有效位长度(该部分开头和结尾都为 1), 接下来为 XOR 结果的有效位部分. 从上面方案来看, 测量值压缩不但用到了当前测量值和前一个测量值, 也用到了当前 XOR 结果和前一个 XOR 的结果, 因为计算出来的 XOR 序列, 相邻两个经常有相似的前导零和后缀零. 这可以通过 Figure 4 来观察. 针对测量值的编码方案, 从 Figure 5 统计可以看到: 大约 59.06% 数据点被压缩到仅剩 1 比特 大约 28.3% 以控制位 ‘10’ 开头, 平均长度为 26.6 比特 大约 12.64% 以控制位 ‘11’ 开头, 平均长度为 39.6 比特, 相比控制位 ‘10’ 方案多出来的 13 比特用于存储前导零长度和有效位部分长度了. 测量值方案潜在的问题: 就是时序对应的数据流按多大的窗口划分 block 才能更好的利用这个压缩方案. 由于中间每个值压缩都与自己的前驱密切相关, block 肯定越大越好, 如果不划分 block 是最佳的. 但是如果 block 太大, 但是用户只想查询一个很小的时间窗口的数据, 就会涉及非常大的计算量来解压缩. 这就牵扯到 trade-off. 从 Figure 6 可以看到 120 分钟也就是两个小时的 block 大小可以达到一个比较理想的压缩率, 平均每个数据点占 1.37 字节. 窗口再大, 压缩率变化不大了. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:5:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#52-时间序列压缩"},{"categories":null,"content":" 5.2 时间序列压缩Gorilla 对压缩算法的诉求: 支持针对浮点数进行压缩而非整数 支持在数据流上进行压缩, 而不是针对静态完整的数据集 无损, 保持整个时序精度不变 Gorilla 的压缩算法受科学计算中的浮点数压缩方案启发, 该方案利用当前值与前一个值的 XOR 比较来生成 delta 编码. Gorilla 不支持跨时间序列进行压缩, 而是在每个时序内进行压缩. 时间戳和具体数值各自独立进行压缩, 压缩时都用到了前一个数据点的信息. Figure 2 是 Gorilla 的压缩过程图示. 2.a 显示了一个数据流, 每个点由两个 64 比特的数构成, 一个是时间戳一个是具体测量值. Gorilla 基于时间将数据流分成 block, 划分时会按照每两小时进行对齐. 可以看到每一个 block 以一个简单的 header 开始, 它含有所在 block 对齐的时间戳, 例子中是凌晨两点. 2.b 是基于 delta-of-delta 压缩后的数据, 可以看到 delta-of-delta 等于 60-62=-2, 用 ‘10’ 两个比特表示(后面详述表示规则), 接下来 7 个比特存储测量值, 一共用了 9 比特. 2.c 显示的是使用 XOR 算法压缩的浮点数, 可以看到当前值与前一个值 XOR 后的结果, 它有 11 个前导零, 整个结果仅有一个有效位 1, 这可以编码为 2 比特的 ‘11’. 存储当前浮点数测量值 24 共用了 14 比特. 具体编码细节后面详述. 5.2.1 时间戳压缩 我们注意到, 在 ODS 中, 绝大多数数据点都是以固定时间间隔到达的. 后面可以看到这是一个非常重要的洞察. 我们不会将时间戳完整存储, 而是采用 delta-of-delta. 假设一个 delta 序列为 60, 60, 59, 61, 那么 delta-of-delta 就是 0, -1, 2. 然后我们按照下面的算法使用变长编码来编码这些 delta-of-delta: block header 存储起始时间戳, 记为 $t_{-1}$, 它以两小时时间窗口来对齐. 当前 block 中第一个数据点的时间戳记为 $t_0$, 真正存储的是它减去 $t_{-1}$ 得到的 delta(只有第一个时间戳对应的存储值为 delta 而非 delta-of-delta, 因为前面就一个起始时间戳), 以 14 比特存储. 接下来针对每个时间戳 $t_n$: 计算其对应的 delta-of-delta: $D=(t_n-t_{n-1})-(t_{n-1}-t_{n-2})$ 如果 $D$ 为 0, 则用一个比特存储 ‘0’ 如果 $D$ 在$[-63, 64]$ 之间, 用 2 个比特存储 ‘10’ , 然后接下来 7 比特存储 $D$ 的具体值. 如果 $D$ 在$[-255, 256]$ 之间, 用 3 个比特存储 ‘110’, 然后接下来 9 比特存储 $D$ 的具体值. 如果 $D$ 在$[-2047, 2048]$ 之间, 用 4 个比特存储 ‘1110’, 然后接下来 12 比特存储 $D$ 的具体值. 其它情况, 用 4 个比特存储 ‘1111’, 然后接下来 32 比特存储 $D$ 的具体值. 上面的取值范围都是通过从生产系统统计出来的, 这几个范围可以帮助达到最大压缩率. 从 Figure 3 观察压缩效果: 可以发现大约 96% 的时间戳可以压缩到 1 个比特. 5.2.2 测量值压缩 Gorilla 只允许存储双精度浮点数的测量值. 根据统计, 时间序列中相邻的数据点大多数时候相差不大, 如果相邻的数据点对应的测量值的符号部分/指数部分/小数部分前半截都差不多一样, 那么我们就可以计算当前值和前一个值的 XOR 来存储而不是采用 delta 编码方案. 当前值和前一个值进行 XOR 后按照下面变长编码方案来存储: 第一个值不压缩 如果 XOR 结果为 0, 则仅存 1 比特的 ‘0’ 如果 XOR 结果非 0, 计算其前导零和后缀零个数, 存储 1 比特 ‘1’, 然后后面跟着下面的 a 或者 b: a. 首先是 1 比特的控制位 ‘0’, 如果当前 XOR 结果的前导零个数和后缀零个数与前一个 XOR 结果的对应部分一样, 仅存储当前 XOR 结果的中间有效位部分(该部分开头和结尾都为 1). 正是因为前述的特性, 可以让我们省去存储前导零长度和后缀零长度的空间, 根据前一个 XOR 结果就能复原当前这个. b. 首先是 1 比特的控制位 ‘1’, 接下来 5 比特为前导零个数, 接下来 6 比特为当前 XOR 结果的有效位长度(该部分开头和结尾都为 1), 接下来为 XOR 结果的有效位部分. 从上面方案来看, 测量值压缩不但用到了当前测量值和前一个测量值, 也用到了当前 XOR 结果和前一个 XOR 的结果, 因为计算出来的 XOR 序列, 相邻两个经常有相似的前导零和后缀零. 这可以通过 Figure 4 来观察. 针对测量值的编码方案, 从 Figure 5 统计可以看到: 大约 59.06% 数据点被压缩到仅剩 1 比特 大约 28.3% 以控制位 ‘10’ 开头, 平均长度为 26.6 比特 大约 12.64% 以控制位 ‘11’ 开头, 平均长度为 39.6 比特, 相比控制位 ‘10’ 方案多出来的 13 比特用于存储前导零长度和有效位部分长度了. 测量值方案潜在的问题: 就是时序对应的数据流按多大的窗口划分 block 才能更好的利用这个压缩方案. 由于中间每个值压缩都与自己的前驱密切相关, block 肯定越大越好, 如果不划分 block 是最佳的. 但是如果 block 太大, 但是用户只想查询一个很小的时间窗口的数据, 就会涉及非常大的计算量来解压缩. 这就牵扯到 trade-off. 从 Figure 6 可以看到 120 分钟也就是两个小时的 block 大小可以达到一个比较理想的压缩率, 平均每个数据点占 1.37 字节. 窗口再大, 压缩率变化不大了. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:5:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#521-时间戳压缩"},{"categories":null,"content":" 5.2 时间序列压缩Gorilla 对压缩算法的诉求: 支持针对浮点数进行压缩而非整数 支持在数据流上进行压缩, 而不是针对静态完整的数据集 无损, 保持整个时序精度不变 Gorilla 的压缩算法受科学计算中的浮点数压缩方案启发, 该方案利用当前值与前一个值的 XOR 比较来生成 delta 编码. Gorilla 不支持跨时间序列进行压缩, 而是在每个时序内进行压缩. 时间戳和具体数值各自独立进行压缩, 压缩时都用到了前一个数据点的信息. Figure 2 是 Gorilla 的压缩过程图示. 2.a 显示了一个数据流, 每个点由两个 64 比特的数构成, 一个是时间戳一个是具体测量值. Gorilla 基于时间将数据流分成 block, 划分时会按照每两小时进行对齐. 可以看到每一个 block 以一个简单的 header 开始, 它含有所在 block 对齐的时间戳, 例子中是凌晨两点. 2.b 是基于 delta-of-delta 压缩后的数据, 可以看到 delta-of-delta 等于 60-62=-2, 用 ‘10’ 两个比特表示(后面详述表示规则), 接下来 7 个比特存储测量值, 一共用了 9 比特. 2.c 显示的是使用 XOR 算法压缩的浮点数, 可以看到当前值与前一个值 XOR 后的结果, 它有 11 个前导零, 整个结果仅有一个有效位 1, 这可以编码为 2 比特的 ‘11’. 存储当前浮点数测量值 24 共用了 14 比特. 具体编码细节后面详述. 5.2.1 时间戳压缩 我们注意到, 在 ODS 中, 绝大多数数据点都是以固定时间间隔到达的. 后面可以看到这是一个非常重要的洞察. 我们不会将时间戳完整存储, 而是采用 delta-of-delta. 假设一个 delta 序列为 60, 60, 59, 61, 那么 delta-of-delta 就是 0, -1, 2. 然后我们按照下面的算法使用变长编码来编码这些 delta-of-delta: block header 存储起始时间戳, 记为 $t_{-1}$, 它以两小时时间窗口来对齐. 当前 block 中第一个数据点的时间戳记为 $t_0$, 真正存储的是它减去 $t_{-1}$ 得到的 delta(只有第一个时间戳对应的存储值为 delta 而非 delta-of-delta, 因为前面就一个起始时间戳), 以 14 比特存储. 接下来针对每个时间戳 $t_n$: 计算其对应的 delta-of-delta: $D=(t_n-t_{n-1})-(t_{n-1}-t_{n-2})$ 如果 $D$ 为 0, 则用一个比特存储 ‘0’ 如果 $D$ 在$[-63, 64]$ 之间, 用 2 个比特存储 ‘10’ , 然后接下来 7 比特存储 $D$ 的具体值. 如果 $D$ 在$[-255, 256]$ 之间, 用 3 个比特存储 ‘110’, 然后接下来 9 比特存储 $D$ 的具体值. 如果 $D$ 在$[-2047, 2048]$ 之间, 用 4 个比特存储 ‘1110’, 然后接下来 12 比特存储 $D$ 的具体值. 其它情况, 用 4 个比特存储 ‘1111’, 然后接下来 32 比特存储 $D$ 的具体值. 上面的取值范围都是通过从生产系统统计出来的, 这几个范围可以帮助达到最大压缩率. 从 Figure 3 观察压缩效果: 可以发现大约 96% 的时间戳可以压缩到 1 个比特. 5.2.2 测量值压缩 Gorilla 只允许存储双精度浮点数的测量值. 根据统计, 时间序列中相邻的数据点大多数时候相差不大, 如果相邻的数据点对应的测量值的符号部分/指数部分/小数部分前半截都差不多一样, 那么我们就可以计算当前值和前一个值的 XOR 来存储而不是采用 delta 编码方案. 当前值和前一个值进行 XOR 后按照下面变长编码方案来存储: 第一个值不压缩 如果 XOR 结果为 0, 则仅存 1 比特的 ‘0’ 如果 XOR 结果非 0, 计算其前导零和后缀零个数, 存储 1 比特 ‘1’, 然后后面跟着下面的 a 或者 b: a. 首先是 1 比特的控制位 ‘0’, 如果当前 XOR 结果的前导零个数和后缀零个数与前一个 XOR 结果的对应部分一样, 仅存储当前 XOR 结果的中间有效位部分(该部分开头和结尾都为 1). 正是因为前述的特性, 可以让我们省去存储前导零长度和后缀零长度的空间, 根据前一个 XOR 结果就能复原当前这个. b. 首先是 1 比特的控制位 ‘1’, 接下来 5 比特为前导零个数, 接下来 6 比特为当前 XOR 结果的有效位长度(该部分开头和结尾都为 1), 接下来为 XOR 结果的有效位部分. 从上面方案来看, 测量值压缩不但用到了当前测量值和前一个测量值, 也用到了当前 XOR 结果和前一个 XOR 的结果, 因为计算出来的 XOR 序列, 相邻两个经常有相似的前导零和后缀零. 这可以通过 Figure 4 来观察. 针对测量值的编码方案, 从 Figure 5 统计可以看到: 大约 59.06% 数据点被压缩到仅剩 1 比特 大约 28.3% 以控制位 ‘10’ 开头, 平均长度为 26.6 比特 大约 12.64% 以控制位 ‘11’ 开头, 平均长度为 39.6 比特, 相比控制位 ‘10’ 方案多出来的 13 比特用于存储前导零长度和有效位部分长度了. 测量值方案潜在的问题: 就是时序对应的数据流按多大的窗口划分 block 才能更好的利用这个压缩方案. 由于中间每个值压缩都与自己的前驱密切相关, block 肯定越大越好, 如果不划分 block 是最佳的. 但是如果 block 太大, 但是用户只想查询一个很小的时间窗口的数据, 就会涉及非常大的计算量来解压缩. 这就牵扯到 trade-off. 从 Figure 6 可以看到 120 分钟也就是两个小时的 block 大小可以达到一个比较理想的压缩率, 平均每个数据点占 1.37 字节. 窗口再大, 压缩率变化不大了. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:5:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#522-测量值压缩"},{"categories":null,"content":" 6 Gorilla 的内存数据结构Figure 7 是针对 Gorilla 的内存数据结构相互关系和各个部分的图示. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:6:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#6-gorilla-的内存数据结构"},{"categories":null,"content":" 6.1 TSmapGorilla 实现涉及的核心数据结构是 Timeseries Map(TSmap), 如 Figure 7 中间部分所示. 6.1.1 TSmap 由两部分构成 一个 C++ 标准库的 vector, 每个元素是一个 shared-pointer, 每个 shared-pointer 指向一个时间序列 TS. 该结构主要用于针对全量数据进行高效地分页扫描. 一个从时间序列名称(大小写不敏感但是保留大小写)到指向时间序列 TS 的 shared-pointer 的无序 map. 该结构主要是用于在常数时间内根据名称查询某个时间序列. 6.1.2 TSmap 的并发与性能关于 TSmap 的并发控制: 有一个专用的 Read-Write 锁保护 map 和 vector 的访问 每个 TS 有一个 1 字节的 spin lock, 由于每个时间序列写吞吐低(秒级或分钟级写一次), 所以针对 spin lock 的争用并不激烈. C++ shared-pointer 使得扫描操作可以在微秒时间内拷贝 vector, 这避免了长时间锁住临界区(critical section)从而影响写入数据流. 当删除一个时间序列时, 其对应的在 vector 的元素会被标记为墓碑(相关内存不会返回系统而是等待重用), 对应的索引被放到池子里待新的时间序列重用之. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:6:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#61-tsmap"},{"categories":null,"content":" 6.1 TSmapGorilla 实现涉及的核心数据结构是 Timeseries Map(TSmap), 如 Figure 7 中间部分所示. 6.1.1 TSmap 由两部分构成 一个 C++ 标准库的 vector, 每个元素是一个 shared-pointer, 每个 shared-pointer 指向一个时间序列 TS. 该结构主要用于针对全量数据进行高效地分页扫描. 一个从时间序列名称(大小写不敏感但是保留大小写)到指向时间序列 TS 的 shared-pointer 的无序 map. 该结构主要是用于在常数时间内根据名称查询某个时间序列. 6.1.2 TSmap 的并发与性能关于 TSmap 的并发控制: 有一个专用的 Read-Write 锁保护 map 和 vector 的访问 每个 TS 有一个 1 字节的 spin lock, 由于每个时间序列写吞吐低(秒级或分钟级写一次), 所以针对 spin lock 的争用并不激烈. C++ shared-pointer 使得扫描操作可以在微秒时间内拷贝 vector, 这避免了长时间锁住临界区(critical section)从而影响写入数据流. 当删除一个时间序列时, 其对应的在 vector 的元素会被标记为墓碑(相关内存不会返回系统而是等待重用), 对应的索引被放到池子里待新的时间序列重用之. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:6:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#611-tsmap-由两部分构成"},{"categories":null,"content":" 6.1 TSmapGorilla 实现涉及的核心数据结构是 Timeseries Map(TSmap), 如 Figure 7 中间部分所示. 6.1.1 TSmap 由两部分构成 一个 C++ 标准库的 vector, 每个元素是一个 shared-pointer, 每个 shared-pointer 指向一个时间序列 TS. 该结构主要用于针对全量数据进行高效地分页扫描. 一个从时间序列名称(大小写不敏感但是保留大小写)到指向时间序列 TS 的 shared-pointer 的无序 map. 该结构主要是用于在常数时间内根据名称查询某个时间序列. 6.1.2 TSmap 的并发与性能关于 TSmap 的并发控制: 有一个专用的 Read-Write 锁保护 map 和 vector 的访问 每个 TS 有一个 1 字节的 spin lock, 由于每个时间序列写吞吐低(秒级或分钟级写一次), 所以针对 spin lock 的争用并不激烈. C++ shared-pointer 使得扫描操作可以在微秒时间内拷贝 vector, 这避免了长时间锁住临界区(critical section)从而影响写入数据流. 当删除一个时间序列时, 其对应的在 vector 的元素会被标记为墓碑(相关内存不会返回系统而是等待重用), 对应的索引被放到池子里待新的时间序列重用之. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:6:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#612-tsmap-的并发与性能"},{"categories":null,"content":" 6.2 ShardMapGorilla 为了做到分布式存储采用了分片机制, 如 Figure 7 左边部分所示. ShardMap 名字里有 map, 但它实际是一个 vector, 每个元素是一个指向 TSmap 的 unique_ptr. 那它为什么叫 map 呢? 因为每个时间序列根据其名称先做 hash(具体算法同 TSmap 的 map 结构)后做 shard(即其被分到哪个 TSmap). shards 即 TSmap 个数固定, 只有几千个. 就像 TSmap, 针对 ShardMap 的访问也由一个 read-write spin lock 控制. 因为 ShardMap 这一层做了 shard 了, 所以 TSmap 内部的 map 就比较小了, C++ 标准库里的 unordered-map 性能足够了. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:6:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#62-shardmap"},{"categories":null,"content":" 6.3 TS说完了最外层的 ShardMap, 也说完了中间的 TSmap, 下面说说最里面的 TS 构成. TS (即 time series), 为 Gorilla 存储每个时间序列的数据结构, 如 Figure 7 右边部分所示. 每个时间序列的数据结构由一系列 closed blocks(每一个两个小时大小)和一个 open block (保存最近两个小时的数据). open block 仅追加压缩过的(压缩算法前面讲过了) \u003c时间戳, 测量值\u003e 数据点. 超过 2 小时, open block 就会转成 closed block, 后者不可更改直至从内存被删除. 一旦 closed, block 就会被拷贝到另外的内存(这些内存从大块的 slabs 分配)从而减少内存碎片. open block 经常因大小变化而重分配内存, 前述的拷贝过程可以减少 Gorilla 整体的内存碎片. 当外部查询时, Gorilla 直接拷贝包含目标数据的 blocks 给客户端, 客户端自己去解压缩. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:6:3","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#63-ts"},{"categories":null,"content":" 7 Gorilla 的磁盘存储结构Gorilla 其中一个设计目标就是高可用, 不能因为单个 host 挂掉而导致集群不可用. Gorilla 通过 GlusterFS 实现数据持久性, 该分布式文件系统支持三副本, 并且兼容 POSIX. 当然你也可以用 Hadoop 或者其它分布式文件系统. 注意这里是单个 host 上数据的持久性, 它们对 host 内存数据对应. 这些数据不用于响应客户查询, 而是用于 host 内存数据持久化以便在 host 挂掉后被重新加载到内存恢复对应数据结构. Gorilla 每个 host 持有多个 shards, 每个 shard 一个目录. 每个目录中保存着如下四类信息: key list 文件. key list 文件就是一个简单的 map, 即从时间序列的字符串类型的 key 到一个整数 ID 的映射. 这个整数 ID 就是对应时间序列在 Tsmap-\u003evector 的索引. 新的 keys 会被追加到 key list 中, Gorilla 会周期性地扫描每个 shard 对应的全部 keys 以重写该文件. 一组 append-only logs. 每个 shard 一个 append-only log. 类似 WAL(write-ahead log), 但因为它不保障 ACID 所以不是真正的 WAL. 新到达的数据点都是先写到这个文件. 由于每个 shard 一个 append-only log, 所以哈希到同一个 shard 的多个时间序列的数据点会交织在一起. 数据编码同内存格式, 但多了一个 32 比特的整数 ID 标识数据点属于哪个时间序列. 每当攒够 64KB 数据刷盘一次, 大约是一两秒的数据, 这可能因为宕机丢失一小部分数据, 不过为了获取更高的磁盘写入效率, 这个 trade-off 就不得不做了. 一组完整的 block 文件. 每隔两小时 Gorilla 会拷贝内存中压缩过的 block 数据到磁盘, 它比 append-only log 文件小多了. 一个 block 文件就是两个小时的数据, 它包含两段: 一组连续的 64kB slabs(就是它们在内存中的样子), 一个 \u003ctime series ID, data block pointer\u003e 列表. checkpoint 文件. 每当一个完整的 block 文件生成, Gorilla 就会创建一个新的 checkpoint 文件同时删除对应的 append-only log. 这个 checkpoint 文件用于标记什么时候一个完整的 block 文件被刷入了磁盘. 如果因为进程崩溃导致 block 文件写入失败, 新进程启动后会发现对应的 checkpoint 文件不存在, 新进程就会认为这个 block 文件有问题, 转而仅从 append-only log 读取数据. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:7:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#7-gorilla-的磁盘存储结构"},{"categories":null,"content":" 8 Gorilla 如何实现高可用Gorilla 容忍两种故障: 单机故障 单集群故障 以上两种都可以通过多副本机制做到快速转移和处理, 也对频繁的版本升级很友好. 多副本机制: 针对同一份数据, Gorilla 会在不同 region 的 DC 里面部署相互独立的集群, 每次写操作同时写到这两个地方, 当然这个双写不保证一致性(还是回到前面讲到的, 时序数据不太注重单个数据点或者一小块数据, 关注的是整体), 一个挂了另一个还能用, 挂了的那个数据恢复到最近 26 个小时大小就继续对外提供服务. 单机挂掉后 Gorilla 处理流程: ShardManager(一个基于 paxos 强一致系统)会将其负责的 shards 重分配给当前集群其它机器. 重分配期间, 写客户端会自己缓冲这些 shards 的数据一分钟. 一般 node 挂掉 30 秒后就启动重分配, 一分钟就可以完成, 如果超过一分钟, 缓冲的数据就会用新的覆盖老的. shards 重分配时, 分到这些 shards 的 nodes 会去 GlusterFS 拉取数据, 一般五分钟内就可以全部恢复. 注意, 前述过程可能会丢失部分数据, 这是可以容忍的. 注意, node 失效期间, 因为部分 shards 不可用, 读响应可能会被标记为 partial, 客户端转而请求另一个 DC 的同样数据, 如果两个 DC 相关均不可用, 则两个 DC 的部分结果和一个表示错误的标识会被返回给客户端. 最后, 如果 Gorilla 都挂了怎么办? 这时就靠 HBase 的 long-term 存储来响应客户请求了, 直到 Gorilla 集群恢复. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:8:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#8-gorilla-如何实现高可用"},{"categories":null,"content":" 9 Gorilla 带来的新分析工具","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:9:0","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#9-gorilla-带来的新分析工具"},{"categories":null,"content":" 9.1 关联分析引擎 Gorilla 提供的关联搜索功能支持用户一次在一百万个时间序列上做交互式地暴力搜索. 关联分析引擎支持针对某个序列和一组其它时间序列做 PPMCC(皮尔逊积矩相关系数), 从而找出形状相似地时间序列, 进而帮助做根因分析(root-cause analysis). 计算 PPMCC 时, 测试时间序列会被广播到多个机器, 这些机器基于目标时序的 key 来确定, 每个机器独立计算相关时序, 然后基于 PPMCC 绝对值排序求出 topN. 最后返回结果. ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:9:1","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#91-关联分析引擎"},{"categories":null,"content":" 9.2 降采样在 Gorilla 上线之前, fb 在 hbase 上允许 map-reduce 任务来计算降采样数据. 有了 Gorilla, 后台进程每两个小时扫一遍全量数据来计算降采样, 不用再去全表扫描 HBase 了.\u000b–End– ","date":"2020-12-05","objectID":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/:9:2","series":null,"tags":["Gorilla","tsdb","distributed system","paper"],"title":"Gorilla: 一个快速, 可扩展的, 内存式时序数据库","uri":"/gorilla-%E4%B8%80%E4%B8%AA%E5%BF%AB%E9%80%9F-%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84-%E5%86%85%E5%AD%98%E5%BC%8F%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/#92-降采样"},{"categories":null,"content":"memtable 可以看作是 log 文件的内存形式, 但是格式不同. 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable(所以可能同时有两个 memtable 存在) 以及磁盘上的各个 level 包含的文件构成了数据全集. memtable 的本质就是一个 SkipList. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#"},{"categories":null,"content":" 1 核心文件和核心类 核心类 所在文件 用途 leveldb::MemTable db/memtable.cc; db/memtable.h 本文主角 leveldb::Arena util/arena.cc; util/arena.h 负责内存管理(仅分配和释放, 无回收) leveldb::SkipList\u003cKey, Comparator\u003e db/skiplist.h 作为 MemTable 类底层存储(基于内存) ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#1-核心文件和核心类"},{"categories":null,"content":" 2 MemTable 核心成员变量 字段 类型 用途 comparator_ struct leveldb::MemTable::KeyComparator 用于 SkipList 比较 key refs_ int 该 memtable 引用计数, memtable 的拷贝构造和赋值构造都是禁用的, 只能通过增加引用计数复用 arena_ class leveldb::Arena 内存池, 给 SkipList 分配 Node 时候使用 table_ typedef leveldb::SkipList\u003cconst char *, leveldb::MemTable::KeyComparator\u003e leveldb::MemTable::Table SkipList, 存储 memtable 里的数据 ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#2-memtable-核心成员变量"},{"categories":null,"content":" 3 MemTable 核心方法 方法 用途 explicit MemTable(const InternalKeyComparator\u0026 comparator) 传入一个自定义或数据库提供的比较器, 生成一个 MemTable 实例. void Ref() 递增引用计数, MemTable 是基于引用计数的, 每个 MemTable 实例初始引用计数为 0, 调用者必须至少调用一次 Ref() 方法. void Unref() 递减引用计数, 计数变为 0 后删除该 MemTable 实例. size_t ApproximateMemoryUsage() 返回当前 memtable 中数据字节数的估计值, 当 memtable 被修改时调用该方法也是安全的, 该方法底层实现直接用的 arena_ 持有内存的字节数. Iterator* NewIterator() 返回一个迭代器, 该迭代器可以用来遍历整个 memtable 的内容. void Add(SequenceNumber seq, ValueType type, const Slice\u0026 key, const Slice\u0026 value) 根据指定的序列号和操作类型将 user_key 转换为 internal_key 然后和 value 一起向 memtable 新增一个数据项. 该数据项是 key 到 value 的映射, 如果操作类型 type==kTypeDeletion 则 value 为空. 最后数据项写入了底层的 SkipList 中. 每个数据项编码格式为 [varint32 类型的 internal_key_size, internal_key, varint32 类型的 value_size, value] internal_key 由 [user_key, tag] 构成. bool Get(const LookupKey\u0026 key, std::string* value, Status* s) 如果 memtable 包含 key 对应的 value, 则将 value 保存在 *value 并返回 true. 如果 memtable 包含 key 对应的 deletion, 则将 NotFound 错误存在 *status, 并返回 true; 其它情况返回 false. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#3-memtable-核心方法"},{"categories":null,"content":" 3.1 MemTable 写方法当需要向 memtable 加入一个 \u003ckey, value\u003e 时, 可以调用 void Add(SequenceNumber seq, ValueType type, const Slice\u0026 key, const Slice\u0026 value) 达成目标. 该方法负责将用户 key 转换为一个 InternalKey, 然后连同 value 一起进行编码, 组成一个形如下述的数据项: +------------------------------+---------+--------+--------+---------------+-----+ | internal key size | seq num |op type |user key| value size |value| | (varint32 type) |(7 bytes)|(1 byte)| |(varint32 type)| | |(user key + seq num + op type)| | | | | | +------------------------------+---------+--------+--------+---------------+-----+ 其中该数据项前四个部分构成了 InternalKey, 该结构是 MemTable 做排序的依据. 最后将该数据项插入到底层的 SkipList 中. 具体处理流程见下面代码注释: void MemTable::Add(SequenceNumber s, ValueType type, const Slice\u0026 key, const Slice\u0026 value) { // 插入到 memtable 的数据项的数据格式为 // (注意, memtable key 的构成与 LookupKey 一样, 即 user_key + 序列号 + 操作类型): // [varint32 类型的 internal_key_size, // user_key, // 序列号 + 操作类型, // varint32 类型的 value_size, // value] // 其中, internal_key_size = user_key size + 8 size_t key_size = key.size(); size_t val_size = value.size(); size_t internal_key_size = key_size + 8; // 编码后的数据项总长度 const size_t encoded_len = VarintLength(internal_key_size) + internal_key_size + VarintLength(val_size) + val_size; // 分配用来存储数据项的内存 char* buf = arena_.Allocate(encoded_len); // 将编码为 varint32 格式的 internal_key_size 写入内存 char* p = EncodeVarint32(buf, internal_key_size); // 将 user_key 写入内存 memcpy(p, key.data(), key_size); p += key_size; // 将序列号和操作类型写入内存. // 注意, 序列号为高 7 个字节; // 将序列号左移 8 位, 空出最低 1 个字节写入操作类型 type. EncodeFixed64(p, (s \u003c\u003c 8) | type); p += 8; // 将编码为 varint32 格式的 value_size 写入内存 p = EncodeVarint32(p, val_size); // 将 value 写入内存 memcpy(p, value.data(), val_size); assert(p + val_size == buf + encoded_len); // 将数据项插入跳跃表 table_.Insert(buf); } 可以看到由于模块化抽象做得好, 整体来说还是比较简单的. 后面章节会重点说一下上述处理中有关内存分配(arena_.Allocate())和数据项插入到 SkipList (table_.Insert()) 的相关操作. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#31-memtable-写方法"},{"categories":null,"content":" 3.2 MemTable 读方法bool Get(const LookupKey\u0026 key, std::string* value, Status* s) 用于实现针对 memtable 的读操作. 当用户调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 1 先查询当前在用的 memtable, 查到返回, 未查到下一步 2 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 3 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. 其中上面 1,2 两步骤查询 memtable 使用的即为本节开头提到的 Get 方法. // 如果 memtable 包含 key 对应的 value, 则将 value 保存在 *value 并返回 true. // 如果 memtable 包含 key 对应的 deletion(memtable 的删除不是真的删除, 而是一个带有删除标记的 Add 操作, // 而且 key 对应的 value 为空), 则将 NotFound 错误存在 *status, 并返回 true; // 其它情况返回 false. // LookupKey 就是 memtable 数据项的前半部分, 布局同 internal key. bool MemTable::Get(const LookupKey\u0026 key, std::string* value, Status* s) { // LookupKey 结构同 internal key. // 注意由于比较时, 当 userkey 一样时会继续比较序列号, // 而且序列号越大对应 key 越小, 所以外部调用 Get() // 方法前用户组装 LookupKey 的时候, 传入的序列号不能小于 // MemTable 每个 key 的序列号. 外部具体实现是采用 DB 记录的最大 // 序列号. Slice memkey = key.memtable_key(); // 为底层的 skiplist 创建一个临时的迭代器 Table::Iterator iter(\u0026table_); // 返回第一个大于等于 memkey 的数据项(数据项在 iter 内部存着), // 这里的第一个很关键, 当 userkey 相同但是序列号不同时, 序列号 // 大的那个 key 对应的数据更新, 同时由于 internal key 比较规则 // 是, userkey 相同序列号越大对应 key 越小, 所以 userkey 相同时 // 序列号最大的那个 key 肯定是第一个. iter.Seek(memkey.data()); // iter 指向有效 node, 即 node 不为 nullptr if (iter.Valid()) { // 每个数据项格式如下: // klength varint32 // userkey char[klength-8] // 源码注释这里有误, 应该是 klength - 8 // tag uint64 // vlength varint32 // value char[vlength] // 通过比较 user_key 部分和 ValueType 部分来确认是否是我们要找的数据项. // 不去比较序列号的原因是上面调用 Seek() 的时候已经跳过了非常大的序列号 // (internal_key 比较逻辑是序列号越大 internal_key 越小, 而我们 // 通过 Seek() 寻找的是第一个大于等于某个 internal_key 的节点). // // 注意, memtable 将底层的 SkipList 的 key(确切应该说是数据项) // 声明为了 char* 类型. 这里的 entry 是 SkipList.Node 里包含的整个数据项. const char* entry = iter.key(); uint32_t key_length; // 解析 internal_key 长度 const char* key_ptr = GetVarint32Ptr(entry, entry+5, \u0026key_length); // 比较 user_key. // 因为 internal_key 包含了 tag 所以任意两个 internal_key // 肯定是不一样的, 而我们真正在意的是 user_key, // 所以这里调用 user_comparator 比较 user_key. if (comparator_.comparator.user_comparator()-\u003eCompare( Slice(key_ptr, key_length - 8), key.user_key()) == 0) { // 解析 tag, 包含 7 字节序列号和 1 字节操作类型(新增/删除) const uint64_t tag = DecodeFixed64(key_ptr + key_length - 8); // 解析 tag 中的 ValueType. // leveldb 删除某个 user_key 的时候不是通过一个插入墓碑消息实现的吗? // 那怎么确保在 SkipList.Seek() 时候返回删除操作对应的数据项, // 而不是之前同样 user_key 对应的真正的插入操作对应的数据项呢? // 机巧就在于 internal_key 的比较原理 user_key 相等的时候, // tag 越大 internal_key 越小, // 这样后执行的删除操作的 tag(序列号递增了, 即使不递增, 但由于 // 删除对应的 ValueType 大于插入对应的 ValueType 也可以确保 // 后执行的删除操作的 tag 大于先执行的插入操作的 tag) // 这里有个比较技巧的地方. switch (static_cast\u003cValueType\u003e(tag \u0026 0xff)) { // 找到了 case kTypeValue: { Slice v = GetLengthPrefixedSlice(key_ptr + key_length); value-\u003eassign(v.data(), v.size()); return true; } // 找到了, 但是已经被删除了 case kTypeDeletion: *s = Status::NotFound(Slice()); return true; } } } return false; } 以上代码重要地方有如下几点: 参数 LookupKey 结构如 internal key, 前文介绍过具体结构不再赘述. 要强调的是, 构造其实例时, user key 部分采用用户传入的, 但是序列号要用数据库目前最大值, 这样可以确保作为比较环节的序列号不会影响我们的查找过程. 在底层 SkipList 查找之前, 构造了一个临时的迭代器, 迭代器结构体待后文介绍 SkipList 时详细介绍. 迭代器的 Seek 方法返回的是第一个大于等于目标 key 的节点, 这个第一个非常非常重要, 这也是为什么我们前面强调 LookupKey 的序列号必须足够达到不污染查找过程. Leveldb 的删除会插入一个墓碑消息, 其标识记录到 key 的 tag 部分, 所以查到数据的时候先不要高兴太早, 需要确认下 tag 的操作类型字段值. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#32-memtable-读方法"},{"categories":null,"content":" 3.3 MemTable 迭代器我们知道, MemTable 和磁盘上的 sstables 文件一起构成了数据全集, 而且 leveldb 支持迭代整个数据库. 这就要求 MemTable 也是可迭代的, 这样才能和磁盘文件迭代器串联在一起构成一个超级迭代器供用户迭代. 从 MemTable 实例返回一个迭代器的方法为: Iterator* MemTable::NewIterator() { return new MemTableIterator(\u0026table_); } MemTable 的迭代器实现很简单, 它提供了一个 class leveldb::MemTableIterator, 但整个类仅仅是一个 wrapper. 因为 MemTable 依赖的 SkipList 提供了一个完整的迭代器, MemTableIterator 仅仅在内部封装了一个 leveldb::SkipList\u003cKey, Comparator\u003e::Iterator 实例即实现了对 MemTable 的迭代, 所以它的实现就很简单了: // 该类全部方法都是对 leveldb::SkipList\u003cKey, Comparator\u003e::Iterator 方法的二次封装 class MemTableIterator: public Iterator { public: explicit MemTableIterator(MemTable::Table* table) : iter_(table) { } virtual bool Valid() const { return iter_.Valid(); } virtual void Seek(const Slice\u0026 k) { iter_.Seek(EncodeKey(\u0026tmp_, k)); } virtual void SeekToFirst() { iter_.SeekToFirst(); } virtual void SeekToLast() { iter_.SeekToLast(); } virtual void Next() { iter_.Next(); } virtual void Prev() { iter_.Prev(); } virtual Slice key() const { return GetLengthPrefixedSlice(iter_.key()); } virtual Slice value() const { Slice key_slice = GetLengthPrefixedSlice(iter_.key()); return GetLengthPrefixedSlice(key_slice.data() + key_slice.size()); } // 该迭代器默认返回 OK virtual Status status() const { return Status::OK(); } private: // 这个是真正干活的家伙 MemTable::Table::Iterator iter_; // 用于 EncodeKey 方法存储编码后的 internal_key std::string tmp_; // No copying allowed MemTableIterator(const MemTableIterator\u0026); void operator=(const MemTableIterator\u0026); }; 下文介绍 SkipList 时会着重介绍迭代器. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#33-memtable-迭代器"},{"categories":null,"content":" 3.4 MemTable 内存估计MemTable 提供了一个方法 ApproximateMemoryUsage() 来返回当前 MemTable 占用的内存空间大小. 它的实现依托底层的 Arena, 实现很简单: size_t MemTable::ApproximateMemoryUsage() { return arena_.MemoryUsage(); } Arena 每次分配内存的时候, 都会将分配的内存字节数累加到计数器上. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:3:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#34-memtable-内存估计"},{"categories":null,"content":" 4 MemTable 比较器之 KeyComparatorMemTable 既然是有序的, 那么任何操作就需要一个比较器. Memtable 内部定义了一个结构体 struct leveldb::MemTable::KeyComparator, 它封装了 MemTable 的比较逻辑, 具体如下: // 一个基于 internal_key 的比较器 // 注意下它的函数调用运算符重载方法的参数类型, 都是 char*, // 原因就是 memtable 底层的 SkipList 的 key 类型就是 char*, // 而类 KeyComparator 对象会被传给 SkipList 作为 key 比较器. struct KeyComparator { const InternalKeyComparator comparator; explicit KeyComparator(const InternalKeyComparator\u0026 c) : comparator(c) { } // 注意, 这个操作符重载方法很关键, 该方法的会先从 char* 类型地址 // 获取 internal_key, 然后对 internal_key 进行比较. // 该方法未在 memtable 直接调用, 而是被底层的 SkipList 使用了. int operator()(const char* a, const char* b) const; }; 该结构体除了在 MemTable::Get() 直接使用比较 user key 以外, 还会被用于构造 SkipList 实例. 其中函数调用运算符的定义如下: int MemTable::KeyComparator::operator()(const char* aptr, const char* bptr) const { // 提取数据项的前半部分, 即 internal_key Slice a = GetLengthPrefixedSlice(aptr); Slice b = GetLengthPrefixedSlice(bptr); // 对 internal_key 进行比较 return comparator.Compare(a, b); } 代码注释中提到的 internal key 在前面章节介绍过了, 不再赘述. 该结构体真正干活的是其封装的 comparator, 它是一个 class leveldb::InternalKeyComparator 实例, 比较器很重要的有两点: 它有一个唯一的名字, 非后向兼容的比较器如果重名则会导致数据库混乱. 毕竟 leveldb 是有序数据库, 顺序至关重要, 一旦被破坏就会混乱. 核心是 Compare 方法, 它决定了什么叫做顺序. InternalKeyComparator 用于比较 internal key, 而 internal key 包含 user key, 而 user key 是用户自己定义的所以比较器也由用户提供. 如果用户不提供, 则采用默认(见 leveldb::Options::Options() 定义)的 class leveldb::\u003cunnamed\u003e::BytewiseComparatorImpl, 它是一个基于字典序进行逐字节比较的内置 comparator 实现. InternalKeyComparator 比较时采用如下处理: 如果 user_key 相等, 则序列号越小 internal_key 越大; 如果序列号也相等, 则操作类型(更新/删除)越小 internal_key 越大(因为 leveldb 可以确保序列号单调递增且唯一, 所以实际上用不到该字段). ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:4:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#4-memtable-比较器之-keycomparator"},{"categories":null,"content":" 5 MemTable 内存管理之 ArenaLeveldb 专门为了管理内存定义了一个类 class leveldb::Arena, 可以把该类看作是 leveldb 的内存池实现, 但是它只负责对完分配, 并不会做回收重利用. 默认情况下, Arena 维护的内存块都是 4KB 大小. 注意, 同 MemTable 类一样, Arena 类全部操作需要调用者确保线程安全. 该类核心字段如下: 字段 类型 用途 alloc_ptr_ char* 指向 arena 当前空闲字节起始地址 alloc_bytes_remaining_ size_t arena 当前空闲字节数 blocks_ std::vector\u003cchar*\u003e 存放通过 new[] 分配的全部内存块 memory_usage_ port::AtomicPointer arena 持有的全部内存字节数 同 MemTable, Arena 也不支持拷贝构造和赋值构造. 该类核心方法如下: 方法 用途 Arena() 默认构造方法, 无任何限制. char* Allocate(size_t bytes) 从该 arena 返回一个指向大小为 bytes 的内存的指针. bytes 必须大于 0, 具体见实现. char* AllocateAligned(size_t bytes) 返回一个对齐后的可用内存的地址. 具体对齐逻辑见实现. size_t MemoryUsage() const 返回该 arena 持有的全部内存的字节数的估计值(未采用同步设施). size_t MemoryUsage() const 返回该 arena 持有的全部内存的字节数的估计值(未采用同步设施). ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:5:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#5-memtable-内存管理之-arena"},{"categories":null,"content":" 5.1 如何向 Arena 申请指定大小的内存可以通过方法 Arena::Allocate 申请指定大小的内存, 该方法会被 memtable 的 Add() 方法直接调用. 如果当前 Arena 内存池剩余内存足够则直接分配, 否则向操作系统进行申请. 具体处理流程如下: inline char* Arena::Allocate(size_t bytes) { // 如果我们允许分配 0 字节内存, 那该方法返回什么的语义就有点乱, // 所以我们不允许返回 0 字节(我们内部也没这个需求). assert(bytes \u003e 0); // arena 剩余内存够则直接分配 if (bytes \u003c= alloc_bytes_remaining_) { char* result = alloc_ptr_; alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } // arena 可用内存不能满足用户需求, 向系统申请新内存 return AllocateFallback(bytes); } 上面的 AllocateFallback() 根据用户需求决定向系统申请的内存大小, 具体处理如下: // 当 arena 可用内存不够时调用该方法来申请新内存 char* Arena::AllocateFallback(size_t bytes) { // 如果用户要申请的内存超过默认内存块大小(4KB)的 1/4, // 则单独按照用户指定大小分配一个内存块, 否则单独分配一个默认 // 大小(4KB)的内存块给用户会造成太多空间浪费. // 新分配的内存块会被加入 arena 然后将其起始地址返回给用户, // 所分配的内存不会纳入 Arena 管理, 但是占用空间会被纳入计数. if (bytes \u003e kBlockSize / 4) { char* result = AllocateNewBlock(bytes); return result; } // 直接分配一个默认大小内存块给用户, 此处所分配 // 内存会被纳入 Arena 管理, 等待用户后续内存申请. // alloc_ptr_ 被覆盖之前指向的空间被浪费了. alloc_ptr_ = AllocateNewBlock(kBlockSize); alloc_bytes_remaining_ = kBlockSize; // result 是返回给用户的 char* result = alloc_ptr_; // 移动 alloc_ptr_ 指向未被占用内存 alloc_ptr_ += bytes; alloc_bytes_remaining_ -= bytes; return result; } 上述处理中还涉及一个辅助方法 AllocateNewBlock, 它负责向系统申请固定大小内存并将其纳入 Arena 管理. 具体处理如下: // 分配一个大小为 block_bytes 的块并将其加入到 arena char* Arena::AllocateNewBlock(size_t block_bytes) { char* result = new char[block_bytes]; blocks_.push_back(result); // 调用者确保线程安全, 这里无需强制同步. memory_usage_.NoBarrier_Store( reinterpret_cast\u003cvoid*\u003e(MemoryUsage() + block_bytes + sizeof(char*))); return result; } ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:5:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#51-如何向-arena-申请指定大小的内存"},{"categories":null,"content":" 5.2 如何向 Arena 申请对齐后的内存除了上面提到的 Arena::Allocate(size_t bytes) 方法, Arena 还提供另一个内存分配方法, 它支持按照特定字节大小将所分配的内存进行对齐, 它的签名为 char* Arena::AllocateAligned(size_t bytes), 该方法没有被 memtable 代码直接调用, 而是由其底层依赖的 SkipList 所使用. 后面分析 SkipList 代码时还会再次说明. // 按照 8 字节或者指针长度进行内存对齐, 然后返回对齐后分配的内存起始地址 char* Arena::AllocateAligned(size_t bytes) { // 如果机器指针的对齐方式超过 8 字节则按照指针的对齐方式进行对齐; // 否则按照 8 字节进行对齐. const int align = (sizeof(void*) \u003e 8) ? sizeof(void*) : 8; // 确保当按照指针大小对齐时, 机器的指针大小是 2 的幂 assert((align \u0026 (align-1)) == 0); // 求 alloc_ptr 与 align 的模运算的结果, // 以确认 alloc_ptr 是否恰好为 align 的整数倍. size_t current_mod = reinterpret_cast\u003cuintptr_t\u003e(alloc_ptr_) \u0026 (align-1); // 如果 alloc_ptr 的值恰好为 align 整数倍, // 则已经满足对齐要求, 可以从该地址直接进行分配; // 否则, 需要进行手工对齐, 比如 align 为 8, alloc_ptr 等于 15, // 则需要将 alloc_ptr 增加 align - current_mod = 8 - 7 = 1 个字节. size_t slop = (current_mod == 0 ? 0 : align - current_mod); // 虽然用户申请 bytes 个字节, 但是因为对齐要求, // 实际消耗的内存大小为 bytes + slop size_t needed = bytes + slop; char* result; // 如果 arena 空闲内存满足要求则直接分配 if (needed \u003c= alloc_bytes_remaining_) { // 将 result 指向对齐后的地址(对齐会导致前 slop 个字节被浪费掉) result = alloc_ptr_ + slop; alloc_ptr_ += needed; alloc_bytes_remaining_ -= needed; } else { // 否则从堆中申请新的内存块, 注意重新分配 // 内存块时 malloc 会保证对齐, 无序再如上做手工对齐. result = AllocateFallback(bytes); } assert((reinterpret_cast\u003cuintptr_t\u003e(result) \u0026 (align-1)) == 0); return result; } ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:5:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#52-如何向-arena-申请对齐后的内存"},{"categories":null,"content":" 6 MemTable 底层存储之 SkipListMemTable 是有序的, 为了确保查询和插入迅速, 它采用了 SkipList 作为存储结构. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6-memtable-底层存储之-skiplist"},{"categories":null,"content":" 6.1 SkipList 原理简介开始之前先大体介绍下 SkipList 这个数据结构. 目前该数据结构在开源项目中用的越来越多代替红黑树等存在, 这里包括 Redis 都用到了该数据结构. 你在学校可能没学过这个数据结构, 但是你肯定在操作系统课上学过多级页表. SkipList 加速原理跟多级页表差不多, 当然后者如此设计也为了节省存储. 整个内存字节空间分成若干页面, 然后把全部页面分成若干组, 然后把上一步若干组进一步分为多个二级组 …; 查找时从最外层找起, 从外向里索引粒度逐渐减小, 最后一下子定位到内存空间某个字节位置, 这里虽然没说排序, 但是由于每个字节已经被编码所以其实也是有序的. SkipList 与之类似, 每个元素所在节点在一个链表上按序排列, 如果我们把每个节点看作内存空间一个字节位置, 那么接下来我们就要为它们建索引了: 第 0 级索引就是原始链表, 一个接一个, 共 $n$ 个节点 第 $1$ 级索引就是在上一级基础上间隔一个选取一个, 这样构成第 $1$ 级索引, 共 $\\frac{n}{2}$ 个节点 第 $2$ 级索引也是在上一级基础设间隔一个取一个, 这样构成第 $2$ 级索引, 共 $\\frac{n}{2^2}$ 个节点 … … 最后从上一级只能提取两个节点了, 这就是最后一级索引了因为没必要再往下分了, 假设共有 h 级, 那么这就是第 h-1 级索引, 该级索引共 $\\frac{n}{2^{h-1}}$ 个节点 我们调过头反着看, 每一层索引节点数都是其上一层索引的 $\\frac{1}{2}$, 这不就是个平衡二叉树吗? 总的节点数为 $2n$ 个. 每次查询时, 从最外层索引向里找, 每次在每一层只需查询一次, 最多查询 h 次也就是 $\\log_2^{2n} = 1 + \\log_2^n \\approx \\log_2^n$ 即可找到或则确认不存在. 我们把最外层看作最高层, 最里层看作最底层, 则 SkipList 的高度为 h. 可以看出时间复杂度同平衡二叉树如红黑树是一样的, 只是空间复杂度要多一倍. 那为什么不用红黑树呢, 嘿嘿, 你手写一次就知道了. 那么 SkipList 是如何确保平衡的呢? 方法强大又朴素, 抛硬币, 五五分. 每次插入元素时, 自上而下定位到其插入位置, 在第 $0$ 级插入元素时, 通过抛硬币的办法决定是否将该元素拔擢到第 $1$ 级索引中, 如果拔擢成功则再次抛硬币决定是否将其继续拔擢到第 $2$ 级索引, 以此类推, 除非上一级拔擢成功则继续抛硬币, 否则停止拔擢过程, 索引插入的时间复杂度也是 $\\log_2^n$. 删除类似, 从外层到内层, 查找到该元素后, 则从该层开始递进删除每一层中的该元素, 时间复杂度依然是 $\\log_2^n$. 好了, 我们来看看 leveldb 是如何实现 SkipList 的. ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#61-skiplist-原理简介"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template\u003cclass Key, class Comparator\u003e struct leveldb::Node\u003cKey, Comparator\u003e 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList\u003cKey, Comparator\u003e::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast\u003cNode*\u003e(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast\u003cNode*\u003e(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template\u003ctypename Key, class Comparator\u003e typename SkipList\u003cKey,Comparator\u003e::Node* SkipList\u003cKey,Comparator\u003e::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template\u003ctypename Key, class Comparator\u003e int SkipList\u003cKey,Comparator\u003e::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template\u003ctypename Key, class Comparator\u003e void SkipList\u003cKey,Comparator\u003e::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#62-skiplist-实现"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#621-skiplist-核心数据成员"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#622-skiplist-核心方法"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6223-辅助数据结构-node"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6224-用于为新数据分配存储的-newnode-方法"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6225-用于确定某个节点索引层数的-randomheight-方法"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6226-用于插入数据的-insert-方法"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6226-用于查找数据是否存在的-contains-方法"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6227-读写都要依赖的辅助方法-findgreaterorequal"},{"categories":null,"content":" 6.2 SkipList 实现下面详细说明 SkipList 实现时的细节以及一些辅助的数据结构. 6.2.1 SkipList 核心数据成员该类核心字段如下: 字段 类型 用途 compare_ Comparator 比较器, 用于比较 key, 初始化以后不可更改 arena_ Arena* 用于分配 Node 所用的内存 head_ Node* dummy node max_height_ port::AtomicPointer 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. rnd_ Random 指向存储当前 skiplist 最大高度的变量的地址, max_height_ \u003c= kMaxHeight. 同 MemTable, SkipList 也不支持拷贝构造和赋值构造. 6.2.2 SkipList 核心方法该类核心方法如下: 方法 用途 explicit SkipList(Comparator cmp, Arena* arena) 构造方法, cmp 用于比较 keys, arena 用做内存池. void Insert(const Key\u0026 key) 将 key 插入到 SkipList 实例中. bool Contains(const Key\u0026 key) const 当且仅当 sliplist 中存在与 key 相等的数据项时才返回 true. 线程安全相关说明： 写操作 Insert 需要外部同步设施, 比如 mutex. 读操作 Contains 需要一个保证, 即读操作执行期间, SkipList 不能被销毁; 只要保证这一点, 读操作不需要额外的同步措施. SkipList 运行不变式： (1) 已分配的 nodes 直到 SkipList 被销毁才能被删除. 这很容易保证, 因为我们不会删除任何 SkipList nodes. (2) 一个 Node 一旦被链接到 SkipList 上, 那这个 Node 的内容, 除了 next/pre 指针以外, 都是 immutable 的. 6.2.2.3 辅助数据结构 NodeSkipList 每个节点由类 template struct leveldb::Node 表示, 它包含两数据成员: key, 其实叫 entry 更符合实际. 因为该成员其实不只包含 key, 还包含 value 部分. 但是只要涉及查询, 比较时仅比较前半部分即 internal key. next_, 默认是一个长度为 1 的 port::AtomicPointer 数组(但是我估计作者们想分配长度为 0 的数组, 但是标准 C/++ 不允许). 这个数组的最大长度取决于当前节点最大要做第几级索引(从 0 开始计数). 一旦确定第几级, 后续调用 leveldb::SkipList::NewNode(const Key \u0026key, int height) 时就能把 Node 的 key 成员和当前数组分配到连续内存中, 这样对缓存友好, 而且释放时可以一次性释放. 如果 next_ 是指向数组的指针则要分多次释放了. Node 的方法都比较简单, 共四个, 详细注释如下: // 自带 acquire 语义, 返回该 node 在第 n 级(计数从 0 开始) 索引层的后继节点的指针 Node* Next(int n) { assert(n \u003e= 0); // 采用 acquire 语义可以确保如下两点: // - 当前线程后续针对 next_[n] 节点的读写不会被重排序到此 load 之前; // - 其它线程在此 load 之前针对 next_[n] 节点的全部写操作此时均可见. return reinterpret_cast(next_[n].Acquire_Load()); } // 自带 release 语义, 设置该 node 在第 n 级(计数从 0 开始) 索引层的后继节点 void SetNext(int n, Node* x) { assert(n \u003e= 0); // 采用 release 语义可以确保如下两点: // - 在此 store 之前, 当前线程针对 next_[n] 节点的读写不会被重排序到此 store 之后; // - 在此 store 之后, 其它线程针对 next_[n] 节点的读写看到的都是此 store 写入的值. next_[n].Release_Store(x); } // 同 Next, 但无同步防护. Node* NoBarrier_Next(int n) { assert(n \u003e= 0); return reinterpret_cast(next_[n].NoBarrier_Load()); } // 同 SetNext, 但无同步防护. void NoBarrier_SetNext(int n, Node* x) { assert(n \u003e= 0); next_[n].NoBarrier_Store(x); } 上面提到的 acquire/release 语义, 将会在本文最后一节做详细介绍, 感兴趣可以阅读. 6.2.2.4 用于为新数据分配存储的 NewNode 方法该方法用于为 SkipList 执行 Insert 方法时为所插入数据分配一个节点. 传入的第一个参数为要存储的数据项(虽然它叫 key, 但只有前半截是所谓的 internal key, 后半截是 value size 和 value); 第二个参数是通过 RandomHeight() 方法预先计算的索引层数, 即该节点最多可以做第几级(从 0 开始计数)索引. template typename SkipList::Node* SkipList::NewNode(const Key\u0026 key, int height) { // 要分配的空间存储的是用户数据和当前节点在 SkipList 各个索引层的后向指针, // 其中后者是现算出来的. char* mem = arena_-\u003eAllocateAligned( // 为啥减 1? 因为 Node.next_ 已默认分配了一项 sizeof(Node) + sizeof(port::AtomicPointer) * (height - 1)); // 此乃定位 new, 即在 mem 指向内存位置创建 Node 对象 return new (mem) Node(key); } 这里分配内存用到了我们前面介绍 Arena 类时分析的 AllocateAligned 方法. 该方法针对指针类型做了内存对齐(AtomicPointer 本身仅一个 void* 指针类型数据成员). 6.2.2.5 用于确定某个节点索引层数的 RandomHeight 方法该方法对应前面讲述 SkipList 原理时如何确定一个节点要被拔擢到最高第几层. 那里说是抛硬币, 五五分. Leveldb 实现 SkipList 时采用更为保守的拔擢策略, 每次递进概率仅为 1/4. // 返回一个高度值, 返回值落于 [1, kMaxHeight], // SkipList 实现默认索引层最多 12 个. template int SkipList::RandomHeight() { // 以 1/kBranching 概率循环递增 height. // 每次拔擢都是在前一次拔擢成功的前提下再进行, 如果前一次失败则停止拔擢. // 假设 kBranching == 4, 则返回 1 概率为 1/4, 返回 2 概率为 1/16, .... static const unsigned int kBranching = 4; // 每个节点最少有一层索引(就是原始链表) int height = 1; while (height \u003c kMaxHeight \u0026\u0026 ((rnd_.Next() % kBranching) == 0)) { height++; } assert(height \u003e 0); assert(height \u003c= kMaxHeight); return height; } 6.2.2.6 用于插入数据的 Insert 方法插入过程说起来也比较简单: 查找待插入数据的前驱节点, 这个是通过从 SkipList 查找第一个不小于待插入数据的节点来做到的. 确定待插入节点的索引层数, 这个就是随机大法. 更新 SkipList 当前索引层数最大值 为待插入数据生成一个新节点 将新节点插入到前驱和后驱之间, 每一个索引层都要插入一遍. 具体代码如下: // 该方法非线程安全, 需要外部同步设施. template void SkipList::Insert(const Key\u0026 key) { // pre 将用于存储 key 对应的各个索引层的前驱节点 Node* prev[kMaxHeight]; // 找到第一个大","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:6:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#6228-辅助数据结构-iterator"},{"categories":null,"content":" 7 番外: C++ 的内存排序本节内容主要参考 cppreference. Leveldb 内部多处用到同步设施, 这一块内容比较多, 后续会单独开辟一片文章介绍 leveldb 的原子类型实现. std::memory_order 指定了如何围绕原子操作对内存访问（包括常规的、非原子的内存访问）进行排序。当无任何顺序限制的时候, 在多核系统中, 当多个线程同时读写多个变量时, 一个线程观测到的某个值的变化顺序可能与负责写的线程的写顺序不同. 实际上, 甚至多个读线程之间各自观测到的变化顺序也互不相同. 此类情况哪怕在单核系统上也会发生, 因为只要内存模型允许, 编译器会在编译程序时进行指令重排序. 库中的全部原子操作的默认行为提供了顺序一致的排序(具体见后文). 这可能会造成性能损失, 但是可以给库中的原子操作提供额外的 std::memory_order 参数来指定具体的顺序限制, 除了保障原子性, 编译器和处理器必须执行这类限制确保顺序性. memory_order_relaxed Relaxed 操作: 仅保证该操作的原子性, 不会针对其它读写操作施加同步或者顺序限制(此即 Relaxed ordering) memory_order_consume 带有此限制的 load 操作针对受影响内存执行一个 consume 行为: 执行 load 操作的当前线程中后续针对该变量的读写操作不会被重排序到 load 操作之前, 从而确保后续读写用的都是最新加载的数据. 其它线程中, 如果先发生过针对同一个原子变量的写操作, 那么在它们 release 该原子变量之前针对该原子变量依赖的其它变量的写操作, 在当前线程 load 该原子变量时也都是可见的. 在大多数平台上, 这仅仅会影响编译器优化(此即 Release-Consume ordering). memory_order_acquire 带有此限制的 load 操作针对受影响内存执行一个 acquire 行为: 执行 load 操作的当前线程中后续依赖该变量的读写操作不会被重排序到该 load 操作之前, 从而确保后续读写用的都是最新加载的数据. 其它线程中, 如果先发生过针对同一个原子变量的写操作, 那么在它们 release 该原子变量之前针对其它变量的写操作(无论该原子变量是否依赖这些变量, 这比 consume 语义更强烈), 在当前线程 load 该原子变量时都是可见的(此即 Release-Acquire ordering). memory_order_release 带有此限制的 store 操作执行一个 release 行为: 执行 store 操作的当前线程中其它针对该变量的读写操作都不会被重排序到当前 store 操作之后, 从而确保之前的读写用的都是本次修改之前的值. 在此 store 之前的针对其它变量的全部写操作对 acquire 该原子变量的其它线程也都是可见的(此即 Release-Acquire ordering), 同时针对该原子变量所依赖的变量的写操作针对 consume 该原子变量的其它线程也都是可见的(此即 Release-Consume ordering). memory_order_acq_rel 带有此限制的 read-modify-write 操作既是一个 acquire 操作也是一个 release 操作. 执行 read-modify-write 操作的当前线程针对同一个变量的其它读写操作不能被重排序到该 store 操作之前或之后. 其它线程中针对该原子变量的写操作在 release 之后针对当前线程的 modify 是可见的, 同理当前线程的 modify 在 release 之后对其它 acquire 该原子变量的其它线程也是可见的. memory_order_seq_cst 带有此限制的 load 操作执行一个 acquire 行为, 带有此限制的 store 操作执行一个 release 行为, 带有此限制的 read-modify-write 操作既执行一个 acquire 行为也执行一个 release 行为, 此外, 该限制确保了一个单一全序, 也就是说, 全部线程观测道德全部修改行为都是一个顺序(此即 Sequentially-consistent ordering). 该限制语义最强. –End– ","date":"2020-10-01","objectID":"/leveldb-annotations-3-memtable/:7:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之三: MemTable 设计与实现","uri":"/leveldb-annotations-3-memtable/#7-番外-c-的内存排序"},{"categories":null,"content":"我们先简单回顾下 log 文件相关的基础知识点, 具体请见 Leveldb 源码详解系列之一: 接口与文件. log 文件(*.log)保存着数据库最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead logging). 当前在用的 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 每个更新操作都被追加到当前的 log 文件和 memtable 中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件对应的 memtable 就会被转换为一个 sorted string table 文件落盘然后一个新的 log 文件就会被创建以保存未来的更新操作. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#"},{"categories":null,"content":" log 文件结构log 文件内容是一系列 blocks, 每个 block 大小为 32KB(有时候最后一个 block 可能装不满). 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] type 取值如下: FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#log-文件结构"},{"categories":null,"content":" 读 log下面分析读 log 相关的类和方法. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#读-log"},{"categories":null,"content":" 核心文件与核心类与读 log 相关的代码定义在下面两个文件中: db/log_reader.h db/log_reader.cc 核心类为 class leveldb::log::Reader. 下面针对这个类核心方法进行分析. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#核心文件与核心类"},{"categories":null,"content":" Reader 构造方法 // 创建一个 Reader 来从 file 中读取和解析 records, // 读取的第一个 record 的起始位置位于文件 initial_offset 或其之后的物理地址. // 如果 reporter 不为空, 则在检测到数据损坏时汇报要丢弃的数据估计大小. // 如果 checksum 为 true, 则在可行的条件比对校验和. // 注意, file 和 reporter 的生命期不能短于 Reader 对象. Reader(SequentialFile* file, Reporter* reporter, bool checksum, uint64_t initial_offset) ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#reader-构造方法"},{"categories":null,"content":" Reader 读取方法bool ReadRecord(Slice* record, std::string* scratch) 方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 所做的事情, 概括地讲就是从文件读取下一个 record 到 *record 中. 如果读取成功, 返回 true; 遇到文件尾返回 false. 如果当前读取的 record 没有被分片, 那就用不到 *scratch 参数来为 *record 做底层存储了; 其它情况需要借助 *scratch 来拼装分片的 record data 部分, 最后封装为一个 Slice 赋值给 *record. 具体处理流程见下面详细注释: bool Reader::ReadRecord(Slice* record, std::string* scratch) { // last_record_offset_ 表示上一次调用 ReadRecord 方法返回的 // record 的起始偏移量, 注意这个 record 是逻辑的. // initial_offset_ 表示用户创建 Reader 时指定的在文件中寻找第一个 record 的起始地址. // 如果条件成立, 表示当前方法是首次被调用. if (last_record_offset_ \u003c initial_offset_) { // 跳到我们要读取的第一个 block 起始位置 if (!SkipToInitialBlock()) { return false; } } scratch-\u003eclear(); record-\u003eclear(); // 指示正在处理的 record 是否被分片了, // 除非逻辑 record 对应的物理 record 类型是 full, 否则就是被分片了. bool in_fragmented_record = false; // 记录我们正在读取的逻辑 record 的起始偏移量. 初值为 0 无实际意义仅为编译器不发警告. // 为啥叫逻辑 record 呢？ // 因为 block 大小限制, 所以 record 可能被分成多个分片(fragment). // 我们管 fragment 叫物理 record, 一个或多个物理 record 构成一个逻辑 record. uint64_t prospective_record_offset = 0; Slice fragment; while (true) { // 从文件读取一个物理 record 并将其 data 部分保存到 fragment, // 同时返回该 record 的 type. const unsigned int record_type = ReadPhysicalRecord(\u0026fragment); // 计算返回的当前 record 在 log file 中的起始地址 // = 当前文件待读取位置 // - buffer 剩余字节数 // - 刚读取的 record 头大小 // - 刚读取 record 数据部分大小 // end_of_buffer_offset_ 表示 log file 待读取字节位置 // buffer_ 表示是对一整个 block 数据的封装, 底层存储为 backing_store_, // 每次执行 ReadPhysicalRecord 时会移动 buffer_ 指针. uint64_t physical_record_offset = end_of_buffer_offset_ - buffer_.size() - kHeaderSize - fragment.size(); // resyncing_ 用于跳过起始地址不符合 initial_offset_ 的 record, // 如果为 true 表示目前还在定位第一个满足条件的逻辑 record 中. // 与 initial_offset_ 的比较判断在上面 ReadPhysicalRecord 中进行. if (resyncing_) { // 只要数据没有损坏或到达文件尾, 而且返回的 record_type 只要 // 不是 kBadRecord(返回该类型其中一个情况就是起始地址不满足条件) // 就说明当前 record 起始地址已经大于 initial_offset_ 了, // 但是如果当前 record 的 type 为 middle 或者 last, // 那么逻辑上这个 record 仍然与不符合 initial_offset_ 的 // 类型为 first 的 record 同属一个逻辑 record, // 所以当前 record 也不是我们要的. if (record_type == kMiddleType) { continue; } else if (record_type == kLastType) { resyncing_ = false; continue; continue; } else { // 如果是 full 类型的 record, 而且这个 record 起始地址 // 不小于 inital_offset_(否则 ReadPhysicalRecord 返回的 // 类型就是 kBadRecord 而非 full), // 满足条件了, 关掉标识. // 如果返回 kBadRecord/kEof(没什么可读了)/ // 未知类型(但是起始位置满足要求), 也会关掉该标识. resyncing_ = false; } } // 注意, 下面 switch 有的 case 是 return, 有的是 break. switch (record_type) { case kFullType: if (in_fragmented_record) { // 早期版本 writer 实现存在 bug. // 即如果上一个 block 末尾保存的是一个 FIRST 类型的 header, // 那么接下来 block 开头应该是一个 MIDDLE 类型的 record, // 但是早期版本写入了 FIRST 类型或者 FULL 类型的 record. if (!scratch-\u003eempty()) { ReportCorruption(scratch-\u003esize(), \"partial record without end(1)\"); } } prospective_record_offset = physical_record_offset; scratch-\u003eclear(); // 赋值构造 // FULL 类型 record 不用借助 scratch 拼装了 *record = fragment; last_record_offset_ = prospective_record_offset; // 读取到一个完整逻辑 record, 完成任务. return true; // 注意, 只有 first 类型的 record 起始地址满足大于 inital_offset_ 的时候 // 才会返回其真实类型 first, 其它情况哪怕是 first 返回也是 kBadRecord. case kFirstType: if (in_fragmented_record) { // 早期版本 writer 实现存在 bug. // 即如果上一个 block 末尾保存的是一个 FIRST 类型的 header, // 那么接下来 block 开头应该是一个 MIDDLE 类型的 record, // 但是早期版本写入了 FIRST 类型或者 FULL 类型的 record. if (!scratch-\u003eempty()) { ReportCorruption(scratch-\u003esize(), \"partial record without end(2)\"); } } // FIRST 类型物理 record 起始地址也是对应逻辑 record 的起始地址 prospective_record_offset = physical_record_offset; // 非 FULL 类型 record 需要借助 scratch 拼装成一个完整的 record data 部分. // 注意只有 first 时采用 assign, first 后面的分片要用 append scratch-\u003eassign(fragment.data(), fragment.size()); // 除了 FULL 类型 record, 都说明当前读取的 record 被分片了, // 还需要后续继续读取. in_fragmented_record = true; // 刚读了 first, 没读完, 继续. break; case kMiddleType: // 都存在 MIDDLE 了, 竟然还说当前 record 没分片, 报错. if (!in_fragmented_record) { ReportCorruption(fragment.size(), \"missing start of fragmented record(1)\"); } else { // 非 FULL 类型 record 需要借助 scratch 拼装成一个 // 完整的 record data 部分, // ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:2:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#reader-读取方法"},{"categories":null,"content":" 写 log下面分析写 log 相关的类和方法. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#写-log"},{"categories":null,"content":" 核心文件与核心类与写 log 相关的代码定义在下面两个文件中: db/log_writer.h db/log_writer.cc 核心类为 class leveldb::log::Writer. 下面针对这个类核心方法进行分析. ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#核心文件与核心类-1"},{"categories":null,"content":" Writer 构造方法 // 创建一个 writer 用于追加数据到 dest 指向的文件. // dest 指向的文件初始必须为空文件; dest 生命期不能短于 writer. explicit Writer(WritableFile *dest); // 创建一个 writer 用于追加数据到 dest 指向的文件. // dest 指向文件初始长度必须为 dest_length; dest 生命期不能短于 writer. Writer(WritableFile *dest, uint64_t dest_length); ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#writer-构造方法"},{"categories":null,"content":" Writer 写方法如果用户想把数据写入 log, 则需要将这些数据封装为 Slice, 然后调用 Writer::AddRecord 将其写入 log 文件. 写入时, 这个 Slice 内容即为 record 的 data 部分, 如果数据量太大导致一个 block(默认 32KB) 装不下, 则这些数据会被分片写入. 也就是说, 这些数据属于一个逻辑 record, 但是因为太大, 被分为若干物理 record 写入到 log 文件. 具体写入流程见源码注释: Status Writer::AddRecord(const Slice\u0026 slice) { const char* ptr = slice.data(); // data 剩余部分长度, 初始值为其原始长度 size_t left = slice.size(); // 如有必要则将 record 分片后写入文件. // 如果 slice 内容为空, 则我们仍将会写入一个长度为 0 的 record 到文件中. Status s; bool begin = true; do { // 当前 block 剩余空间大小 const int leftover = kBlockSize - block_offset_; assert(leftover \u003e= 0); // 如果当前 block 剩余空间不足容纳 record 的 header(7 字节) // 则剩余空间作为 trailer 填充 0, 然后切换到新的 block. if (leftover \u003c kHeaderSize) { if (leftover \u003e 0) { assert(kHeaderSize == 7); // 最终填充多少 0 由 leftover 决定, 最大 6 字节 dest_-\u003eAppend(Slice(\"\\x00\\x00\\x00\\x00\\x00\\x00\", leftover)); } block_offset_ = 0; } // 到这一步, block (可能因为不足 kHeaderSize 在上面已经切换到了下个 block) // 最终剩余字节必定大约等于 kHeaderSize assert(kBlockSize - block_offset_ - kHeaderSize \u003e= 0); // block 当前剩余空闲字节数. // 除了待写入 header, 当前 block 还剩多大空间, 可能为 0; // block 最后剩下空间可能只够写入一个新 record 的 header 了 const size_t avail = kBlockSize - block_offset_ - kHeaderSize; // 可以写入当前 block 的 record data 剩余内容的长度, 可能为 0 const size_t fragment_length = (left \u003c avail) ? left : avail; RecordType type; // 判断是否将 record 剩余内容分片 const bool end = (left == fragment_length); if (begin \u0026\u0026 end) { // 如果该 record 内容第一次写入文件, 而且, // 如果 block 剩余空间可以容纳 record data 全部内容, // 则写入一个 full 类型 record type = kFullType; } else if (begin) { // 如果该 record 内容第一写入文件, 而且, // 如果 block 剩余空间无法容纳 record data 全部内容, // 则写入一个 first 类型 record. // 注意, 此时是 record 第一次写入即它是一个新 record, // 该 block 剩余空间可能只够容纳 header 了, // 则在 block 尾部写入一个 FIRST 类型 header, record data 不写入, // 等下次循环会切换到下个 block, 然后又会重新写入一个 // 非 FIRST 类型的 header (注意下面会将 begin 置为 false) // 而不是紧接着在新 block 只写入 data 部分. type = kFirstType; } else if (end) { // 如果这不是该 record 内容第一写入文件, 而且, // 如果 block 剩余空间可以容纳 record data 剩余内容, // 则写入一个 last 类型 record type = kLastType; } else { // 如果这不是该 record 内容第一写入文件, 而且, // 如果 block 剩余空间无法容纳 record data 剩余内容, // 则写入一个 middle 类型 record type = kMiddleType; } // 将类型为 type, data 长度为 fragment_length 的 record 写入 log 文件. s = EmitPhysicalRecord(type, ptr, fragment_length); ptr += fragment_length; left -= fragment_length; // 即使当前 block 剩余空间只够写入一个新 record 的 FIRST 类型 header, // record 也算写入过了 begin = false; // 写入不出错且 record 再无剩余内容则写入完毕 } while (s.ok() \u0026\u0026 left \u003e 0); return s; } ‘AddRecord写入 record 时依赖的辅助方法EmitPhysicalRecord`. 该方法负责组装 record header, 然后连同 payload 写入文件. Status Writer::EmitPhysicalRecord(RecordType t, const char* ptr, size_t n) { // data 大小必须能够被 16 位无符号整数表示, 因为 record 的 length 字段只有两字节 assert(n \u003c= 0xffff); // 要写入的内容不能超过当前 block 剩余空间大小 assert(block_offset_ + kHeaderSize + n \u003c= kBlockSize); // buf 用于组装 record header char buf[kHeaderSize]; // 将数据长度编码到 length 字段, 小端字节序 // length 低 8 位安排在低地址位置 buf[4] = static_cast\u003cchar\u003e(n \u0026 0xff); // 然后写入 length 高 8 位安排在高地址位置 buf[5] = static_cast\u003cchar\u003e(n \u003e\u003e 8); // 将 type 编码到 type 字段, type 紧随 length 之后 1 字节 buf[6] = static_cast\u003cchar\u003e(t); // 计算 type 和 data 的 crc 并编码安排在最前面 4 个字节 uint32_t crc = crc32c::Extend(type_crc_[t], ptr, n); crc = crc32c::Mask(crc); // 将 crc 写入到 header 前四个字节 EncodeFixed32(buf, crc); // 写入 header Status s = dest_-\u003eAppend(Slice(buf, kHeaderSize)); if (s.ok()) { // 写入 payload s = dest_-\u003eAppend(Slice(ptr, n)); if (s.ok()) { // 刷入文件 s = dest_-\u003eFlush(); } } // 当前 block 剩余空间起始偏移量. // 注意, 这里不管 header 和 data 是否写成功. block_offset_ += kHeaderSize + n; return s; } –End– ","date":"2020-09-22","objectID":"/leveldb-annotations-2-log-read-write/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之二: log 读写","uri":"/leveldb-annotations-2-log-read-write/#writer-写方法"},{"categories":null,"content":"[toc] 最新的 Go Weekly 推送了这篇文章, eBPF 作为新时代的剖析工具正在如火如荼发展, 读完感觉用来入门很好, 就根据自己理解编译了这篇文章. 做实验过程遇到一些问题, 在最后加了一个番外章节可参考. 下面正式开始. 不用重新编译/部署线上程序而是借助 eBPF 即可实现对程序进行调试, 接下来我们会用一个系列文章介绍我们是怎么做的, 这是开篇. 本篇描述了如何使用 gobpf 和 uprobe 来构建一个跟踪 Go 程序函数入口参数变化的应用. 这里介绍的技术可以扩展到其它编译型语言, 如 C++, Rust 等等. 本系列文章后续将会讨论如何使用 eBPF 来跟踪 HTTP/gRPC 数据和 SSL 等等. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:0:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#"},{"categories":null,"content":" 介绍当调试程序时, 我们一般对捕获程序的运行时状态非常感兴趣. 因为这可以让我们检查程序在干什么, 并能让我们确定 bug 出现在程序的哪一块. 观察运行时状态的一个简单方式是使用调试器. 比如针对 Go 程序, 我们可以使用 Delve 和 gdb. Delve 和 gdb 在开发环境中做调试表现没得说, 但是我们一般不会在线上使用此类工具. 它们的长处同时也是它们的短处, 因为调试器会导致线上程序中断, 甚至如果在调试过程中不小心改错某个变量的值而导致线上程序出现异常. 为了让线上调试过程的侵入和影响更小, 我们将会探索使用增强版的 BPF(eBPF, Linux 4.x+ 内核可用)和更高级的 Go 库 gobpf 来达成目标. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:1:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#介绍"},{"categories":null,"content":" 什么是 eBPF扩展型 BPF(eBPF) 是一项在 Linux 4.x+ 内核可用的技术. 你可以把它看作一个轻量级的沙箱 VM, 它运行在 Linux 内核中并且提供了针对内核内存的可信访问. 就像下面要说的, eBPF 允许内核运行 BPF 字节码. 虽然可用的前端(这里指的是编译器前端)语言多样, 但通常都是 C 语言的真子集. 通常 C 代码先通过 Clang 被编译为 BPF 字节码, 然后字节被验证以确保可以安全执行. 这些严格的验证保证了机器码不会有意或无意地危及 Linux 内核, 同时也确保了 BPF 探针在每次被触发时将会执行有限数目的指令. 这些保证确保了 eBPF 可以被用于性能敏感的应用中, 比如包过滤, 网络监控等等. 从功能上说, eBPF 允许你针对某些事件(如定时器事件, 网络事件或是函数调用事件)运行受限的 C 代码. 当因为一个函数调用事件被触发时, 我们把这些 eBPF 代码叫做探针. 这些探针既可以针对内核函数调用事件被触发(这时叫 kprobe, k 即 kernelspace), 也可以针对用户空间的函数调用事件被触发(这时叫 uprobe, u 即 userspace). 本篇文章讲解如何通过 uprobe 实现函数参数的动态追踪. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:2:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#什么是-ebpf"},{"categories":null,"content":" UprobesUprobes 允许我们通过插入一个 debug trap 指令(在 x86 上就是 int3)触发一个软中断从而实现对运行在用户空间的程序进行拦截. 这也是调试器的工作原理. uprobe 运行过程本质上与其它 BPF 程序一样, 可以总结为下面图示: 用于跟踪的 BPF(来自 Brendan Gregg) 编译和验证过的 BPF 程序作为 uprobe 的一部分被执行, 同时执行结果写入到一个 buffer 中. 下面让我们研究下 uprobes 如何起作用的. 为了演示部署 uprobes 并捕获函数参数, 我们会用到这个简单的 demo 应用. 该 demo 相关部分下面介绍. main() 方法是一个简单的 HTTP server, 它暴露了一个监听 /e 端点的 GET 接口, 该接口通过迭代逼近计算自然常数 e(也叫欧拉数). computeE 方法有一个参数 iters, 它指定了逼近时的迭代次数. 迭代次数越多, 结果越精确, 当然耗费 CPU 也越多. 迭代逼近算法不是我们本次关注重点, 感兴趣的可以自己研究下. 我们仅对追踪调用 computeE 方法时的参数感兴趣. func computeE(iterations int64) float64 { res := 2.0 fact := 1.0 for i := int64(2); i \u003c iterations; i++ { fact *= float64(i) res += 1 / fact } return res } func main() { http.HandleFunc(\"/e\", func(w http.ResponseWriter, r *http.Request) { // ... 省略代码用于从 get 请求中解析 iters 参数, 若为空则使用默认值 w.Write([]byte(fmt.Sprintf(\"e = %0.4f\\n\", computeE(iters)))) }) // 启动 server... } 为了进行后面的实验以及为最后采用 gdb 验证修改生效, 我们采用如下指令编译该代码: $ go build -gcflags \"-N -l\" app.go 为了理解 uprobe 如何工作的, 我们看看可执行文件中要追踪的符号. 既然 uprobes 通过插入一个 debug trap 指令到可执行文件来实现, 我们先要确定要追踪的函数地址是什么. Go 程序在 Linux 上的二进制采用 ELF 格式存储 debug 信息, 该信息甚至在优化过的二进制中也是存在的, 除非 debug 数据被裁剪掉了. 我们可以使用命令 objdump 来检查二进制文件中的符号: ## 执行下面命令之前需要你先将上面 go 程序编译为名为 app 的二进制文件. ## objdump --syms 可以从可执行程序中导出全部符号, 然后通过 grep 查找 computeE. ## 具体输出可能与你机器上不同, 这没什么问题. $ objdump --syms app | grep computeE 00000000000x6600e0 g F .text 000000000000004b main.computeE 从上述输出可以看到, computeE 方法的入口地址为 0x0x6600e0. 为了看一下这个地址附近的指令, 我们可以通过 objdump 来反汇编该二进制文件(通过命令行选项 -d). 反汇编代码如下: $ objdump -d app | grep -A 1 0x6600e0 00000000000x6600e0 \u003cmain.computeE\u003e: 0x6600e0: 48 8b 44 24 08 mov 0x8(%rsp),%rax 从上面汇编代码可以看到当 computeE 方法被调用时会执行哪些指令. 第一条指令是 mov 0x8(%rsp),%rax, 该指令将距寄存器 rsp 保存的地址(栈指针寄存器保存的是 computeE 方法的入口地址)相对偏移量为 0x8 处的内容移动到寄存器 rax 中. 这个被移动的值即为 computeE 方法的入参 iterations 的值. Go 程序的参数通过栈来传递. 好了, 记住上面提到的信息, 我们来看看如何实现针对 computeE 方法的参数追踪. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:3:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#uprobes"},{"categories":null,"content":" 构建追踪程序我们给这个追踪程序起个名叫 Tracer. 为了捕获前面提到的事件, 我们需要注册一个 uprobe 函数, 并且还得有个用户态函数负责去读 uprobe 的输出, 具体如下图所示: 我们编写一个叫做 tracer 的应用, 由它负责注册 BPF 代码, 同时读取这些 BPF 代码的输出. 如上图所示, uprobe 将会简单地输出到一个 perf-buffer 中, 该结构体是用于 perf 事件的 linux 内核数据结构. 万事俱备, 我们来看看当我们增加一个 uprobe 时会发生哪些事情. 下面的图显示了 Linux 内核如何使用一个 uprobe 来修改一个已有的二进制程序. 前文提到的软中断 int3 作为第一条指令被插入到 main.computeE 方法中. 这条指令将会在执行时触发一个软中断, 从而允许 Linux 内核来执行 BPF 代码. 然后我们把 computeE 每次被调用时的参数输出到 perf-buffer 中, 这些值会被我们编写的 tracer 应用异步地读取. 就我们这个需求来说, 相应的 BPF 代码很简单, C 代码如下: #include \u003cuapi/linux/ptrace.h\u003e BPF_PERF_OUTPUT(trace); // 该函数将会被注册, 以便每次 main.computeE 被调用时该函数也会被调用 inline int computeECalled(struct pt_regs *ctx) { // main.computeE 的入参保存在了 ax 寄存器里. long val = ctx-\u003eax; trace.perf_submit(ctx, \u0026val, sizeof(val)); return 0; } 我们注册上面代码以便 main.computeE 方法被调用它们也会被执行. 这些代码被执行时, 我们仅仅读取函数参数然后写到 perf-buffer 中. 实现这个功能需要很多样板代码, 为了方便示意这里都省掉了, 完整的例子见这里. 好了, 我们现在有个针对 main.computeE 的功能齐全的端到端参数追踪器了! 执行结果见下面动图: 上述动图执行步骤如下: 1 在 localhost:9090 启动待追踪程序 ./app, 此时我们可以用 curl 访问该应用了, 具体命令为 curl http://localhost:9090/e?iters=10 2 启动 trace 应用, 注意指定参数 sudo ./trace --binary ../app/app, 参数是第一步中待追踪程序对应的二进制文件的路径. 3 不停的执行 curl 命令, 使其 iters 参数取值不同, 则会看到 trace 应用输出你指定的 iters 值. 还有个有意思的事情, 我们真的可以通过 GDB 看到针对二进制文件的修改. 下面我们 dump 出 0x0x6600e0 处的指令, 在我们运行 trace 之前是这样的: $ gdb ./app (gdb) display /4i 0x6600e0 1: x/4i 0x6600e0 0x6600e0 \u003cmain.computeE\u003e: sub $0x20,%rsp 0x6600e4 \u003cmain.computeE+4\u003e: mov %rbp,0x18(%rsp) 0x6600e9 \u003cmain.computeE+9\u003e: lea 0x18(%rsp),%rbp 0x6600ee \u003cmain.computeE+14\u003e: xorps %xmm0,%xmm0 在我们运行 trace 之后, 再次查看: $ gdb ./app (gdb) display /4i 0x65fecf 2: x/4i 0x6600e0 0x6600e0 \u003cmain.computeE\u003e: int3 0x6600e1 \u003cmain.computeE+1\u003e: sub $0x20,%esp 0x6600e4 \u003cmain.computeE+4\u003e: mov %rbp,0x18(%rsp) 0x6600e9 \u003cmain.computeE+9\u003e: lea 0x18(%rsp),%rbp 看到了吗? 0x6600e0 插入了 int3 指令. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:4:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#构建追踪程序"},{"categories":null,"content":" 番外下面说一下实验过程遇到的问题以及解决办法. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:5:0","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#番外"},{"categories":null,"content":" 安装 BCC编译前文提到的 trace 应用之前需要安装 bcc. 以 Ubuntu 16.04 为例(其它系统请参考这里): sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 4052245BD4284CDD echo \"deb https://repo.iovisor.org/apt/$(lsb_release -cs) $(lsb_release -cs) main\" | sudo tee /etc/apt/sources.list.d/iovisor.list sudo apt-get update sudo apt-get install bcc-tools libbcc-examples linux-headers-$(uname -r) 如果安装速度慢, 而且你设置了 http_proxy/https_proxy, 请编辑 /etc/sudoers 新增一行 Defaults env_keep = \"http_proxy https_proxy\", 这样速度至少会有百倍提升. ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:5:1","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#安装-bcc"},{"categories":null,"content":" too many arguments 编译错误 ## github.com/iovisor/gobpf/bcc ../../../../go/pkg/mod/github.com/iovisor/gobpf@v0.0.0-20200614202714-e6b321d32103/bcc/module.go:98:40: too many arguments in call to _Cfunc_bpf_module_create_c_from_string have (*_Ctype_char, number, **_Ctype_char, _Ctype_int, _Ctype__Bool, nil) want (*_Ctype_char, _Ctype_uint, **_Ctype_char, _Ctype_int, _Ctype__Bool) ../../../../go/pkg/mod/github.com/iovisor/gobpf@v0.0.0-20200614202714-e6b321d32103/bcc/module.go:230:28: too many arguments in call to _C2func_bcc_func_load have (unsafe.Pointer, _Ctype_int, *_Ctype_char, *_Ctype_struct_bpf_insn, _Ctype_int, *_Ctype_char, _Ctype_uint, _Ctype_int, *_Ctype_char, _Ctype_uint, nil) want (unsafe.Pointer, _Ctype_int, *_Ctype_char, *_Ctype_struct_bpf_insn, _Ctype_int, *_Ctype_char, _Ctype_uint, _Ctype_int, *_Ctype_char, _Ctype_uint) 原因为这一行增加的特性 Update bcc_func_load to libbcc 0.11 with hardware offload support, 以及这一行增加的特性 bcc: update bpf_module_create_c_from_string for bcc 0.11.0 (fixes #202). 我没有深究具体是什么导致的(初步怀疑是系统版本), 如果你急着看结果, 可以根据上面报错地址知道到 module.go 文件, 把涉及的两个函数的最后一个 nil 参数去掉就可以顺利编译了. –End– ","date":"2020-09-21","objectID":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/:5:2","series":null,"tags":["eBPF","golang"],"title":"使用 eBPF 调试生产环境的 Go 程序","uri":"/%E4%BD%BF%E7%94%A8-ebpf-%E8%B0%83%E8%AF%95%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E7%9A%84-go-%E7%A8%8B%E5%BA%8F/#too-many-arguments-编译错误"},{"categories":null,"content":" 从哪里着手分析 leveldb 实现在了解了其基本使用以后, 如果想理解 leveldb 基本原理, 则有两个抓手. 第一个是 include 目录下的头文件, 尤其是 db.h , 第二个就是它的文件类型及其格式. 下面我们就从接口和文件两个方向来切入 leveldb 的设计与实现. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#从哪里着手分析-leveldb-实现"},{"categories":null,"content":" leveldb 常用的接口","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#leveldb-常用的接口"},{"categories":null,"content":" Open /** * 打开一个名为 name 的数据库. * * 打开成功, 会把一个指向基于堆内存的数据库指针存储到 *dbptr, 同时返回 OK; 如果打开失败, * 存储 nullptr 到 *dbptr 同时返回一个错误状态. * * 调用者不再使用这个数据库时需要负责释放 *dbptr 指向的内存. * * @param options 控制数据库行为和性能的参数配置 * @param name 数据库名称 * @param dbptr 存储指向堆内存中数据库的指针 * @return */ static Status Open(const Options\u0026 options, const std::string\u0026 name, DB** dbptr); 该方法在数据库启动时调用, 主要工作由 leveldb::DBImpl::Recover 方法完成, 后者主要做如下事情: 调用其 VersionSet 成员的 leveldb::VersionSet::Recover 方法. 该方法从磁盘读取 CURRENT 文件, 进而读取 MANIFEST 文件内容, 然后在内存重建 level 架构: 读取 CURRENT 文件(不存在则新建)找到最新的 MANIFEST 文件(不存在则新建)的名称 读取该 MANIFEST 文件内容与当前 Version 保存的 level 架构合并保存到一个新建的 Version 中, 然后将这个新的 version 作为当前的 version, 即最新的 level 架构信息. 清理过期的文件 这一步我们可以打开全部 sstables, 但最好等会再打开 将 log 文件块转换为一个新的 level-0 sstable 将接下来的要写的数据写入一个新的 log 文件 遍历数据库目录下全部文件. 筛选出 sorted string table 文件, 验证 VersionSet 包含的 level 架构图有效性; 同时将全部 log 文件筛选换出来后续反序列化成 memtable. 恢复 log 文件时会按照从旧到新逐个 log 文件恢复, 这样新的修改会覆盖旧的, 如果对应 memtable 太大了, 将其转为 sorted string table 文件写入磁盘, 同时将其对应的 table 对象放到 table_cache_ 缓存. 若发生 memtable 落盘表示 level 架构新增文件则将 save_manifest 标记为 true, 表示需要写变更日志到 manifest 文件. 恢复 log 文件主要由方法 leveldb::DBImpl::RecoverLogFile 负责完成. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#open"},{"categories":null,"content":" Put /** * 将 \u003ckey, value\u003e 对写入数据库, 成功返回 OK, 失败返回错误状态. * @param options 本次写操作相关的配置参数, 如果有需要可以将该参数中的 sync 置为 true, 不容易丢数据但更慢. * @param key Slice 类型的 key * @param value Slice 类型的 value * @return 返回类型为 Status */ virtual Status Put(const WriteOptions\u0026 options, const Slice\u0026 key, const Slice\u0026 value) = 0; 该方法主要依赖 leveldb::DBImpl::Write 实现. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#put"},{"categories":null,"content":" Delete /** * 从数据删除指定键为 key 的键值对. 如果 key 不存在不算错. * * @param options 本次写操作相关的配置参数, 如果有需要可以将该参数中的 sync 置为 true, 不容易丢数据但更慢. * @param key 要删除数据项对应的 key * @return */ virtual Status Delete(const WriteOptions\u0026 options, const Slice\u0026 key) = 0; 该方法主要依赖 leveldb::DBImpl::Write 实现. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#delete"},{"categories":null,"content":" Write /** * 对数据库进行批量更新写操作. * * 该方法线程安全, 内部自带同步. * * @param options 本次写操作相关的配置参数, 如果有需要可以将该参数中的 sync 置为 true, 不容易丢数据但更慢. * @param updates 要进行的批量更新操作 * @return */ virtual Status Write(const WriteOptions\u0026 options, WriteBatch* updates) = 0; 该方法是 leveldb::DBImpl::Write 原型. 针对调用 db 进行的写操作, 都会生成一个对应的 struct leveldb::DBImpl::Writer, 其封装了写入数据和写入进度. 新构造的 writer 会被放入一个队列. 循环检查, 若当前 writer 工作没完成并且不是队首元素, 则当前有其它 writer 在写, 挂起当前 writer 等待条件成熟. 当前 writer 如果被排在前面的 writer 给合并写入了, 那么它的 done 就被标记为完成了. 否则会被其它在写入的 writer 调用其 signal 将其唤醒执行写入工作. 当执行写入工作时(被前一个执行写入并完成工作的 writer 唤醒了), 首先确认是否为本次该 writer 写操作分配新的 log 文件, 如果需要则分配. 因为该 writer 成为队首 writer 了, 则它负责将队列前面若干 writers 的 batch 合并为一个(该工作由leveldb::DBImpl::BuildBatchGroup 负责完成), 注意, 被合并的 writers 不出队(待合并写入完成再出队, 具体见后面描述), 所以写 log 期间队首 writer 不变. 具体写入工作由 leveldb::log::Writer::AddRecord 负责, 就是将数据序列化为 record 写入 log 文件. 如果追加 log 文件成功,则将被追加的数据插入到内存中的 memtable 中. 待写入完毕, 该 writer 将参与前述 batch group 写入 log 文件的 writer 都取出来并设置为写入完成, 即将其出队, 将其 done 置为 true, 同时向其发送信号将其唤醒, 被唤醒后它会检查其 done 标识并返回. 最后唤醒队首 writer 执行下一个合并写入. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#write"},{"categories":null,"content":" Get /** * 查询键为 key 的数据项, 如果存在则将对应的 value 地址存储到第二个参数中. * * 如果 key 不存在, 第二个参数不变, 返回值为 IsNotFound Status. * * @param options 本次读操作对应的配置参数 * @param key 要查询的 key, Slice 引用类型 * @param value 存储与 key 对应的值的指针, string 指针类型 * @return */ virtual Status Get(const ReadOptions\u0026 options, const Slice\u0026 key, std::string* value) = 0; 先查询当前在用的 memtable(具体工作由 leveldb::MemTable::Get 负责, 本质就是 SkipList 查询, 速度很快) 如果没有则查询正在转换为 sorted string table 的 memtable 中寻找 如果没有则我们在磁盘上采用从底向上 level-by-level 的寻找目标 key. 针对上述第 3 步, 具体由 db VersionSet 的当前 Version 负责, 因为该结构保存了 db 当前最新的 level 架构信息, 即每个 level 及其对应的文件列表和每个文件的键范围. 对应方法为 leveldb::Version::Get, 具体为: 从低 level 向高 level 寻找. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. 另外需要注意的的是, 参数 options 可以配置一个快照, 快照对应了数据库历史上的一个操作序列号, 查询时仅查询不大于该序列号的操作范围. 针对同样的 key, 如果历史上有多次更新操作, 而用户想查找特定更新, 这就是实现途径. 如果没有配置快照选项, 默认采用当前最大序列号进行查询. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#get"},{"categories":null,"content":" NewIterator /** * 返回基于堆内存的迭代器, 可以用该迭代器遍历整个数据库的内容. * 该函数返回的迭代器初始是无效的(在使用迭代器之前, 调用者必须在其上调用 Seek 方法). * * 当不再使用时, 调用者应该释放该迭代器对应的内存, 而且迭代器必须在数据库释放之前进行释放. * @param options 本次读操作对应的配置参数 * @return */ virtual Iterator* NewIterator(const ReadOptions\u0026 options) = 0; 该方法负责将内存 memtable(可能有两个, 一个在写, 一个写完待存盘) 和磁盘 sorted string table 文件全部数据结构串起来构造一个大一统迭代器, 可以遍历整个数据库. 上述工作其实是由 leveldb::Iterator *leveldb::DBImpl::NewInternalIterator 负责完成的. 该方法实现涉及到 leveldb 特别精巧的迭代器的实现. 这个单独可以写一篇文章来专门介绍. 这里大致说下处理流程: 1 初始化一个列表 2 把当前 memtable 迭代器加入列表中 3 把待写盘 memtable 迭代器追加到列表中 4 将当前 version 维护的 level 架构中每个 sorted string table 文件对应的迭代器追加到列表中. 针对 level-0 和其它 levels 处理方式不同. 由于 level-0 文件之间可能存在重叠, 所以按照文件生成顺序(这极其重要, 其实就是按照 key 从小到大, 只有这样才能确保最后生成的迭代器能够从小到大按序遍历整个数据库) 为每个文件生成一个两级迭代器(TwoLevelIterator, 该结构巧妙地将索引块和数据块结合到了一起)追加到列表中. 针对 level-1 及其以上 level, 按照从低 level 到高 level(这极其重要, 原因同 level-0), 为每个 level 生成一个两级迭代器, 数据结构依然是 TwoLevelIterator, 不过这里把每个 level 的文件列表抽象成了第一级索引, 然后每个文件对应的 table 对象抽象层二级索引. 最后将前述全部迭代器构成的迭代器列表再级联成一个大一统的迭代器 MergingIterator. 这其实也是一个两级迭代器, 第一级指向迭代器列表, 第二级是某个迭代器指向的内容的迭代器. 最后返回给调用者的就是 MergingIterator 实例. 可以调用它的相关方法在整个数据库上寻找目标 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:6","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#newiterator"},{"categories":null,"content":" GetSnapshot /** * 返回当前 DB 状态的一个快照. * 使用该快照创建的全部迭代器将会都指向一个当前 DB 的一个稳定快照. * * 当不再使用该快照时, 调用者必须调用 ReleaseSnapshot 将其释放. * @return */ virtual const Snapshot* GetSnapshot() = 0; 用数据库当前最新的更新操作对应的序列号创建一个快照. 快照最核心的就是那个操作序列号, 因为查询时会把 用户提供的 key(我们叫做 user_key)和操作序列号一起构成一个 internal_key(数据库存储的 key 就是它), 针对 user_key 相等的情况比如针对 hello 这个 user_key Put 多次, 则每次序列号就不一样, 于是根据特定序列号可以查询到特定的那次 Put 写入的 value 值. 这个新生成的快照会被挂载到一个双向链表上, 用完后可以调用 ReleaseSnapshot 将其释放掉. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:7","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#getsnapshot"},{"categories":null,"content":" ReleaseSnapshot /** * 释放一个之前获取的快照, 释放后, 调用者不能再使用该快照了. * @param snapshot 指向要释放的快照的指针 */ virtual void ReleaseSnapshot(const Snapshot* snapshot) = 0; 从双向链表上删除指定的快照. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:8","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#releasesnapshot"},{"categories":null,"content":" GetProperty /** * DB 实现可以通过该方法导出自身状态相关的信息. 如果提供的属性可以被 DB 实现理解, * 那么第二个参数将会存储该属性对应的当前值同时该方法返回 true, 其它情况该方法返回 false. * * 合法的属性名称包括: * * \"leveldb.num-files-at-level\u003cN\u003e\" - 返回 level \u003cN\u003e 的文件个数, 其中 \u003cN\u003e 是一个数字. * * \"leveldb.stats\" - 返回多行字符串, 描述该 DB 内部操作相关的统计数据. * * \"leveldb.sstables\" - 返回多行字符串, 描述构成该 DB 的全部 sstable 相关信息. * * \"leveldb.approximate-memory-usage\" - 返回被该 DB 使用的内存字节数近似值 * @param property 要查询的属性名称 * @param value 保存属性名称对应的属性值 * @return */ virtual bool GetProperty(const Slice\u0026 property, std::string* value) = 0; leveldb 实现在内部做了一些统计, 可以通过这个接口进行查询. 不过目前可查询状态不多, 具体如下: “leveldb.num-files-at-level” - 返回 level 的文件个数, 其中 是一个 ASCII 格式的数字. “leveldb.stats” - 返回多行字符串, 描述该 DB 内部操作相关的统计数据. “leveldb.sstables” - 返回多行字符串, 描述构成该 DB 的全部 sstable 相关信息. “leveldb.approximate-memory-usage” - 返回被该 DB 使用的内存字节数近似值 ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:9","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#getproperty"},{"categories":null,"content":" GetAppoximateSizes /** * 对于 [0, n-1] 中每个 i, 将位于 [range[i].start .. range[i].limit) * 中全部 keys 所占用文件系统空间近似大小存储到 sizes[i] 中. * * 注意, 如果数据被压缩过了, 那么返回的 sizes 存储的就是压缩后数据所占用文件系统空间大小. * * 返回结果可能不包含最近刚写入的数据所占用空间. * @param range 指定要查询一组 keys 范围 * @param n range 和 sizes 两个数组的大小 * @param sizes 存储查询到的每个 range 对应的文件系统空间近似大小 */ virtual void GetApproximateSizes(const Range* range, int n, uint64_t* sizes) = 0; 计算 range 包含的键区间在磁盘上占用的空间大小, 每个子区间占用会保存到 sizes 对应位置. 计算过程也很简单, 就是遍历 range 列表, 针对每个子区间起止 key, 去数据库中确认其大致字节偏移, 然后\"止\"-“始” 即为子区间占用空间的大致大小. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:10","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#getappoximatesizes"},{"categories":null,"content":" CompactRange /** * 将 key 范围 [*begin,*end] 对应的底层存储压紧, 注意范围是左闭右闭. * * 尤其是, 压实过程会将已经删除或者复写过的数据会被丢弃, 同时会将数据 * 重新安放以减少后续数据访问操作的成本. * 这个操作是为那些理解底层实现的用户准备的. * * 如果 begin==nullptr, 则从第一个键开始; 如果 end==nullptr 则到最后一个键为止. * 所以, 如果像下面这样做则意味着压紧整个数据库: * * db-\u003eCompactRange(nullptr, nullptr); * @param begin 起始键 * @param end 截止键 */ virtual void CompactRange(const Slice* begin, const Slice* end) = 0; 手动触发与目标键区间重叠的文件压实. 具体为: 检查每个 level, 确认其包含的键区间释放与目标键区间有交集. 因为当前在写 memtable 可能与目标键区间有交集, 所以强制触发一次 memtable 压实(即将当前 memtable 文件转为 sorted string table 文件并写入磁盘)并生成新 log 文件和对应的 memtable. 针对与目标键区间有交集的各个 level 触发一次手动压实 具体压实过程后续会写一篇文章进行介绍. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:2:11","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#compactrange"},{"categories":null,"content":" leveldb 的文件类型下面分别介绍 leveldb 最重要的几个文件类型. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#leveldb-的文件类型"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件格式"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件格式的好处"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件的缺点并不是"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#log-文件主要接口"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#写-log"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#读-log"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#与-log-文件配套的-memtable"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#结构"},{"categories":null,"content":" log 文件一个 log 文件(*.log)保存着最近一系列更新操作, 它相当于 leveldb 的 WAL(write-ahead log). 每个更新操作都被追加到当前的 log 文件中. 当 log 文件大小达到一个预定义的大小时(默认大约 4MB), 这个 log 文件就会被转换为一个 sorted string table (见下文)然后一个新的 log 文件就会被创建以保存未来的更新操作. 当前 log 文件内容同时也会被记录到一个内存数据结构中(即 memtable ). 这个结构加上全部 sorted string tables (*.ldb) 才是完整数据, 一起确保每个读操作都能查到当前最新. log 文件格式log 文件内容是一系列 blocks, 每个 block 大小为 32KB. 唯一的例外就是, log 文件末尾可能包含一个不完整的 block. 每个 block 由一系列 records 构成, 具体定义如下(熟悉编译原理的应该对下述写法不陌生): // 即 0 或多个 records, 0 或 1 个 trailer. // 最大为 32768 字节. block := record* trailer? record := // 下面提到的 type 和 data[] 的 crc32c 校验和, 小端字节序 checksum: uint32 // 下面的 data[] 的长度, 小端字节序 length: uint16 // 类型, FULL、FIRST、MIDDLE、LAST 取值之一 type: uint8 // 用户数据 data: uint8[length] 如果一个 block 剩余字节不超过 6 个(checksum 字段长度 + length 字段长度 + type 字段长度 = 7), 则不会再构造任何 record, 如前括号解释因为大小不合适. 这些剩余空间会被用于构造一个 trailer, reader 读取该文件时候会忽略之. 此外, 如果当前 block 恰好剩余 7 个字节(正好可以容纳 record 中的 checksum + length + type), 并且一个新的非 0 长度的 record 要被写入, 那么 writer 必须在此处写入一个 FIRST 类型的 record(但是 length 字段值为 0, data 字段为空. 用户数据 data 部分需要写入下个 block, 而且下个 block 起始还是要写入一个 header 不过其 type 为 middle)来填满该 block 尾部的 7 个字节, 然后在接下来的 blocks 中写入全部用户数据. 未来可能加入更多的 record 类型. Readers 可以跳过它们不理解的 record 类型, 也可以在跳过时进行报告. FULL == 1 FIRST == 2 MIDDLE == 3 LAST == 4 FULL 类型的 record 包含了一个完整的用户 record 的内容. FIRST、MIDDLE、LAST 这三个类型用于被分割成多个 fragments(典型的理由是某个 record 跨越了多个 block 边界) 的用户 record. FIRST 表示某个用户 record 的第一个 fragment, LAST 表示某个用户 record 的最后一个 fragment, MIDDLE 表示某个用户 record 的中间 fragments. 举例: 考虑下面一系列用户 records: A: 长度 1000 B: 长度 97270 C: 长度 8000 A 会被作为 FULL 类型的 record 存储到第一个 block, 第一个 block 剩余空间为 32768 - 7 - 1000 = 31761; B 会被分割为 3 个 fragments: 第一个 fragment 占据第一个 block 剩余空间, 共存入 31761 - 7 = 31754, 剩余 65516; 第二个 fragment 占据第二个 block 的全部空间, 存入 32768 - 7 = 32761, 剩余 65516 - 32761 = 32755; 第三个 fragment 占据第三个 block 的起始空间共 7 + 32755 = 32762. 所以最后在第三个 block 剩下 32768 - 32762 = 6 个字节, 这几个字节会被填充 0 作为 trailer. C 将会被作为 FULL 类型的 record 存储到第四个 block 中. MANIFEST 文件的格式同 log 文件, 只是记录的具体内容不同, 前者记录的针对 level 架构的文件级别变更(新增/删除), 后者记录的是用户数据 key-value 变更. log 文件格式的好处log 文件格式的好处是(总结一句话就是容易划分边界): 不必进行任何启发式地 resyncing(可以理解为寻找一个 block 的边界) —— 直接跳到下个 block 边界进行扫描即可, 因为每个 block 大小是固定的(32768 个字节, 除非文件尾部的 block 未写满). 如果数据有损坏, 直接跳到下个 block. 这个文件格式的附带好处是, 当一个 log 文件的部分内容作为一个 record 嵌入到另一个 log 文件时(即当一个逻辑 record 分为多个物理 records, 一部分 records 位于前一个 log 文件, 剩下 records 位于下个 log 文件), 我们不会分不清楚. 在估计出来的边界处做分割(比如为 mapreduce 应用)变得简单了: 找到下个 block 的边界, 如果起始是 MIDDLE 或者 LAST 类型的 record, 则跳过直到我们找到一个 FULL 或者 FIRST record 为止, 就可以在此处做分割, 一部分投递到一个计算任务, 另一部分(直到分界处)投递到另一个计算任务. log 文件的缺点(并不是)log 文件格式的缺点: 没有打包小的 records. 通过增加一个新的 record 类型可以解决这个问题, 所以这个问题是当前实现的不足而不是 log 格式的缺陷. 没有压缩. 同样地, 这个也可以通过增加一个新的 record 类型来解决. log 文件主要接口下面介绍下 log 文件的读写实现. 写 log leveldb::Status leveldb::log::Writer::AddRecord(const leveldb::Slice \u0026slice) 该接口做的事情就是把外部传入的 Slice 封装成若干 records 追加到 log 文件中. 该方法会被 leveldb::Status leveldb::DBImpl::Write(const leveldb::WriteOptions \u0026options, leveldb::WriteBatch *my_batch) 调用以响应用户的写操作. DBImpl 是 DB 的派生类, 其 Put 和 Delete 方法真正工作是由派生类的 Write 负责的. 读 log bool leveldb::log::Reader::ReadRecord(leveldb::Slice *record, string *scratch) 该方法负责从 log 文件读取内容并反序列化为 Record. 该方法会在 db 的 Open 方法中调用, 负责将磁盘上的 log 文件转换为内存中 memtable. 其它数据库恢复场景也会用到该方法. 与 log 文件配套的 memtablememtable 可以看作是 log 文件的内存形式, 但是格式不同. 结构它的本质就是一个 SkipList. 用途我们已经知道, 每个 log 文件在内存有一个对应的 memtable, 它和正在压实的 memtable 以及磁盘上的各个 level 包含的文件构成了数据全集. 所以当调用 DB 的 Get 方法查询某个 key 的时候, 具体步骤是这样的(具体实现位于 leveldb::Status leveldb::Version::Get(const leveldb::ReadOptions \u0026options, const leveldb::LookupKey \u0026k, string *value, leveldb::Version::GetStats *stats), DB 的 Get 方法会调用前述实现.): 先查询当前在用的 memtable, 查到返回, 未查到下一步 查询正在转换为 sorted string table 的 memtable 中寻找, 查到返回, 未查到下一步 在磁盘上采用从底向上 level-by-level 的寻找目标 key. 由于 level 越低数据越新, 因此, 当我们在一个较低的 level 找到数据的时候, 不用在更高的 levels 找了. 由于 level-0 文件之间可能存在重叠, 而且针对同一个 key, 后产生的文件数据更新所以先将包含 key 的文件找出来按照文件号从大到小(对应文件从新到老)排序查找 key; 针对 level-1 及其以上 level, 由于每个 level 内文件之间不存在重叠, 于是在每个 level 中直接采用二分查找定位 key. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#用途"},{"categories":null,"content":" sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L \u003e= 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: \u003cbeginning_of_file\u003e [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) \u003cend_of_file\u003e 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter.\u003cName\u003e 到 filter block 的 BlockHandle 的映射. 其中, \u003cName\u003e 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [f","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件"},{"categories":null,"content":" sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L \u003e= 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [f","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件格式"},{"categories":null,"content":" sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L \u003e= 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [f","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#filter-meta-block"},{"categories":null,"content":" sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L \u003e= 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [f","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#stats-meta-block"},{"categories":null,"content":" sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L \u003e= 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [f","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件主要接口"},{"categories":null,"content":" sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L \u003e= 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [f","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件读接口"},{"categories":null,"content":" sorted string table 文件sorted string table(*.ldb) 文件就是 leveldb 的数据库文件了. 每一个 level 都对应一组有序的 sorted string table 文件. 每个 sorted string table 文件保存着按 key 排序的一系列数据项. 每个数据项要么是一个与某个 key 对应的 value, 要么是某个 key 的删除标记. (删除标记其它地方又叫墓碑消息, 用于声明时间线上在此之前的同名 key 对应的记录都失效了, 后台线程负责对这类记录进行压实, 即拷贝到另一个文件时物理删除这类记录.). 注意, leveldb 是一个 append 类型而非 MySQL 那种 in-place 修改的数据库. sorted string tables 文件被组织成一系列 levels. 一个 log 文件生成的对应 sorted string table 文件会被放到一个特殊的 young level(也被叫做 level-0). 当 young 文件数目超过某个阈值(当前是 4), 全部 young 文件就会和 level-1 与之重叠的全部文件进行合并, 进而生成一系列新的 level-1 文件(每 2MB 数据就会生成一个新的 level-1 文件). level-0 的文件之间可能存在键区间重叠, 但是其它每层 level 内部文件之间是不存在重叠情况的. 我们下面来说下 level-1 及其以上的 level 的文件如何合并. 当 level-L (L \u003e= 1)的文件总大小超过了 $10^L$ MB(即 level-1 超过了 10MB, level-2 超过了 100MB, …), 此时一个 level-L 文件就会和 level-(L+1) 中与自己键区间重叠的全部文件进行合并, 然后为 level-(L+1) 生成一组新的文件. 这些合并操作可以实现将 young level 中新的 updates 一点一点搬到最高的那层 level, 这个迁移过程使用的都是块读写(最小化了昂贵的 seek 操作的时间消耗). sorted string table 文件格式leveldb sorted string table (又叫 sstable) 文件主要包含五个部分, 即多个 data blocks, 多个 meta blocks, 一个 metaindex block, 一个 index block 以及一个 footer, 具体格式如下: [data block 1] [data block 2] ... [data block N] [meta block 1] ... [meta block K] [metaindex block] [index block] [Footer] (fixed size; starts at file_size - sizeof(Footer)) 不像 kafka 存储结构数据文件和索引文件是各自独立的(在查询时索引文件用了根据具体 key 定位是哪个数据文件), 该文件把索引和数据保存到了一个文件中. 每次从文件查询数据时会先查询索引, 索引是指向数据的指针, 具体叫做 BlockHandle, 包含着下述信息: // 对应 block 起始位置在文件中的偏移量 offset: varint64 // 对应 block 的大小 size: varint64 如果你没用过 protobuf 之类的二进制编解码协议, 可能对 varint64 不太熟悉, 可以参考这里 varints 了解一下. 本质就是对数据类型进行(二次)无损编码, 使其更加紧凑, 可以节省带宽或者存储空间. 下面详细解释下上面提到的文件格式: 文件里存的是一系列 key/value 对, 而且按照 key 排过序了, 同时被划分到了多个 blocks 中. 这些 blocks 从文件起始位置开始一个接一个. 每个 data block 组织形式在 block_builder.cc 定义, 用户可以选择对 data block 进行压缩(注意前面讲 log 文件的时候说不支持对 block 进行压缩是 log 文件目前的缺点). 全部 data blocks 之后是一组 meta blocks. 已经支持的 meta block 类型见下面描述, 将来可能会加入更多的类型. 每个 meta block 组织形式在 block_builder.cc 定义, 同样地, 用户可以选择对其进行压缩. 全部 meta blocks 后是一个 metaindex block. 每个 meta block 都有一个对应的 entry 保存在该部分, 其中 key 就是某个 meta block 的名字, value 是一个指向该 meta block 的 BlockHandle. 紧随 metaindex block 之后是一个 index block. 针对每个 data block 都有一个对应的 entry 包含在该部分, 其中 key 为大于等于对应 data block 最后(也是最大的, 因为排序过了)一个 key 同时小于接下来的 data block 第一个 key 的字符串; value 是指向一个对应 data block 的 BlockHandle. 在每个文件的末尾是一个固定长度的 footer, 它非常关键, 它虽然位于文件尾部却是文件的入口. 固定长度的好处就是读取文件时, 用 file size 减去这个固定长度就能定位到 footer 起始偏移, 然后就可以解析了. 它包含了一个指向 metaindex block 的 BlockHandle 和一个指向 index block 的 BlockHandle 以及一个 magic number. 具体格式如下: // 指向 metaindex 的 BlockHandle metaindex_handle: char[p]; // 指向 index 的 BlockHandle index_handle: char[q]; // 用于维持固定长度的 padding 0, // (其中 40 == 2*BlockHandle::kMaxEncodedLength) padding: char[40-p-q]; // 具体内容为 0xdb4775248b80fb57 (小端字节序) magic: fixed64; 注意 footer 存的都是 index-of-xx, 找到 index 就可以找到 xx 了. “filter” Meta Block目前 sstable 只有一种类型的 meta block, 那就是 filter. 如果打开(创建)数据库的时候指定了一个 FilterPolicy, 那么一个 filter block 就会被存储到每个 sstable 中. metaindex block 包含了一个 entry, 它是从 filter. 到 filter block 的 BlockHandle 的映射. 其中, 是一个由 filter policy 的 Name()方法返回的字符串. filter block 保存着一系列 filters, 其中 filter i 包含了方法 void leveldb::FilterPolicy::CreateFilter(const Slice *keys, int n, string *dst) const 针对入参 keys 的计算结果(存储在输出型参数 dst). 参数 keys 属于一个 data block, 该 data block 对应的文件偏移量落在下面的范围里: [ i*base ... (i+1)*base-1 ] 当前, 上面的 base 是 2KB. 举个例子, 如果 block X 和 block Y 起始地址落在 [ 0KB .. 2KB ] 范围内, 则 X 和 Y 中的全部 keys 将会在调用 FilterPolicy::CreateFilter() 时被转换为一个 filter, 然后这个 filter 会作为第一个(为啥是第一个, 因为 X、Y 起始地址落在第一个地址空间 [ 0KB .. 2KB ] 里) filter 被保存在 filter block 中. (用大白话再说一遍, 每个 FilterPolicy 都有一个唯一的名字, 在 metaindex block 通过这个名字就能找到对应的 filter block 了. 而 filter block 存的就是用这个 FilterPolicy 构造的一系列 filters, 为啥是一系列呢？因为 data blocks 太多了, 所以分了区间, 每几个 data blocks 对应一个 filter, 具体几个根据上面那个带 base 的公式来算. 再说说 filter 是怎么回事. data block 保存的不是键值对构成的 records 嘛, 根据前面说的键区间限制, 把每几个 blocks 的全部键根据某个 FilterPolicy 算一下就得到了一个 filter, 然后把这个 filter 保存到了 filter block 的第 i 个位置. ) 具有 N 个 filter 的 filter block 格式如下: [filter 0] [f","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#sorted-string-table-文件写接口"},{"categories":null,"content":" MANIFEST 文件MANIFEST 文件可以看作 leveldb 存储元数据的地方. 它列出了每一个 level 及其包含的全部 sorted string table 文件, 每个 sorted string table 文件对应的键区间, 以及其它重要的元数据. 每当重新打开数据库的时候, 就会创建一个新的 MANIFEST 文件(文件名中嵌有一个新生成的数字). MANIFEST 文件被格式化成形同 log 文件的格式, 针对它所服务的数据的变更都会被追加到该文件后面. 比如每当某个 level 发生文件新增或者删除操作时, 就会有一条日志被追加到 MANIFEST 中. MANIFEST 文件在实现时又叫 descriptor 文件, 文件格式同 log 文件, 所以写入/读取方法就复用了. 其每条日志就是一个序列化后的 leveldb::VersionEdit. 每次针对 level 架构有文件增删时都要写日志到 manifest 文件. 与 MANIFEST 相关的数据结构之 VersionSet每个 db 都有一个 class leveldb::VersionSet 实例, 它保存了 db 当前的 level 架构视图(具体存储结构为其 Version 成员). MANIFEST 文件可以看作是它所维护的信息的反映. 它的重要方法有: VersionSet::Recover 负责在打开数据库时将 MANIFEST 文件反序列化构造 level 架构视图, 这个过程会依赖 VersionEdit 类. VersionSet::LogAndApply 负责将当前 VersionEdit 和当前 Version 进行合并, 然后序列化为一条日志记录到 MANIFEST 文件. 最后把新的 version 替换当前 version. 与 MANIFEST 相关的 Version 结构class leveldb::Version 是 leveldb 数据库 level 架构的内存表示, 它存储了每一个 level 及其全部的文件信息(文件名, 键范围等等). 每次调用 db 的 Get 方法在 memtable 找不到目标 key 时就会到各个 level 的文件去搜寻, 这个搜寻过程所依赖的就是数据库 VersionSet(下面介绍) 保存的当前 Version 存储的 level 架构信息进行的, 具体实现见 leveldb::Version::Get 方法. 当条件满足时, VersionSet 会将当前 Version 和当前 VesionEdit 合并生成一个新的 Version 替换当前 Version. 与 MANIFEST 相关的 VersionEditMANIFEST 文件的每一条日志就是一个序列化的 class leveldb::VersionEdit. 它可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#manifest-文件"},{"categories":null,"content":" MANIFEST 文件MANIFEST 文件可以看作 leveldb 存储元数据的地方. 它列出了每一个 level 及其包含的全部 sorted string table 文件, 每个 sorted string table 文件对应的键区间, 以及其它重要的元数据. 每当重新打开数据库的时候, 就会创建一个新的 MANIFEST 文件(文件名中嵌有一个新生成的数字). MANIFEST 文件被格式化成形同 log 文件的格式, 针对它所服务的数据的变更都会被追加到该文件后面. 比如每当某个 level 发生文件新增或者删除操作时, 就会有一条日志被追加到 MANIFEST 中. MANIFEST 文件在实现时又叫 descriptor 文件, 文件格式同 log 文件, 所以写入/读取方法就复用了. 其每条日志就是一个序列化后的 leveldb::VersionEdit. 每次针对 level 架构有文件增删时都要写日志到 manifest 文件. 与 MANIFEST 相关的数据结构之 VersionSet每个 db 都有一个 class leveldb::VersionSet 实例, 它保存了 db 当前的 level 架构视图(具体存储结构为其 Version 成员). MANIFEST 文件可以看作是它所维护的信息的反映. 它的重要方法有: VersionSet::Recover 负责在打开数据库时将 MANIFEST 文件反序列化构造 level 架构视图, 这个过程会依赖 VersionEdit 类. VersionSet::LogAndApply 负责将当前 VersionEdit 和当前 Version 进行合并, 然后序列化为一条日志记录到 MANIFEST 文件. 最后把新的 version 替换当前 version. 与 MANIFEST 相关的 Version 结构class leveldb::Version 是 leveldb 数据库 level 架构的内存表示, 它存储了每一个 level 及其全部的文件信息(文件名, 键范围等等). 每次调用 db 的 Get 方法在 memtable 找不到目标 key 时就会到各个 level 的文件去搜寻, 这个搜寻过程所依赖的就是数据库 VersionSet(下面介绍) 保存的当前 Version 存储的 level 架构信息进行的, 具体实现见 leveldb::Version::Get 方法. 当条件满足时, VersionSet 会将当前 Version 和当前 VesionEdit 合并生成一个新的 Version 替换当前 Version. 与 MANIFEST 相关的 VersionEditMANIFEST 文件的每一条日志就是一个序列化的 class leveldb::VersionEdit. 它可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#与-manifest-相关的数据结构之-versionset"},{"categories":null,"content":" MANIFEST 文件MANIFEST 文件可以看作 leveldb 存储元数据的地方. 它列出了每一个 level 及其包含的全部 sorted string table 文件, 每个 sorted string table 文件对应的键区间, 以及其它重要的元数据. 每当重新打开数据库的时候, 就会创建一个新的 MANIFEST 文件(文件名中嵌有一个新生成的数字). MANIFEST 文件被格式化成形同 log 文件的格式, 针对它所服务的数据的变更都会被追加到该文件后面. 比如每当某个 level 发生文件新增或者删除操作时, 就会有一条日志被追加到 MANIFEST 中. MANIFEST 文件在实现时又叫 descriptor 文件, 文件格式同 log 文件, 所以写入/读取方法就复用了. 其每条日志就是一个序列化后的 leveldb::VersionEdit. 每次针对 level 架构有文件增删时都要写日志到 manifest 文件. 与 MANIFEST 相关的数据结构之 VersionSet每个 db 都有一个 class leveldb::VersionSet 实例, 它保存了 db 当前的 level 架构视图(具体存储结构为其 Version 成员). MANIFEST 文件可以看作是它所维护的信息的反映. 它的重要方法有: VersionSet::Recover 负责在打开数据库时将 MANIFEST 文件反序列化构造 level 架构视图, 这个过程会依赖 VersionEdit 类. VersionSet::LogAndApply 负责将当前 VersionEdit 和当前 Version 进行合并, 然后序列化为一条日志记录到 MANIFEST 文件. 最后把新的 version 替换当前 version. 与 MANIFEST 相关的 Version 结构class leveldb::Version 是 leveldb 数据库 level 架构的内存表示, 它存储了每一个 level 及其全部的文件信息(文件名, 键范围等等). 每次调用 db 的 Get 方法在 memtable 找不到目标 key 时就会到各个 level 的文件去搜寻, 这个搜寻过程所依赖的就是数据库 VersionSet(下面介绍) 保存的当前 Version 存储的 level 架构信息进行的, 具体实现见 leveldb::Version::Get 方法. 当条件满足时, VersionSet 会将当前 Version 和当前 VesionEdit 合并生成一个新的 Version 替换当前 Version. 与 MANIFEST 相关的 VersionEditMANIFEST 文件的每一条日志就是一个序列化的 class leveldb::VersionEdit. 它可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#与-manifest-相关的-version-结构"},{"categories":null,"content":" MANIFEST 文件MANIFEST 文件可以看作 leveldb 存储元数据的地方. 它列出了每一个 level 及其包含的全部 sorted string table 文件, 每个 sorted string table 文件对应的键区间, 以及其它重要的元数据. 每当重新打开数据库的时候, 就会创建一个新的 MANIFEST 文件(文件名中嵌有一个新生成的数字). MANIFEST 文件被格式化成形同 log 文件的格式, 针对它所服务的数据的变更都会被追加到该文件后面. 比如每当某个 level 发生文件新增或者删除操作时, 就会有一条日志被追加到 MANIFEST 中. MANIFEST 文件在实现时又叫 descriptor 文件, 文件格式同 log 文件, 所以写入/读取方法就复用了. 其每条日志就是一个序列化后的 leveldb::VersionEdit. 每次针对 level 架构有文件增删时都要写日志到 manifest 文件. 与 MANIFEST 相关的数据结构之 VersionSet每个 db 都有一个 class leveldb::VersionSet 实例, 它保存了 db 当前的 level 架构视图(具体存储结构为其 Version 成员). MANIFEST 文件可以看作是它所维护的信息的反映. 它的重要方法有: VersionSet::Recover 负责在打开数据库时将 MANIFEST 文件反序列化构造 level 架构视图, 这个过程会依赖 VersionEdit 类. VersionSet::LogAndApply 负责将当前 VersionEdit 和当前 Version 进行合并, 然后序列化为一条日志记录到 MANIFEST 文件. 最后把新的 version 替换当前 version. 与 MANIFEST 相关的 Version 结构class leveldb::Version 是 leveldb 数据库 level 架构的内存表示, 它存储了每一个 level 及其全部的文件信息(文件名, 键范围等等). 每次调用 db 的 Get 方法在 memtable 找不到目标 key 时就会到各个 level 的文件去搜寻, 这个搜寻过程所依赖的就是数据库 VersionSet(下面介绍) 保存的当前 Version 存储的 level 架构信息进行的, 具体实现见 leveldb::Version::Get 方法. 当条件满足时, VersionSet 会将当前 Version 和当前 VesionEdit 合并生成一个新的 Version 替换当前 Version. 与 MANIFEST 相关的 VersionEditMANIFEST 文件的每一条日志就是一个序列化的 class leveldb::VersionEdit. 它可以看作一个 on-fly 的 Version. 它会记录 db 运行过程中删除的文件列表和新增的文件列表. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#与-manifest-相关的-versionedit"},{"categories":null,"content":" CURRENT 文件CURRENT 文件是一个简单的文本文件. 由于每次重新打开数据库都会生成一个 MANIFEST 文件, 所以需要一个地方记录最新的 MANIFEST 文件是哪个, CURRENT 就干这个事情, 它相当于一个指针, 其内容即是当前最新的 MANIFEST 文件的名称. ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#current-文件"},{"categories":null,"content":" 文件位置与命名各类型文件位置与命名如下: dbname/CURRENT dbname/LOCK dbname/LOG dbname/LOG.old dbname/MANIFEST-[0-9]+ dbname/[0-9]+.(log|sst|ldb) 其中 dbname 为用户指定. –End– ","date":"2020-09-11","objectID":"/leveldb-annotations-1-interfaces-and-files/:3:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之一: 接口与文件","uri":"/leveldb-annotations-1-interfaces-and-files/#文件位置与命名"},{"categories":null,"content":"Leveldb 是一个高速 KV 数据库, 它提供了一个持久性的 KV 存储. 其中 keys 和 values 都是随机字节数组, 并且存储时根据用户指定的比较函数对 keys 进行排序. 它由 Google 开发的, 其作者为大名鼎鼎的 Sanjay Ghemawat (sanjay@google.com) 和 Jeff Dean (jeff@google.com). 感谢他们对人类的贡献. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:0:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#"},{"categories":null,"content":" 基本介绍该部分主要介绍 leveldb 的功能, 局限性以及性能等. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#基本介绍"},{"categories":null,"content":" 特性 keys 和 values 都可以是随机的字节数组. 数据被按照 key 的顺序进行存储. 调用者可以提供一个定制的比较函数来覆盖默认的比较器. 基础操作有 Put(key,value), Get(key), Delete(key). 多个更改可以在一个原子批处理中一起生效. 用户可以创建一个瞬时快照来获取数据的一致性视图. 支持针对数据的前向和后向遍历. 数据通过 Snappy 压缩程序库自动压缩. 与外部交互的操作都被抽象成了接口(如文件系统操作等), 所以用户可以根据接口定制自己期望的操作系统交互行为. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#特性"},{"categories":null,"content":" 局限性 LevelDB 不是 SQL 数据库. 它没有关系数据模型, 不支持 SQL 查询, 也不支持索引. 同时只能有一个进程(可能是具有多线程的进程)访问一个特定的数据库. 该程序库没有内置基于网络的 CS 架构, 有需求的用户可以自己封装. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#局限性"},{"categories":null,"content":" 性能下面是通过运行 db_bench 程序得出的性能测试报告. 测试配置我们使用的是一个包含一百万数据项的数据库, 其中 key 是 16 字节, value 是 100 字节, value 压缩后大约是原来的一半, 测试配置如下: LevelDB: version 1.1 Date: Sun May 1 12:11:26 2011 CPU: 4 x Intel(R) Core(TM)2 Quad CPU Q6600 @ 2.40GHz CPUCache: 4096 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 Raw Size: 110.6 MB (estimated) File Size: 62.9 MB (estimated) 写性能“fill” 基准测试创建了一个全新的数据库, 以顺序(下面 “seq” 结尾者)或者随机(下面 “random” 结尾者)方式写入. “fillsync” 基准测试每次写操作都将数据从操作系统刷到磁盘; 其它的操作会将数据保存在系统中一段时间. “overwrite” 基准测试做随机写, 这些操作会更新数据库中已有的键. fillseq : 1.765 micros/op; 62.7 MB/s fillsync : 268.409 micros/op; 0.4 MB/s (10000 ops) fillrandom : 2.460 micros/op; 45.0 MB/s overwrite : 2.380 micros/op; 46.5 MB/s 上述每个 “op” 对应一个 key/value 对的写操作. 也就是说, 一个随机写基准测试每秒大约进行四十万次写操作(1,000,000/2.46). 每个 “fillsync” 操作时间消耗(大约 0.3 毫秒)少于一次磁盘寻道(大约 10 毫秒). 我们怀疑这是因为磁盘本身将更新操作缓存到了内存, 并且在数据真正落盘前返回响应. 该方式是否安全取决于断电后磁盘是否有备用电力将数据落盘. 读性能我们分别给出正向顺序读、反向顺序读的性能以及随机查询的性能指标. 注意, 基准测试创建的数据库很小. 因此该性能报告描述的是 leveldb 的全部数据集能放入到内存的场景. 如果数据不在操作系统缓存中, 读取一点数据的性能消耗主要在于一到两次的磁盘寻道. 写性能基本不会受数据集是否能放入内存的影响. readrandom : 16.677 micros/op; (approximately 60,000 reads per second) readseq : 0.476 micros/op; 232.3 MB/s readreverse : 0.724 micros/op; 152.9 MB/s LevelDB 会在后台压实底层的数据来改善读性能. 上面列出的结果是在经过一系列随机写操作后得出的. 如果经过压实(通常是自动触发), 那么上述指标会更好. readrandom : 11.602 micros/op; (approximately 85,000 reads per second) readseq : 0.423 micros/op; 261.8 MB/s readreverse : 0.663 micros/op; 166.9 MB/s 读操作消耗高的地方有一些来自重复解压从磁盘读取的数据块. 如果我们能提供足够的缓存给 leveldb 来将解压后的数据保存在内存中, 读性能会进一步改善: readrandom : 9.775 micros/op; (approximately 100,000 reads per second before compaction) readrandom : 5.215 micros/op; (approximately 190,000 reads per second after compaction) ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#性能"},{"categories":null,"content":" 性能下面是通过运行 db_bench 程序得出的性能测试报告. 测试配置我们使用的是一个包含一百万数据项的数据库, 其中 key 是 16 字节, value 是 100 字节, value 压缩后大约是原来的一半, 测试配置如下: LevelDB: version 1.1 Date: Sun May 1 12:11:26 2011 CPU: 4 x Intel(R) Core(TM)2 Quad CPU Q6600 @ 2.40GHz CPUCache: 4096 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 Raw Size: 110.6 MB (estimated) File Size: 62.9 MB (estimated) 写性能“fill” 基准测试创建了一个全新的数据库, 以顺序(下面 “seq” 结尾者)或者随机(下面 “random” 结尾者)方式写入. “fillsync” 基准测试每次写操作都将数据从操作系统刷到磁盘; 其它的操作会将数据保存在系统中一段时间. “overwrite” 基准测试做随机写, 这些操作会更新数据库中已有的键. fillseq : 1.765 micros/op; 62.7 MB/s fillsync : 268.409 micros/op; 0.4 MB/s (10000 ops) fillrandom : 2.460 micros/op; 45.0 MB/s overwrite : 2.380 micros/op; 46.5 MB/s 上述每个 “op” 对应一个 key/value 对的写操作. 也就是说, 一个随机写基准测试每秒大约进行四十万次写操作(1,000,000/2.46). 每个 “fillsync” 操作时间消耗(大约 0.3 毫秒)少于一次磁盘寻道(大约 10 毫秒). 我们怀疑这是因为磁盘本身将更新操作缓存到了内存, 并且在数据真正落盘前返回响应. 该方式是否安全取决于断电后磁盘是否有备用电力将数据落盘. 读性能我们分别给出正向顺序读、反向顺序读的性能以及随机查询的性能指标. 注意, 基准测试创建的数据库很小. 因此该性能报告描述的是 leveldb 的全部数据集能放入到内存的场景. 如果数据不在操作系统缓存中, 读取一点数据的性能消耗主要在于一到两次的磁盘寻道. 写性能基本不会受数据集是否能放入内存的影响. readrandom : 16.677 micros/op; (approximately 60,000 reads per second) readseq : 0.476 micros/op; 232.3 MB/s readreverse : 0.724 micros/op; 152.9 MB/s LevelDB 会在后台压实底层的数据来改善读性能. 上面列出的结果是在经过一系列随机写操作后得出的. 如果经过压实(通常是自动触发), 那么上述指标会更好. readrandom : 11.602 micros/op; (approximately 85,000 reads per second) readseq : 0.423 micros/op; 261.8 MB/s readreverse : 0.663 micros/op; 166.9 MB/s 读操作消耗高的地方有一些来自重复解压从磁盘读取的数据块. 如果我们能提供足够的缓存给 leveldb 来将解压后的数据保存在内存中, 读性能会进一步改善: readrandom : 9.775 micros/op; (approximately 100,000 reads per second before compaction) readrandom : 5.215 micros/op; (approximately 190,000 reads per second after compaction) ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#测试配置"},{"categories":null,"content":" 性能下面是通过运行 db_bench 程序得出的性能测试报告. 测试配置我们使用的是一个包含一百万数据项的数据库, 其中 key 是 16 字节, value 是 100 字节, value 压缩后大约是原来的一半, 测试配置如下: LevelDB: version 1.1 Date: Sun May 1 12:11:26 2011 CPU: 4 x Intel(R) Core(TM)2 Quad CPU Q6600 @ 2.40GHz CPUCache: 4096 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 Raw Size: 110.6 MB (estimated) File Size: 62.9 MB (estimated) 写性能“fill” 基准测试创建了一个全新的数据库, 以顺序(下面 “seq” 结尾者)或者随机(下面 “random” 结尾者)方式写入. “fillsync” 基准测试每次写操作都将数据从操作系统刷到磁盘; 其它的操作会将数据保存在系统中一段时间. “overwrite” 基准测试做随机写, 这些操作会更新数据库中已有的键. fillseq : 1.765 micros/op; 62.7 MB/s fillsync : 268.409 micros/op; 0.4 MB/s (10000 ops) fillrandom : 2.460 micros/op; 45.0 MB/s overwrite : 2.380 micros/op; 46.5 MB/s 上述每个 “op” 对应一个 key/value 对的写操作. 也就是说, 一个随机写基准测试每秒大约进行四十万次写操作(1,000,000/2.46). 每个 “fillsync” 操作时间消耗(大约 0.3 毫秒)少于一次磁盘寻道(大约 10 毫秒). 我们怀疑这是因为磁盘本身将更新操作缓存到了内存, 并且在数据真正落盘前返回响应. 该方式是否安全取决于断电后磁盘是否有备用电力将数据落盘. 读性能我们分别给出正向顺序读、反向顺序读的性能以及随机查询的性能指标. 注意, 基准测试创建的数据库很小. 因此该性能报告描述的是 leveldb 的全部数据集能放入到内存的场景. 如果数据不在操作系统缓存中, 读取一点数据的性能消耗主要在于一到两次的磁盘寻道. 写性能基本不会受数据集是否能放入内存的影响. readrandom : 16.677 micros/op; (approximately 60,000 reads per second) readseq : 0.476 micros/op; 232.3 MB/s readreverse : 0.724 micros/op; 152.9 MB/s LevelDB 会在后台压实底层的数据来改善读性能. 上面列出的结果是在经过一系列随机写操作后得出的. 如果经过压实(通常是自动触发), 那么上述指标会更好. readrandom : 11.602 micros/op; (approximately 85,000 reads per second) readseq : 0.423 micros/op; 261.8 MB/s readreverse : 0.663 micros/op; 166.9 MB/s 读操作消耗高的地方有一些来自重复解压从磁盘读取的数据块. 如果我们能提供足够的缓存给 leveldb 来将解压后的数据保存在内存中, 读性能会进一步改善: readrandom : 9.775 micros/op; (approximately 100,000 reads per second before compaction) readrandom : 5.215 micros/op; (approximately 190,000 reads per second after compaction) ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#写性能"},{"categories":null,"content":" 性能下面是通过运行 db_bench 程序得出的性能测试报告. 测试配置我们使用的是一个包含一百万数据项的数据库, 其中 key 是 16 字节, value 是 100 字节, value 压缩后大约是原来的一半, 测试配置如下: LevelDB: version 1.1 Date: Sun May 1 12:11:26 2011 CPU: 4 x Intel(R) Core(TM)2 Quad CPU Q6600 @ 2.40GHz CPUCache: 4096 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 Raw Size: 110.6 MB (estimated) File Size: 62.9 MB (estimated) 写性能“fill” 基准测试创建了一个全新的数据库, 以顺序(下面 “seq” 结尾者)或者随机(下面 “random” 结尾者)方式写入. “fillsync” 基准测试每次写操作都将数据从操作系统刷到磁盘; 其它的操作会将数据保存在系统中一段时间. “overwrite” 基准测试做随机写, 这些操作会更新数据库中已有的键. fillseq : 1.765 micros/op; 62.7 MB/s fillsync : 268.409 micros/op; 0.4 MB/s (10000 ops) fillrandom : 2.460 micros/op; 45.0 MB/s overwrite : 2.380 micros/op; 46.5 MB/s 上述每个 “op” 对应一个 key/value 对的写操作. 也就是说, 一个随机写基准测试每秒大约进行四十万次写操作(1,000,000/2.46). 每个 “fillsync” 操作时间消耗(大约 0.3 毫秒)少于一次磁盘寻道(大约 10 毫秒). 我们怀疑这是因为磁盘本身将更新操作缓存到了内存, 并且在数据真正落盘前返回响应. 该方式是否安全取决于断电后磁盘是否有备用电力将数据落盘. 读性能我们分别给出正向顺序读、反向顺序读的性能以及随机查询的性能指标. 注意, 基准测试创建的数据库很小. 因此该性能报告描述的是 leveldb 的全部数据集能放入到内存的场景. 如果数据不在操作系统缓存中, 读取一点数据的性能消耗主要在于一到两次的磁盘寻道. 写性能基本不会受数据集是否能放入内存的影响. readrandom : 16.677 micros/op; (approximately 60,000 reads per second) readseq : 0.476 micros/op; 232.3 MB/s readreverse : 0.724 micros/op; 152.9 MB/s LevelDB 会在后台压实底层的数据来改善读性能. 上面列出的结果是在经过一系列随机写操作后得出的. 如果经过压实(通常是自动触发), 那么上述指标会更好. readrandom : 11.602 micros/op; (approximately 85,000 reads per second) readseq : 0.423 micros/op; 261.8 MB/s readreverse : 0.663 micros/op; 166.9 MB/s 读操作消耗高的地方有一些来自重复解压从磁盘读取的数据块. 如果我们能提供足够的缓存给 leveldb 来将解压后的数据保存在内存中, 读性能会进一步改善: readrandom : 9.775 micros/op; (approximately 100,000 reads per second before compaction) readrandom : 5.215 micros/op; (approximately 190,000 reads per second after compaction) ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:1:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#读性能"},{"categories":null,"content":" 使用举例下面从构建和头文件介绍开始, 对 leveldb 的基本使用进行介绍. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:0","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#使用举例"},{"categories":null,"content":" 构建该工程开箱支持 CMake. 所以构建起来超简单: $ mkdir -p build \u0026\u0026 cd build $ cmake -DCMAKE_BUILD_TYPE=Release .. \u0026\u0026 cmake --build . $ sudo make install 更多高级用法请请参照 CMake 文档和本项目的 CMakeLists.txt. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:1","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#构建"},{"categories":null,"content":" 头文件介绍LevelDB 对外的接口都包含在 include/*.h 中. 除了该目录下的文件, 用户不应该依赖其它目录下任何文件. include/db.h: 主要的接口在这, 使用 leveldb 从这里开始. include/options.h: 使用 leveldb 过程各种操作包括读写有关的控制参数. include/comparator.h: 比较函数的抽象, 如果你想用逐字节比较 key 那么可以直接使用默认的比较器. 如果你想定制排序逻辑(如处理不同的字符编解码等)可以定制自己的比较函数. include/iterator.h: 迭代数据的接口. 你可以从一个 DB 对象获取到一个迭代器. include/write_batch.h: 原子地应用多个更新到一个数据库. include/slice.h: 类似 string, 维护着指向字节数组的一个指针和相应长度. include/status.h: 许多公共接口都会返回 Status, 用于指示成功或其它各种错误. include/env.h: 操作系统环境的抽象. 在 util/env_posix.cc 中有一个该接口的 posix 实现. include/table.h, include/table_builder.h: 底层的模块, 大多数客户端可能不会直接用到. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:2","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#头文件介绍"},{"categories":null,"content":" 打开(或新建)一个数据库leveldb 数据库都有一个名字, 该名字对应了文件系统上一个目录, 而且该数据库内容全都存在该目录下. 下面的例子显示了如何打开一个数据库以及在必要情况下创建之. #include \u003ccassert\u003e #include \"leveldb/db.h\" leveldb::DB* db; leveldb::Options options; options.create_if_missing = true; leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); assert(status.ok()); ... 如果你想在数据库已存在的时候触发一个异常, 将下面这行加到 leveldb::DB::Open 调用之前: options.error_if_exists = true; ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:3","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#打开或新建一个数据库"},{"categories":null,"content":" Status 类型你可能注意到上面的 leveldb::Status 类型了. leveldb 中大部分方法在遇到错误的时候会返回该类型的值. 你可以检查它是否为 ok, 然后打印关联的错误信息即可: leveldb::Status s = ...; if (!s.ok()) cerr \u003c\u003c s.ToString() \u003c\u003c endl; ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:4","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#status-类型"},{"categories":null,"content":" 关闭数据库当数据库不再使用的时候, 像下面这样直接删除数据库对象就可以了: ... open the db as described above ... ... do something with db ... delete db; 非常简单是不是? 因为 DB 类的实现是基于 RAII 的, 在 delete 时触发析构方法自动进行清理工作. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:5","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#关闭数据库"},{"categories":null,"content":" 数据库读写数据库提供了 Put、Delete 以及 Get 方法来修改、查询数据库. 下面的代码展示了将 key1 对应的 value 移动(先拷贝后删除)到 key2 下. std::string value; leveldb::Status s = db-\u003eGet(leveldb::ReadOptions(), key1, \u0026value); if (s.ok()) s = db-\u003ePut(leveldb::WriteOptions(), key2, value); if (s.ok()) s = db-\u003eDelete(leveldb::WriteOptions(), key1); ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:6","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#数据库读写"},{"categories":null,"content":" 原子更新注意, 上一小节中如果进程在 Put 了 key2 之后但是删除 key1 之前挂了, 那么同样的 value 就出现在了多个 keys 之下. 该问题可以通过使用 WriteBatch 类原子地应用一组操作来避免. #include \"leveldb/write_batch.h\" ... std::string value; leveldb::Status s = db-\u003eGet(leveldb::ReadOptions(), key1, \u0026value); if (s.ok()) { leveldb::WriteBatch batch; batch.Delete(key1); batch.Put(key2, value); s = db-\u003eWrite(leveldb::WriteOptions(), \u0026batch); } WriteBatch 保存着一系列将被应用到数据库的编辑操作, 这些操作会按照添加的顺序依次被执行. 注意, 我们先执行 Delete 后执行 Put, 这样如果 key1 和 key2 一样的情况下我们也不会错误地丢失数据. 除了原子性, WriteBatch 也能加速更新过程, 因为可以把一大批独立的操作添加到同一个 batch 中然后一次性执行. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:7","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#原子更新"},{"categories":null,"content":" 同步写操作默认地, leveldb 每个写操作都是异步的: 进程把要写的内容 push 给操作系统后立马返回. 从操作系统内存到底层持久性存储的迁移异步地发生. 当然, 也可以把某个写操作的 sync 标识打开, 以等到数据真正被记录到持久化存储再让写操作返回. (在 Posix 系统上, 这是通过在写操作返回前调用 fsync(...) 或 fdatasync(...) 或 msync(..., MS_SYNC) 来实现的. ) leveldb::WriteOptions write_options; write_options.sync = true; db-\u003ePut(write_options, ...); 异步写通常比同步写快 1000 倍. 异步写的缺点是, 一旦机器崩溃可能会导致最后几个更新操作丢失. 注意, 仅仅是写进程崩溃(而非机器重启)则不会引起任何更新操作丢失, 因为哪怕 sync 标识为 false, 在进程退出之前写操作也已经从进程内存 push 到了操作系统. 异步写总是可以安全使用. 比如你要将大量的数据写入数据库, 如果丢失了最后几个更新操作, 你可以重启整个写过程. 如果数据量非常大, 一个优化点是, 每进行 N 个异步写操作则进行一次同步地写操作, 如果期间发生了崩溃, 重启自从上一个成功的同步写操作以来的更新操作即可. (同步的写操作可以同时更新一个标识, 该标识用于描述崩溃重启后从何处开始重启更新操作. ) WriteBatch 可以作为异步写操作的替代品. 多个更新操作可以放到同一个 WriteBatch 中然后通过一次同步写(即 write_options.sync 置为 true)一起应用. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:8","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#同步写操作"},{"categories":null,"content":" 并发一个数据库同时只能被一个进程打开. LevelDB 会从操作系统获取一把锁来防止多进程同时打开同一个数据库. 在单个进程中, 同一个 leveldb::DB 对象可以被多个并发的线程安全地使用, 也就是说, 不同的线程可以写入或者获取迭代器, 或者针对同一个数据库调用 Get, 前述全部操作均不需要借助外部同步设施(leveldb 实现会自动地确保必要的同步). 但是其它对象, 比如 Iterator 或者 WriteBatch 需要外部自己提供同步保证. 如果两个线程共享此类对象, 需要使用锁进行互斥访问. 具体见对应的头文件. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:9","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#并发"},{"categories":null,"content":" 迭代数据库下面的用例展示了如何打印数据库中全部的 (key, value) 对. leveldb::Iterator* it = db-\u003eNewIterator(leveldb::ReadOptions()); for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { cout \u003c\u003c it-\u003ekey().ToString() \u003c\u003c \": \" \u003c\u003c it-\u003evalue().ToString() \u003c\u003c endl; } assert(it-\u003estatus().ok()); // Check for any errors found during the scan delete it; 下面的用例展示了如何打印 [start, limit) 范围内数据: for (it-\u003eSeek(start); it-\u003eValid() \u0026\u0026 it-\u003ekey().ToString() \u003c limit; it-\u003eNext()) { ... } 当然你也可以反向遍历(注意, 反向遍历可能比正向遍历要慢一些, 具体见前面的读性能基准测试). for (it-\u003eSeekToLast(); it-\u003eValid(); it-\u003ePrev()) { ... } ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:10","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#迭代数据库"},{"categories":null,"content":" 快照快照提供了针对整个 KV 存储的一致性只读视图. ReadOptions::snapshot 不为空表示读操作应该作用在 DB 的某个特定版本上; 若为空, 则读操作将会作用在当前版本的一个隐式的快照上. 快照通过调用 DB::GetSnapshot() 方法创建: leveldb::ReadOptions options; options.snapshot = db-\u003eGetSnapshot(); ... apply some updates to db ... // 获取与前面快照对应的数据库迭代器 leveldb::Iterator* iter = db-\u003eNewIterator(options); ... read using iter to view the state when the snapshot was created ... delete iter; db-\u003eReleaseSnapshot(options.snapshot); 注意, 当一个快照不再使用的时候, 应该通过 DB::ReleaseSnapshot 接口进行释放. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:11","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#快照"},{"categories":null,"content":" Slice 切片it-\u003ekey() 和 it-\u003evalue() 调用返回的值是 leveldb::Slice 类型的实例. 熟悉 Golang 或者 Rust 的同学对 slice 应该不陌生. slice 是一个简单的数据结构, 包含一个长度和一个指向外部字节数组的指针. 返回一个切片比返回 std::string 更加高效, 因为不需要隐式地拷贝大量的 keys 和 values. 另外, leveldb 方法不返回空字符结尾的 C 风格地字符串, 因为 leveldb 的 keys 和 values 允许包含 \\0 字节. C++ 风格的 string 和 C 风格的空字符结尾的字符串很容易转换为一个切片: leveldb::Slice s1 = \"hello\"; std::string str(\"world\"); leveldb::Slice s2 = str; 一个切片也很容易转换回 C++ 风格的字符串: std::string str = s1.ToString(); assert(str == std::string(\"hello\")); 注意, 当使用切片时, 调用者要确保它内部指针指向的外部字节数组保持存活. 比如, 下面的代码就有问题: leveldb::Slice slice; if (...) { std::string str = ...; slice = str; } Use(slice); 当 if 语句结束的时候, str 将会被销毁, 切片的底层存储也随之消失了, 后面再用就出问题了. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:12","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#slice-切片"},{"categories":null,"content":" 比较器前面的例子中用的都是默认的比较函数, 即逐字节按照字典序比较. 你可以定制自己的比较函数, 然后在打开数据库的时候传入. 只需继承 leveldb::Comparator 然后定义相关逻辑即可, 下面是一个例子: class TwoPartComparator : public leveldb::Comparator { public: // Three-way comparison function: // if a \u003c b: negative result // if a \u003e b: positive result // else: zero result int Compare(const leveldb::Slice\u0026 a, const leveldb::Slice\u0026 b) const { int a1, a2, b1, b2; ParseKey(a, \u0026a1, \u0026a2); ParseKey(b, \u0026b1, \u0026b2); if (a1 \u003c b1) return -1; if (a1 \u003e b1) return +1; if (a2 \u003c b2) return -1; if (a2 \u003e b2) return +1; return 0; } // Ignore the following methods for now: const char* Name() const { return \"TwoPartComparator\"; } void FindShortestSeparator(std::string*, const leveldb::Slice\u0026) const {} void FindShortSuccessor(std::string*) const {} }; 然后使用上面定义的比较器打开数据库: // 实例化比较器 TwoPartComparator cmp; leveldb::DB* db; leveldb::Options options; options.create_if_missing = true; // 将比较器赋值给 options.comparator options.comparator = \u0026cmp; // 打开数据库 leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... 后向兼容性比较器 Name 方法返回的结果在创建数据库时会被绑定到数据库上, 后续每次打开都会进行检查. 如果名称改了, 对 leveldb::DB::Open 的调用就会失败. 因此, 当且仅当在新的 key 格式和比较函数与已有的数据库不兼容而且已有数据不再被需要的时候再修改比较器名称. 总而言之, 一个数据库只能对应一个比较器, 而且比较器由名字唯一确定, 一旦修改名称或者比较器逻辑, 数据库的操作逻辑就统统会出错, 毕竟 leveldb 是一个有序的 KV 存储. 如果非要修改比较逻辑怎么办呢? 你可以根据预先规划一点一点的演进你的 key 格式, 注意, 事先的演进规划非常重要. 比如, 你可以存储一个版本号在每个 key 的结尾(大多数场景, 一个字节足够了). 当你想要切换到新的 key 格式的时候(比如新增 third-part 到上面例子 TwoPartComparator 处理的 keys 中), 那么你需要做的是: (a) 保持比较器名称不变 (b) 递增新 keys 的版本号 (c) 修改比较器函数以让其使用版本号来决定如何进行排序. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:13","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#比较器"},{"categories":null,"content":" 比较器前面的例子中用的都是默认的比较函数, 即逐字节按照字典序比较. 你可以定制自己的比较函数, 然后在打开数据库的时候传入. 只需继承 leveldb::Comparator 然后定义相关逻辑即可, 下面是一个例子: class TwoPartComparator : public leveldb::Comparator { public: // Three-way comparison function: // if a \u003c b: negative result // if a \u003e b: positive result // else: zero result int Compare(const leveldb::Slice\u0026 a, const leveldb::Slice\u0026 b) const { int a1, a2, b1, b2; ParseKey(a, \u0026a1, \u0026a2); ParseKey(b, \u0026b1, \u0026b2); if (a1 \u003c b1) return -1; if (a1 \u003e b1) return +1; if (a2 \u003c b2) return -1; if (a2 \u003e b2) return +1; return 0; } // Ignore the following methods for now: const char* Name() const { return \"TwoPartComparator\"; } void FindShortestSeparator(std::string*, const leveldb::Slice\u0026) const {} void FindShortSuccessor(std::string*) const {} }; 然后使用上面定义的比较器打开数据库: // 实例化比较器 TwoPartComparator cmp; leveldb::DB* db; leveldb::Options options; options.create_if_missing = true; // 将比较器赋值给 options.comparator options.comparator = \u0026cmp; // 打开数据库 leveldb::Status status = leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... 后向兼容性比较器 Name 方法返回的结果在创建数据库时会被绑定到数据库上, 后续每次打开都会进行检查. 如果名称改了, 对 leveldb::DB::Open 的调用就会失败. 因此, 当且仅当在新的 key 格式和比较函数与已有的数据库不兼容而且已有数据不再被需要的时候再修改比较器名称. 总而言之, 一个数据库只能对应一个比较器, 而且比较器由名字唯一确定, 一旦修改名称或者比较器逻辑, 数据库的操作逻辑就统统会出错, 毕竟 leveldb 是一个有序的 KV 存储. 如果非要修改比较逻辑怎么办呢? 你可以根据预先规划一点一点的演进你的 key 格式, 注意, 事先的演进规划非常重要. 比如, 你可以存储一个版本号在每个 key 的结尾(大多数场景, 一个字节足够了). 当你想要切换到新的 key 格式的时候(比如新增 third-part 到上面例子 TwoPartComparator 处理的 keys 中), 那么你需要做的是: (a) 保持比较器名称不变 (b) 递增新 keys 的版本号 (c) 修改比较器函数以让其使用版本号来决定如何进行排序. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:13","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#后向兼容性"},{"categories":null,"content":" 性能调优通过修改 include/leveldb/options.h 中定义的类型的默认值来对 leveldb 的性能进行调优. Block 大小Leveldb 把相邻的 keys 组织在同一个 block 中(具体见后面文章针对 sorted string table 文件格式的描述), 而且 block 是 leveldb 把数据从内存到转移到持久化存储和从持久化存储转移到内存的基本单位. 默认的, 压缩前 block 大约为 4KB. 经常处理大块数据的应用可能希望把这个值调大, 而针对数据做\"点读\" 的应用可能希望这个值小一点, 这样性能可能会更高一些. 但是, 没有证据表明该值小于 1KB 或者大于几个 MB 的时候性能会表现更好. 同时要注意, 针对大的 block size, 压缩效率会更高一些. 压缩每个 block 在写入持久存储之前都会被单独压缩. 压缩默认是开启的, 因为默认的压缩算法非常快, 而且对于不可压缩的数据会自动关闭压缩功能. 极少有场景会让用户想要完全关闭压缩功能, 除非基准测试显示关闭压缩会显著改善性能. 按照下面方式做就关闭了压缩功能: leveldb::Options options; options.compression = leveldb::kNoCompression; ... leveldb::DB::Open(options, name, ...) .... 缓存数据库的内容存储在文件系统的一组文件里, 每个文件保存着一系列压缩后的 blocks. 如果 options.block_cache 不为空, 它就会被用于缓存频繁被使用的 block 内容(已解压缩). #include \"leveldb/cache.h\" leveldb::Options options; // 打开数据库之前分配一个 100MB 的 LRU Cache 用于缓存解压的 blocks options.block_cache = leveldb::NewLRUCache(100 * 1048576); // 100MB cache leveldb::DB* db; // 打开数据库 leveldb::DB::Open(options, name, \u0026db); ... use the db ... delete db delete options.block_cache; 注意 cache 保存的是未压缩的数据, 因此应该根据应用程序所需的数据大小来设置它的大小. (已压缩数据的缓存工作交给操作系统的 buffer cache 或者用户提供的定制的 Env 实现去干. ) 当执行一个大块数据读操作时, 应用程序可能想要取消缓存功能, 这样读进来的大块数据就不会导致 cache 中当前大部分数据被置换出去, 我们可以为它提供一个单独的 iterator 来达到该目的: leveldb::ReadOptions options; // 缓存设置为关闭 options.fill_cache = false; // 用该设置去创建一个新的迭代器 leveldb::Iterator* it = db-\u003eNewIterator(options); // 用该迭代器去处理大块数据 for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { ... } Key 的布局设计注意, 磁盘传输的单位以及磁盘缓存的单位都是一个 block. 相邻的 keys(已排序)总是在同一个 block 中. 因此应用程序可以通过把需要一起访问的 keys 放在一起, 同时把不经常使用的 keys 放到一个独立的键空间区域来提升性能. 举个例子, 假设我们正基于 leveldb 实现一个简单的文件系统. 我们打算存储到这个文件系统的数据项类型如下: filename -\u003e permission-bits, length, list of file_block_ids file_block_id -\u003e data 我们可以给上面表示 filename 的 key 增加一个字符前缀, 比如 ‘/’, 然后给表示 file_block_id 的 key 增加另一个不同的前缀, 比如 ‘0’, 这样这些不同用途的 key 就具有了各自独立的键空间区域, 扫描元数据的时候我们就不用读取和缓存大块文件内容数据了. 过滤器鉴于 leveldb 数据在磁盘上的组织形式, 一次 Get() 调用可能涉及多次磁盘读操作. 可配置的 FilterPolicy 机制可以用来大幅减少磁盘读次数. leveldb::Options options; // 设置启用基于布隆过滤器的过滤策略 options.filter_policy = NewBloomFilterPolicy(10); leveldb::DB* db; // 用该设置打开数据库 leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... use the database ... delete db; delete options.filter_policy; 上述代码将一个基于布隆过滤器的过滤策略与数据库进行了关联. 基于布隆过滤器的过滤方式依赖于如下事实, 在内存中保存每个 key 的部分位(在上面例子中是 10 位, 因为我们传给 NewBloomFilterPolicy 的参数是 10). 这个过滤器将会使得 Get() 调用中非必须的磁盘读操作大约减少 100 倍. 每个 key 用于过滤器的位数增加将会进一步减少读磁盘次数, 当然也会占用更多内存空间. 我们推荐数据集无法全部放入内存同时又存在大量随机读的应用设置一个过滤器策略. 如果你在使用定制的比较器, 你应该确保你在用的过滤器策略与你的比较器兼容. 举个例子, 如果一个比较器在比较 key 的时候忽略结尾的空格, 那么NewBloomFilterPolicy 一定不能与此比较器共存. 相反, 应用应该提供一个定制的过滤器策略, 而且它也应该忽略键的尾部空格. 示例如下: class CustomFilterPolicy : public leveldb::FilterPolicy { private: FilterPolicy* builtin_policy_; public: CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) {} ~CustomFilterPolicy() { delete builtin_policy_; } const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; } void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Use builtin bloom filter code after removing trailing spaces // 将尾部空格移除后再使用内置的布隆过滤器 std::vector\u003cSlice\u003e trimmed(n); for (int i = 0; i \u003c n; i++) { trimmed[i] = RemoveTrailingSpaces(keys[i]); } return builtin_policy_-\u003eCreateFilter(\u0026trimmed[i], n, dst); } }; 当然也可以自己提供非基于布隆过滤器的过滤器策略, 具体见 leveldb/filter_policy.h. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:14","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#性能调优"},{"categories":null,"content":" 性能调优通过修改 include/leveldb/options.h 中定义的类型的默认值来对 leveldb 的性能进行调优. Block 大小Leveldb 把相邻的 keys 组织在同一个 block 中(具体见后面文章针对 sorted string table 文件格式的描述), 而且 block 是 leveldb 把数据从内存到转移到持久化存储和从持久化存储转移到内存的基本单位. 默认的, 压缩前 block 大约为 4KB. 经常处理大块数据的应用可能希望把这个值调大, 而针对数据做\"点读\" 的应用可能希望这个值小一点, 这样性能可能会更高一些. 但是, 没有证据表明该值小于 1KB 或者大于几个 MB 的时候性能会表现更好. 同时要注意, 针对大的 block size, 压缩效率会更高一些. 压缩每个 block 在写入持久存储之前都会被单独压缩. 压缩默认是开启的, 因为默认的压缩算法非常快, 而且对于不可压缩的数据会自动关闭压缩功能. 极少有场景会让用户想要完全关闭压缩功能, 除非基准测试显示关闭压缩会显著改善性能. 按照下面方式做就关闭了压缩功能: leveldb::Options options; options.compression = leveldb::kNoCompression; ... leveldb::DB::Open(options, name, ...) .... 缓存数据库的内容存储在文件系统的一组文件里, 每个文件保存着一系列压缩后的 blocks. 如果 options.block_cache 不为空, 它就会被用于缓存频繁被使用的 block 内容(已解压缩). #include \"leveldb/cache.h\" leveldb::Options options; // 打开数据库之前分配一个 100MB 的 LRU Cache 用于缓存解压的 blocks options.block_cache = leveldb::NewLRUCache(100 * 1048576); // 100MB cache leveldb::DB* db; // 打开数据库 leveldb::DB::Open(options, name, \u0026db); ... use the db ... delete db delete options.block_cache; 注意 cache 保存的是未压缩的数据, 因此应该根据应用程序所需的数据大小来设置它的大小. (已压缩数据的缓存工作交给操作系统的 buffer cache 或者用户提供的定制的 Env 实现去干. ) 当执行一个大块数据读操作时, 应用程序可能想要取消缓存功能, 这样读进来的大块数据就不会导致 cache 中当前大部分数据被置换出去, 我们可以为它提供一个单独的 iterator 来达到该目的: leveldb::ReadOptions options; // 缓存设置为关闭 options.fill_cache = false; // 用该设置去创建一个新的迭代器 leveldb::Iterator* it = db-\u003eNewIterator(options); // 用该迭代器去处理大块数据 for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { ... } Key 的布局设计注意, 磁盘传输的单位以及磁盘缓存的单位都是一个 block. 相邻的 keys(已排序)总是在同一个 block 中. 因此应用程序可以通过把需要一起访问的 keys 放在一起, 同时把不经常使用的 keys 放到一个独立的键空间区域来提升性能. 举个例子, 假设我们正基于 leveldb 实现一个简单的文件系统. 我们打算存储到这个文件系统的数据项类型如下: filename -\u003e permission-bits, length, list of file_block_ids file_block_id -\u003e data 我们可以给上面表示 filename 的 key 增加一个字符前缀, 比如 ‘/’, 然后给表示 file_block_id 的 key 增加另一个不同的前缀, 比如 ‘0’, 这样这些不同用途的 key 就具有了各自独立的键空间区域, 扫描元数据的时候我们就不用读取和缓存大块文件内容数据了. 过滤器鉴于 leveldb 数据在磁盘上的组织形式, 一次 Get() 调用可能涉及多次磁盘读操作. 可配置的 FilterPolicy 机制可以用来大幅减少磁盘读次数. leveldb::Options options; // 设置启用基于布隆过滤器的过滤策略 options.filter_policy = NewBloomFilterPolicy(10); leveldb::DB* db; // 用该设置打开数据库 leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... use the database ... delete db; delete options.filter_policy; 上述代码将一个基于布隆过滤器的过滤策略与数据库进行了关联. 基于布隆过滤器的过滤方式依赖于如下事实, 在内存中保存每个 key 的部分位(在上面例子中是 10 位, 因为我们传给 NewBloomFilterPolicy 的参数是 10). 这个过滤器将会使得 Get() 调用中非必须的磁盘读操作大约减少 100 倍. 每个 key 用于过滤器的位数增加将会进一步减少读磁盘次数, 当然也会占用更多内存空间. 我们推荐数据集无法全部放入内存同时又存在大量随机读的应用设置一个过滤器策略. 如果你在使用定制的比较器, 你应该确保你在用的过滤器策略与你的比较器兼容. 举个例子, 如果一个比较器在比较 key 的时候忽略结尾的空格, 那么NewBloomFilterPolicy 一定不能与此比较器共存. 相反, 应用应该提供一个定制的过滤器策略, 而且它也应该忽略键的尾部空格. 示例如下: class CustomFilterPolicy : public leveldb::FilterPolicy { private: FilterPolicy* builtin_policy_; public: CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) {} ~CustomFilterPolicy() { delete builtin_policy_; } const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; } void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Use builtin bloom filter code after removing trailing spaces // 将尾部空格移除后再使用内置的布隆过滤器 std::vector trimmed(n); for (int i = 0; i \u003c n; i++) { trimmed[i] = RemoveTrailingSpaces(keys[i]); } return builtin_policy_-\u003eCreateFilter(\u0026trimmed[i], n, dst); } }; 当然也可以自己提供非基于布隆过滤器的过滤器策略, 具体见 leveldb/filter_policy.h. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:14","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#block-大小"},{"categories":null,"content":" 性能调优通过修改 include/leveldb/options.h 中定义的类型的默认值来对 leveldb 的性能进行调优. Block 大小Leveldb 把相邻的 keys 组织在同一个 block 中(具体见后面文章针对 sorted string table 文件格式的描述), 而且 block 是 leveldb 把数据从内存到转移到持久化存储和从持久化存储转移到内存的基本单位. 默认的, 压缩前 block 大约为 4KB. 经常处理大块数据的应用可能希望把这个值调大, 而针对数据做\"点读\" 的应用可能希望这个值小一点, 这样性能可能会更高一些. 但是, 没有证据表明该值小于 1KB 或者大于几个 MB 的时候性能会表现更好. 同时要注意, 针对大的 block size, 压缩效率会更高一些. 压缩每个 block 在写入持久存储之前都会被单独压缩. 压缩默认是开启的, 因为默认的压缩算法非常快, 而且对于不可压缩的数据会自动关闭压缩功能. 极少有场景会让用户想要完全关闭压缩功能, 除非基准测试显示关闭压缩会显著改善性能. 按照下面方式做就关闭了压缩功能: leveldb::Options options; options.compression = leveldb::kNoCompression; ... leveldb::DB::Open(options, name, ...) .... 缓存数据库的内容存储在文件系统的一组文件里, 每个文件保存着一系列压缩后的 blocks. 如果 options.block_cache 不为空, 它就会被用于缓存频繁被使用的 block 内容(已解压缩). #include \"leveldb/cache.h\" leveldb::Options options; // 打开数据库之前分配一个 100MB 的 LRU Cache 用于缓存解压的 blocks options.block_cache = leveldb::NewLRUCache(100 * 1048576); // 100MB cache leveldb::DB* db; // 打开数据库 leveldb::DB::Open(options, name, \u0026db); ... use the db ... delete db delete options.block_cache; 注意 cache 保存的是未压缩的数据, 因此应该根据应用程序所需的数据大小来设置它的大小. (已压缩数据的缓存工作交给操作系统的 buffer cache 或者用户提供的定制的 Env 实现去干. ) 当执行一个大块数据读操作时, 应用程序可能想要取消缓存功能, 这样读进来的大块数据就不会导致 cache 中当前大部分数据被置换出去, 我们可以为它提供一个单独的 iterator 来达到该目的: leveldb::ReadOptions options; // 缓存设置为关闭 options.fill_cache = false; // 用该设置去创建一个新的迭代器 leveldb::Iterator* it = db-\u003eNewIterator(options); // 用该迭代器去处理大块数据 for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { ... } Key 的布局设计注意, 磁盘传输的单位以及磁盘缓存的单位都是一个 block. 相邻的 keys(已排序)总是在同一个 block 中. 因此应用程序可以通过把需要一起访问的 keys 放在一起, 同时把不经常使用的 keys 放到一个独立的键空间区域来提升性能. 举个例子, 假设我们正基于 leveldb 实现一个简单的文件系统. 我们打算存储到这个文件系统的数据项类型如下: filename -\u003e permission-bits, length, list of file_block_ids file_block_id -\u003e data 我们可以给上面表示 filename 的 key 增加一个字符前缀, 比如 ‘/’, 然后给表示 file_block_id 的 key 增加另一个不同的前缀, 比如 ‘0’, 这样这些不同用途的 key 就具有了各自独立的键空间区域, 扫描元数据的时候我们就不用读取和缓存大块文件内容数据了. 过滤器鉴于 leveldb 数据在磁盘上的组织形式, 一次 Get() 调用可能涉及多次磁盘读操作. 可配置的 FilterPolicy 机制可以用来大幅减少磁盘读次数. leveldb::Options options; // 设置启用基于布隆过滤器的过滤策略 options.filter_policy = NewBloomFilterPolicy(10); leveldb::DB* db; // 用该设置打开数据库 leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... use the database ... delete db; delete options.filter_policy; 上述代码将一个基于布隆过滤器的过滤策略与数据库进行了关联. 基于布隆过滤器的过滤方式依赖于如下事实, 在内存中保存每个 key 的部分位(在上面例子中是 10 位, 因为我们传给 NewBloomFilterPolicy 的参数是 10). 这个过滤器将会使得 Get() 调用中非必须的磁盘读操作大约减少 100 倍. 每个 key 用于过滤器的位数增加将会进一步减少读磁盘次数, 当然也会占用更多内存空间. 我们推荐数据集无法全部放入内存同时又存在大量随机读的应用设置一个过滤器策略. 如果你在使用定制的比较器, 你应该确保你在用的过滤器策略与你的比较器兼容. 举个例子, 如果一个比较器在比较 key 的时候忽略结尾的空格, 那么NewBloomFilterPolicy 一定不能与此比较器共存. 相反, 应用应该提供一个定制的过滤器策略, 而且它也应该忽略键的尾部空格. 示例如下: class CustomFilterPolicy : public leveldb::FilterPolicy { private: FilterPolicy* builtin_policy_; public: CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) {} ~CustomFilterPolicy() { delete builtin_policy_; } const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; } void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Use builtin bloom filter code after removing trailing spaces // 将尾部空格移除后再使用内置的布隆过滤器 std::vector trimmed(n); for (int i = 0; i \u003c n; i++) { trimmed[i] = RemoveTrailingSpaces(keys[i]); } return builtin_policy_-\u003eCreateFilter(\u0026trimmed[i], n, dst); } }; 当然也可以自己提供非基于布隆过滤器的过滤器策略, 具体见 leveldb/filter_policy.h. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:14","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#压缩"},{"categories":null,"content":" 性能调优通过修改 include/leveldb/options.h 中定义的类型的默认值来对 leveldb 的性能进行调优. Block 大小Leveldb 把相邻的 keys 组织在同一个 block 中(具体见后面文章针对 sorted string table 文件格式的描述), 而且 block 是 leveldb 把数据从内存到转移到持久化存储和从持久化存储转移到内存的基本单位. 默认的, 压缩前 block 大约为 4KB. 经常处理大块数据的应用可能希望把这个值调大, 而针对数据做\"点读\" 的应用可能希望这个值小一点, 这样性能可能会更高一些. 但是, 没有证据表明该值小于 1KB 或者大于几个 MB 的时候性能会表现更好. 同时要注意, 针对大的 block size, 压缩效率会更高一些. 压缩每个 block 在写入持久存储之前都会被单独压缩. 压缩默认是开启的, 因为默认的压缩算法非常快, 而且对于不可压缩的数据会自动关闭压缩功能. 极少有场景会让用户想要完全关闭压缩功能, 除非基准测试显示关闭压缩会显著改善性能. 按照下面方式做就关闭了压缩功能: leveldb::Options options; options.compression = leveldb::kNoCompression; ... leveldb::DB::Open(options, name, ...) .... 缓存数据库的内容存储在文件系统的一组文件里, 每个文件保存着一系列压缩后的 blocks. 如果 options.block_cache 不为空, 它就会被用于缓存频繁被使用的 block 内容(已解压缩). #include \"leveldb/cache.h\" leveldb::Options options; // 打开数据库之前分配一个 100MB 的 LRU Cache 用于缓存解压的 blocks options.block_cache = leveldb::NewLRUCache(100 * 1048576); // 100MB cache leveldb::DB* db; // 打开数据库 leveldb::DB::Open(options, name, \u0026db); ... use the db ... delete db delete options.block_cache; 注意 cache 保存的是未压缩的数据, 因此应该根据应用程序所需的数据大小来设置它的大小. (已压缩数据的缓存工作交给操作系统的 buffer cache 或者用户提供的定制的 Env 实现去干. ) 当执行一个大块数据读操作时, 应用程序可能想要取消缓存功能, 这样读进来的大块数据就不会导致 cache 中当前大部分数据被置换出去, 我们可以为它提供一个单独的 iterator 来达到该目的: leveldb::ReadOptions options; // 缓存设置为关闭 options.fill_cache = false; // 用该设置去创建一个新的迭代器 leveldb::Iterator* it = db-\u003eNewIterator(options); // 用该迭代器去处理大块数据 for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { ... } Key 的布局设计注意, 磁盘传输的单位以及磁盘缓存的单位都是一个 block. 相邻的 keys(已排序)总是在同一个 block 中. 因此应用程序可以通过把需要一起访问的 keys 放在一起, 同时把不经常使用的 keys 放到一个独立的键空间区域来提升性能. 举个例子, 假设我们正基于 leveldb 实现一个简单的文件系统. 我们打算存储到这个文件系统的数据项类型如下: filename -\u003e permission-bits, length, list of file_block_ids file_block_id -\u003e data 我们可以给上面表示 filename 的 key 增加一个字符前缀, 比如 ‘/’, 然后给表示 file_block_id 的 key 增加另一个不同的前缀, 比如 ‘0’, 这样这些不同用途的 key 就具有了各自独立的键空间区域, 扫描元数据的时候我们就不用读取和缓存大块文件内容数据了. 过滤器鉴于 leveldb 数据在磁盘上的组织形式, 一次 Get() 调用可能涉及多次磁盘读操作. 可配置的 FilterPolicy 机制可以用来大幅减少磁盘读次数. leveldb::Options options; // 设置启用基于布隆过滤器的过滤策略 options.filter_policy = NewBloomFilterPolicy(10); leveldb::DB* db; // 用该设置打开数据库 leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... use the database ... delete db; delete options.filter_policy; 上述代码将一个基于布隆过滤器的过滤策略与数据库进行了关联. 基于布隆过滤器的过滤方式依赖于如下事实, 在内存中保存每个 key 的部分位(在上面例子中是 10 位, 因为我们传给 NewBloomFilterPolicy 的参数是 10). 这个过滤器将会使得 Get() 调用中非必须的磁盘读操作大约减少 100 倍. 每个 key 用于过滤器的位数增加将会进一步减少读磁盘次数, 当然也会占用更多内存空间. 我们推荐数据集无法全部放入内存同时又存在大量随机读的应用设置一个过滤器策略. 如果你在使用定制的比较器, 你应该确保你在用的过滤器策略与你的比较器兼容. 举个例子, 如果一个比较器在比较 key 的时候忽略结尾的空格, 那么NewBloomFilterPolicy 一定不能与此比较器共存. 相反, 应用应该提供一个定制的过滤器策略, 而且它也应该忽略键的尾部空格. 示例如下: class CustomFilterPolicy : public leveldb::FilterPolicy { private: FilterPolicy* builtin_policy_; public: CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) {} ~CustomFilterPolicy() { delete builtin_policy_; } const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; } void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Use builtin bloom filter code after removing trailing spaces // 将尾部空格移除后再使用内置的布隆过滤器 std::vector trimmed(n); for (int i = 0; i \u003c n; i++) { trimmed[i] = RemoveTrailingSpaces(keys[i]); } return builtin_policy_-\u003eCreateFilter(\u0026trimmed[i], n, dst); } }; 当然也可以自己提供非基于布隆过滤器的过滤器策略, 具体见 leveldb/filter_policy.h. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:14","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#缓存"},{"categories":null,"content":" 性能调优通过修改 include/leveldb/options.h 中定义的类型的默认值来对 leveldb 的性能进行调优. Block 大小Leveldb 把相邻的 keys 组织在同一个 block 中(具体见后面文章针对 sorted string table 文件格式的描述), 而且 block 是 leveldb 把数据从内存到转移到持久化存储和从持久化存储转移到内存的基本单位. 默认的, 压缩前 block 大约为 4KB. 经常处理大块数据的应用可能希望把这个值调大, 而针对数据做\"点读\" 的应用可能希望这个值小一点, 这样性能可能会更高一些. 但是, 没有证据表明该值小于 1KB 或者大于几个 MB 的时候性能会表现更好. 同时要注意, 针对大的 block size, 压缩效率会更高一些. 压缩每个 block 在写入持久存储之前都会被单独压缩. 压缩默认是开启的, 因为默认的压缩算法非常快, 而且对于不可压缩的数据会自动关闭压缩功能. 极少有场景会让用户想要完全关闭压缩功能, 除非基准测试显示关闭压缩会显著改善性能. 按照下面方式做就关闭了压缩功能: leveldb::Options options; options.compression = leveldb::kNoCompression; ... leveldb::DB::Open(options, name, ...) .... 缓存数据库的内容存储在文件系统的一组文件里, 每个文件保存着一系列压缩后的 blocks. 如果 options.block_cache 不为空, 它就会被用于缓存频繁被使用的 block 内容(已解压缩). #include \"leveldb/cache.h\" leveldb::Options options; // 打开数据库之前分配一个 100MB 的 LRU Cache 用于缓存解压的 blocks options.block_cache = leveldb::NewLRUCache(100 * 1048576); // 100MB cache leveldb::DB* db; // 打开数据库 leveldb::DB::Open(options, name, \u0026db); ... use the db ... delete db delete options.block_cache; 注意 cache 保存的是未压缩的数据, 因此应该根据应用程序所需的数据大小来设置它的大小. (已压缩数据的缓存工作交给操作系统的 buffer cache 或者用户提供的定制的 Env 实现去干. ) 当执行一个大块数据读操作时, 应用程序可能想要取消缓存功能, 这样读进来的大块数据就不会导致 cache 中当前大部分数据被置换出去, 我们可以为它提供一个单独的 iterator 来达到该目的: leveldb::ReadOptions options; // 缓存设置为关闭 options.fill_cache = false; // 用该设置去创建一个新的迭代器 leveldb::Iterator* it = db-\u003eNewIterator(options); // 用该迭代器去处理大块数据 for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { ... } Key 的布局设计注意, 磁盘传输的单位以及磁盘缓存的单位都是一个 block. 相邻的 keys(已排序)总是在同一个 block 中. 因此应用程序可以通过把需要一起访问的 keys 放在一起, 同时把不经常使用的 keys 放到一个独立的键空间区域来提升性能. 举个例子, 假设我们正基于 leveldb 实现一个简单的文件系统. 我们打算存储到这个文件系统的数据项类型如下: filename -\u003e permission-bits, length, list of file_block_ids file_block_id -\u003e data 我们可以给上面表示 filename 的 key 增加一个字符前缀, 比如 ‘/’, 然后给表示 file_block_id 的 key 增加另一个不同的前缀, 比如 ‘0’, 这样这些不同用途的 key 就具有了各自独立的键空间区域, 扫描元数据的时候我们就不用读取和缓存大块文件内容数据了. 过滤器鉴于 leveldb 数据在磁盘上的组织形式, 一次 Get() 调用可能涉及多次磁盘读操作. 可配置的 FilterPolicy 机制可以用来大幅减少磁盘读次数. leveldb::Options options; // 设置启用基于布隆过滤器的过滤策略 options.filter_policy = NewBloomFilterPolicy(10); leveldb::DB* db; // 用该设置打开数据库 leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... use the database ... delete db; delete options.filter_policy; 上述代码将一个基于布隆过滤器的过滤策略与数据库进行了关联. 基于布隆过滤器的过滤方式依赖于如下事实, 在内存中保存每个 key 的部分位(在上面例子中是 10 位, 因为我们传给 NewBloomFilterPolicy 的参数是 10). 这个过滤器将会使得 Get() 调用中非必须的磁盘读操作大约减少 100 倍. 每个 key 用于过滤器的位数增加将会进一步减少读磁盘次数, 当然也会占用更多内存空间. 我们推荐数据集无法全部放入内存同时又存在大量随机读的应用设置一个过滤器策略. 如果你在使用定制的比较器, 你应该确保你在用的过滤器策略与你的比较器兼容. 举个例子, 如果一个比较器在比较 key 的时候忽略结尾的空格, 那么NewBloomFilterPolicy 一定不能与此比较器共存. 相反, 应用应该提供一个定制的过滤器策略, 而且它也应该忽略键的尾部空格. 示例如下: class CustomFilterPolicy : public leveldb::FilterPolicy { private: FilterPolicy* builtin_policy_; public: CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) {} ~CustomFilterPolicy() { delete builtin_policy_; } const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; } void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Use builtin bloom filter code after removing trailing spaces // 将尾部空格移除后再使用内置的布隆过滤器 std::vector trimmed(n); for (int i = 0; i \u003c n; i++) { trimmed[i] = RemoveTrailingSpaces(keys[i]); } return builtin_policy_-\u003eCreateFilter(\u0026trimmed[i], n, dst); } }; 当然也可以自己提供非基于布隆过滤器的过滤器策略, 具体见 leveldb/filter_policy.h. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:14","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#key-的布局设计"},{"categories":null,"content":" 性能调优通过修改 include/leveldb/options.h 中定义的类型的默认值来对 leveldb 的性能进行调优. Block 大小Leveldb 把相邻的 keys 组织在同一个 block 中(具体见后面文章针对 sorted string table 文件格式的描述), 而且 block 是 leveldb 把数据从内存到转移到持久化存储和从持久化存储转移到内存的基本单位. 默认的, 压缩前 block 大约为 4KB. 经常处理大块数据的应用可能希望把这个值调大, 而针对数据做\"点读\" 的应用可能希望这个值小一点, 这样性能可能会更高一些. 但是, 没有证据表明该值小于 1KB 或者大于几个 MB 的时候性能会表现更好. 同时要注意, 针对大的 block size, 压缩效率会更高一些. 压缩每个 block 在写入持久存储之前都会被单独压缩. 压缩默认是开启的, 因为默认的压缩算法非常快, 而且对于不可压缩的数据会自动关闭压缩功能. 极少有场景会让用户想要完全关闭压缩功能, 除非基准测试显示关闭压缩会显著改善性能. 按照下面方式做就关闭了压缩功能: leveldb::Options options; options.compression = leveldb::kNoCompression; ... leveldb::DB::Open(options, name, ...) .... 缓存数据库的内容存储在文件系统的一组文件里, 每个文件保存着一系列压缩后的 blocks. 如果 options.block_cache 不为空, 它就会被用于缓存频繁被使用的 block 内容(已解压缩). #include \"leveldb/cache.h\" leveldb::Options options; // 打开数据库之前分配一个 100MB 的 LRU Cache 用于缓存解压的 blocks options.block_cache = leveldb::NewLRUCache(100 * 1048576); // 100MB cache leveldb::DB* db; // 打开数据库 leveldb::DB::Open(options, name, \u0026db); ... use the db ... delete db delete options.block_cache; 注意 cache 保存的是未压缩的数据, 因此应该根据应用程序所需的数据大小来设置它的大小. (已压缩数据的缓存工作交给操作系统的 buffer cache 或者用户提供的定制的 Env 实现去干. ) 当执行一个大块数据读操作时, 应用程序可能想要取消缓存功能, 这样读进来的大块数据就不会导致 cache 中当前大部分数据被置换出去, 我们可以为它提供一个单独的 iterator 来达到该目的: leveldb::ReadOptions options; // 缓存设置为关闭 options.fill_cache = false; // 用该设置去创建一个新的迭代器 leveldb::Iterator* it = db-\u003eNewIterator(options); // 用该迭代器去处理大块数据 for (it-\u003eSeekToFirst(); it-\u003eValid(); it-\u003eNext()) { ... } Key 的布局设计注意, 磁盘传输的单位以及磁盘缓存的单位都是一个 block. 相邻的 keys(已排序)总是在同一个 block 中. 因此应用程序可以通过把需要一起访问的 keys 放在一起, 同时把不经常使用的 keys 放到一个独立的键空间区域来提升性能. 举个例子, 假设我们正基于 leveldb 实现一个简单的文件系统. 我们打算存储到这个文件系统的数据项类型如下: filename -\u003e permission-bits, length, list of file_block_ids file_block_id -\u003e data 我们可以给上面表示 filename 的 key 增加一个字符前缀, 比如 ‘/’, 然后给表示 file_block_id 的 key 增加另一个不同的前缀, 比如 ‘0’, 这样这些不同用途的 key 就具有了各自独立的键空间区域, 扫描元数据的时候我们就不用读取和缓存大块文件内容数据了. 过滤器鉴于 leveldb 数据在磁盘上的组织形式, 一次 Get() 调用可能涉及多次磁盘读操作. 可配置的 FilterPolicy 机制可以用来大幅减少磁盘读次数. leveldb::Options options; // 设置启用基于布隆过滤器的过滤策略 options.filter_policy = NewBloomFilterPolicy(10); leveldb::DB* db; // 用该设置打开数据库 leveldb::DB::Open(options, \"/tmp/testdb\", \u0026db); ... use the database ... delete db; delete options.filter_policy; 上述代码将一个基于布隆过滤器的过滤策略与数据库进行了关联. 基于布隆过滤器的过滤方式依赖于如下事实, 在内存中保存每个 key 的部分位(在上面例子中是 10 位, 因为我们传给 NewBloomFilterPolicy 的参数是 10). 这个过滤器将会使得 Get() 调用中非必须的磁盘读操作大约减少 100 倍. 每个 key 用于过滤器的位数增加将会进一步减少读磁盘次数, 当然也会占用更多内存空间. 我们推荐数据集无法全部放入内存同时又存在大量随机读的应用设置一个过滤器策略. 如果你在使用定制的比较器, 你应该确保你在用的过滤器策略与你的比较器兼容. 举个例子, 如果一个比较器在比较 key 的时候忽略结尾的空格, 那么NewBloomFilterPolicy 一定不能与此比较器共存. 相反, 应用应该提供一个定制的过滤器策略, 而且它也应该忽略键的尾部空格. 示例如下: class CustomFilterPolicy : public leveldb::FilterPolicy { private: FilterPolicy* builtin_policy_; public: CustomFilterPolicy() : builtin_policy_(NewBloomFilterPolicy(10)) {} ~CustomFilterPolicy() { delete builtin_policy_; } const char* Name() const { return \"IgnoreTrailingSpacesFilter\"; } void CreateFilter(const Slice* keys, int n, std::string* dst) const { // Use builtin bloom filter code after removing trailing spaces // 将尾部空格移除后再使用内置的布隆过滤器 std::vector trimmed(n); for (int i = 0; i \u003c n; i++) { trimmed[i] = RemoveTrailingSpaces(keys[i]); } return builtin_policy_-\u003eCreateFilter(\u0026trimmed[i], n, dst); } }; 当然也可以自己提供非基于布隆过滤器的过滤器策略, 具体见 leveldb/filter_policy.h. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:14","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#过滤器"},{"categories":null,"content":" 校验和Leveldb 将一个校验和与它存储在文件系统中的全部数据进行关联. 根据激进程度有两种方式控制校验和的核对: ReadOptions::verify_checksums 可以设置为 true 来强制核对从文件系统读取的全部数据的进行校验和检查. 默认为 false. Options::paranoid_checks 在数据库打开之前设置为 true 可以使得数据库一旦检测到数据损毁即报错. 取决于数据库损坏部位, 报错时机可能是打开数据库后的时候, 也可能是在后续执行某个操作的时候. 该配置默认是关闭状态, 这样即使持久性存储部分虽坏数据库也能继续使用. 如果数据库损坏了(当开启 Options::paranoid_checks 的时候可能就打不开了), leveldb::RepairDB 函数可以用于对尽可能多的数据进行修复. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:15","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#校验和"},{"categories":null,"content":" 近似空间大小GetApproximateSizes 方法用于获取一个或多个键区间占据的文件系统近似大小(单位, 字节). leveldb::Range ranges[2]; ranges[0] = leveldb::Range(\"a\", \"c\"); ranges[1] = leveldb::Range(\"x\", \"z\"); uint64_t sizes[2]; leveldb::Status s = db-\u003eGetApproximateSizes(ranges, 2, sizes); 上述代码结果是, size[0] 保存 [a..c) 键区间对应的文件系统大致字节数, size[1] 保存 [x..z) 键区间对应的文件系统大致字节数. ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:16","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#近似空间大小"},{"categories":null,"content":" 环境变量由 leveldb 发起的全部文件操作以及其它的操作系统调用最后都会被路由给一个 leveldb::Env 对象. 用户也可以提供自己的 Env 实现以达到更好的控制. 比如, 如果应用程序想要针对 leveldb 的文件 IO 引入一个人工延迟以限制 leveldb 对同一个系统中其它应用的影响: // 定制自己的 Env class SlowEnv : public leveldb::Env { ... implementation of the Env interface ... }; SlowEnv env; leveldb::Options options; // 用定制的 Env 打开数据库 options.env = \u0026env; Status s = leveldb::DB::Open(options, ...); ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:17","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#环境变量"},{"categories":null,"content":" 移植性如果某个特定平台提供 leveldb/port/port.h 导出的类型/方法/函数实现, 那么 leveldb 可以被移植到该平台上, 更多细节见 leveldb/port/port_example.h. 另外, 新平台可能还需要一个新的默认的 leveldb::Env 实现. 具体可参考 leveldb/util/env_posix.h 实现. –End– ","date":"2020-09-11","objectID":"/leveldb-annotations-0-usage-and-examples/:2:18","series":null,"tags":["leveldb","LSM-Tree","db","kv"],"title":"Leveldb 源码详解系列之零: 基本介绍与使用举例","uri":"/leveldb-annotations-0-usage-and-examples/#移植性"},{"categories":null,"content":"之前的博客采用 OctoPress 搭建, 但是当时同步数据没有把 _post 下面的 md 原始文件同步, 导致这次迁移没法把之前几年积攒的文章搬过来. Hello world, again! ","date":"2020-09-10","objectID":"/hello-world-again/:0:0","series":null,"tags":["trivial"],"title":"Hello World, Again","uri":"/hello-world-again/#"}]